<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Lee_yl&#39;s blog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Lee_yl&#39;s blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lee_yl&#39;s blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/">





  <title>Lee_yl's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lee_yl's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/概率论1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/概率论1/" itemprop="url">概率论</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/" itemprop="url" rel="index">
                    <span itemprop="name">先导知识</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/概率论基本知识/" itemprop="url" rel="index">
                    <span itemprop="name">概率论基本知识</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="一、均值、方差、协方差"><a href="#一、均值、方差、协方差" class="headerlink" title="一、均值、方差、协方差"></a>一、均值、方差、协方差</h1><ul>
<li>期望/均值：实验中每次可能结果的概率乘其结果的总和。<ul>
<li>E(X)=∑xP(X), x表示随机变量的取值，P(X)表示随机变量X=x的概率。</li>
</ul>
</li>
<li>方差：概率分布的数据期望，反映了随机变量取值的变异程度。<ul>
<li>D(x ) = E{[X-E(X)]^2} =E(X^2) - [ E(X)]^2</li>
</ul>
</li>
</ul>
<p><img src="..\imgs\协方差.jpg" alt></p>
<ul>
<li>协方差：度量两个随机变量关系的统计量<ul>
<li><img src="..\imgs\协方差2.jpg" alt></li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/标准化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/标准化/" itemprop="url">标准化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/标准化/" itemprop="url" rel="index">
                    <span itemprop="name">标准化</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h2 id="一、BN批标准化"><a href="#一、BN批标准化" class="headerlink" title="一、BN批标准化"></a>一、BN批标准化</h2><h3 id="1、BN的基本动机"><a href="#1、BN的基本动机" class="headerlink" title="1、BN的基本动机"></a>1、BN的基本动机</h3><ol>
<li><strong>（初始数据分布一致）</strong>神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布      不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。</li>
<li><strong>（中间数据分布一致）</strong>然而随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变      化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合 不同的数据分布，增大训练的复杂度以及过拟合的风险。</li>
</ol>
<p>原因在于神经网络学习过程<strong>本质上是为了学习数据的分布</strong>，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另一方面，一旦在mini-batch梯度下降训练的时候，每批训练数据的分布不相同，那么网络就要在每次迭代的时候去学习以适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对所有训练数据做一个Normalization预处理的原因。</p>
<h3 id="2、-BN的原理"><a href="#2、-BN的原理" class="headerlink" title="2、 BN的原理"></a>2、 BN的原理</h3><p>BN首先是把所有的样本的统计分布标准化，降低了batch内不同样本的差异性，然后又允许batch内的各个样本有各自的统计分布。</p>
<p>BN是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），将所有批数据强制在统一的数据分布下，即对该层 的任意一个神经元（假设为第k维）x^(k) 采用如下公式:</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/过拟合欠拟合/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/过拟合欠拟合/" itemprop="url">过拟合</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/过拟合/" itemprop="url" rel="index">
                    <span itemprop="name">过拟合</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h1><h1 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h1><blockquote>
<p>过拟合：模型对于训练数拟合呈过当的情况，反映到评估指标上，就是模型在训练集上表现很好，测试集和新数据上表现较差。</p>
<p>欠拟合：模型在训练和和预测时表现都不好的情况。</p>
</blockquote>
<p><img src="..\imgs\过拟合欠拟合.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/正则化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/正则化/" itemprop="url">正则化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/正则化/" itemprop="url" rel="index">
                    <span itemprop="name">正则化</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="正则化-惩罚项"><a href="#正则化-惩罚项" class="headerlink" title="正则化/惩罚项"></a>正则化/惩罚项</h1><h2 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h2><blockquote>
<p> <strong>（1）范数：</strong><img src="..\imgs\范数.jpg" alt></p>
<p><strong>（2）方差和偏差：</strong></p>
<p>Error = Bias + Variance</p>
<ul>
<li>Error反映的是整个模型的准确度，</li>
<li>Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，</li>
<li>Variance反映的是模型每一次输出结果与模型输出期望之间的误差（描述的是样本上训练的模型在测试集上的表现。），即模型的稳定性。</li>
<li>欠拟合是高bias，过拟合是高variance。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>（3）正则化的目的</strong>：减少模型参数大小或者参数数量，缓解过拟合。</p>
<p>正则化的作用是给模型加一个先验，lasso(l1)认为模型是拉普拉斯分布，ridge(l2)认为是高斯分布，正则项对应参数的协方差，协方差越小，这个模型的variance越小，泛化 能力越强，也就抵抗了过拟合。</p>
<p><strong>（4）正则化通用形式：</strong></p>
<p>​        Loss_with_regularization = loss(w,x) + λf(w)</p>
<ul>
<li>正则化恒为非负</li>
<li>f(w)不能为负数，若其为负数，Loss(w,x)+λf(x)本来尽可能想让其变小，那f(x)为负数，f(x)绝对值会越学越大。</li>
</ul>
<p><strong>(5) 正则化方法：</strong>L1正则、L2正则、Dropout正则</p>
</blockquote>
<h2 id="二、-从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】"><a href="#二、-从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】" class="headerlink" title="二、 从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】"></a>二、 从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】</h2><p><strong>过拟合就是模型在学习训练样本时将噪声异常值也学习得非常好，使得模型参数过多，模型较复杂，给参数加上一个先验约束，可降低过拟合。</strong></p>
<p> <img src="..\imgs\正则化1.jpg" alt></p>
<h2 id="三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；"><a href="#三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；" class="headerlink" title="三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；"></a>三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；</h2><p><strong>区别</strong>：L1假设参数服从拉普拉斯分布，L2则符合高斯分布；</p>
<p>L1范数更容易产生稀疏的权重，L2范数更容易产生分散的权重。</p>
<p><strong>原因</strong>：（L1稀疏的原因，L2不稀疏的原因）【几何、公式两个角度】</p>
<p><strong>场景</strong>：具有高维的数据特征时采用L1正则效果好一点。因为L1具有稀疏性。</p>
<h2 id="四、解释L1范数更容易产生稀疏的权重，L2不的原因："><a href="#四、解释L1范数更容易产生稀疏的权重，L2不的原因：" class="headerlink" title="四、解释L1范数更容易产生稀疏的权重，L2不的原因："></a>四、解释L1范数更容易产生稀疏的权重，L2不的原因：</h2><h3 id="（1）几何角度"><a href="#（1）几何角度" class="headerlink" title="（1）几何角度"></a>（1）几何角度</h3><p>L2正则项约束后的解空间是圆形，L1正则项约束后的解空间是多方形，L1易在角点发生交点，从而产生稀疏解。</p>
<blockquote>
<p>绿色等高线代表未施加正则化的代价函数，菱形和圆形分别代表L1和L2正则化约束，L1-ball 与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的”等高线”除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性。相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小</p>
</blockquote>
<p><img src="..\imgs\L1正则解释.jpg" alt></p>
<h3 id="（2）公式角度：（拉格朗日求导）"><a href="#（2）公式角度：（拉格朗日求导）" class="headerlink" title="（2）公式角度：（拉格朗日求导）"></a>（2）公式角度：（拉格朗日求导）</h3><blockquote>
<p>深度学习花书7.1节（202页左右）。带L1正则化的最优参数w=sign(w<em>) max{|w</em>|- a/H , 0}，其中w代表未正则化的目标函数的最优参数，H代表海森矩阵，a是正则化系数，只要a足够大，w就会在更大区间范围内使w变为0，而带L2正则化的最优参数w=H/(H+a)▪w,只要w不为0，w也不为0.</p>
</blockquote>
<p><strong>1、稀疏性的约束：</strong></p>
<p><img src="..\imgs\正则公式1.jpg" alt></p>
<p>​    L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</p>
<p><strong>2、不好求解，松弛为L1，L2：</strong></p>
<p><img src="..\imgs\正则公式2.jpg" alt></p>
<p><strong>3、拉格朗日</strong></p>
<p><img src="..\imgs\正则公式3.jpg" alt></p>
<h3 id="（3）贝叶斯先验"><a href="#（3）贝叶斯先验" class="headerlink" title="（3）贝叶斯先验"></a>（3）贝叶斯先验</h3><blockquote>
<p>L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验，而拉普拉斯先验使参数为0的可能性更大。</p>
</blockquote>
<p><strong>L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出。</strong></p>
<p><strong>L2正则化可通过假设权重w的先验分布为高斯分布，由最大后验概率估计导出。</strong></p>
<p><strong>详细解释：</strong> <a href="https://blog.csdn.net/m0_38045485/article/details/82147817" target="_blank" rel="noopener">https://blog.csdn.net/m0_38045485/article/details/82147817</a></p>
<p><img src="..\imgs\正则公式4.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/广告校准/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/广告校准/" itemprop="url">广告校准</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-25T00:00:00+08:00">
                2023-02-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/计算广告/" itemprop="url" rel="index">
                    <span itemprop="name">计算广告</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/计算广告/校准广告/" itemprop="url" rel="index">
                    <span itemprop="name">校准广告</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<p><a href="https://zhuanlan.zhihu.com/p/460061332" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/460061332</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/582530785" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/582530785</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/398235467" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/398235467</a></p>
<h1 id="广告校准"><a href="#广告校准" class="headerlink" title="广告校准"></a>广告校准</h1><h2 id="1-业务背景"><a href="#1-业务背景" class="headerlink" title="1. 业务背景"></a>1. 业务背景</h2><blockquote>
<p>广告三大角色：广告主、媒体、DSP</p>
<ol>
<li>ctr : (Click Through Rate) 点击率 = click / show， 曝光广告中用户点击的概率。</li>
<li>cvr: (Conversion Rate) 转化率 = order / click，点击广告中用户转化的概率。（如注册，激活，创角等）</li>
<li>cpa：(Cost per Action) 转化成本 = cost / order, 表示广告主每获得一个转化需付的成本。</li>
<li>ecpm / rpm：= ctr <em> cvr </em> cpa<ol>
<li>ecpm : 对广告主来说，(Effective Cost Per Mile) 每千次展示的有效费用。 </li>
<li>rpm：对DSP来说，(Revenue Per Mile)每千次展示的收入。</li>
</ol>
</li>
<li>pctr: (Predict CTR) 预估点击率</li>
<li>pcvr: (Predict CVR) 预估转化率</li>
</ol>
</blockquote>
<h2 id="2-面临的问题"><a href="#2-面临的问题" class="headerlink" title="2. 面临的问题"></a>2. 面临的问题</h2><blockquote>
<p>（1）模型准确性存在偏差，受限于</p>
<p>​    实际分布和离线分布的差异</p>
<p>​    模型学习能力</p>
<p>（2）预估模型的准确性度量</p>
<p>​    <strong>AUC：</strong>仅作为排序指标，无法度量预估值的大小准确性</p>
<p>​    <strong>COPC：</strong>（Click On Predict Click) = sum( 实际ctr) / sum(pctr)</p>
<p>​            用于评估某段细分的流量模型预估值是否偏差较大。</p>
<p>（3）校准评价指标：</p>
<p>​    <strong>PCOC：</strong>（predict click over click）COPC是相反的指标。</p>
<p>​    <strong>cal-N：（calibration-N）</strong></p>
<p>​        cal-N将样本集合分桶后分别计算PCOC，并计算与1的偏差作为标准误差。举个例子，将pctr根据值大小划分为多个桶，每个桶为一个簇，计算每个簇的PCOC及其与1的偏差 数学公式:</p>
<p><img src="..\imgs\校准评价指标.jpg" alt></p>
<p>​    <strong>GC-N：（grouped calibration-N）</strong></p>
<p>​        在具体业务场景下，有时会重点关注某一维度下的校准效果(如广告计费维度)，GC-N可以解决这个问题，它可以在cal-N基础上自定义各维度权重。例如，下面这个式子定义了m个广告计划的GC-N 数学公式:</p>
</blockquote>
<h2 id="3-校准算法"><a href="#3-校准算法" class="headerlink" title="3. 校准算法"></a>3. 校准算法</h2><p><img src="https://pic2.zhimg.com/80/v2-c452e8c2822418e0b744fe751123b275_720w.webp" alt="img"></p>
<h3 id="（1）-Bias-Correction：负采样率修正"><a href="#（1）-Bias-Correction：负采样率修正" class="headerlink" title="（1） Bias Correction：负采样率修正"></a>（1） Bias Correction：负采样率修正</h3><blockquote>
<p><strong>原因：</strong>正负样本不均衡情况下，负采样通常可以提升模型的AUC精度，但pctr值会发生变化，与真实差距扩大。</p>
<p><strong>校准公式：</strong></p>
<p><img src="..\imgs\采样校准1.jpg" alt></p>
<p>因此可以计算出校准后的bias: <strong>b′=b+log(n)</strong></p>
</blockquote>
<p><strong>代码：</strong></p>
<p>注意在导出模型时，最终结果过完sigmoid，再进行bias_correct。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_correct</span><span class="params">(b, nr)</span>:</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">	nr: neg_sample_rate</span></span><br><span class="line"><span class="string">	</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">    <span class="keyword">if</span> nr &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> b + math.log(nr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line">nr = <span class="number">0.1</span> <span class="comment"># 负采样率</span></span><br><span class="line">last_op[<span class="string">'bias'</span>] = bias_correct(last_op[<span class="string">'bias'</span>]， nr) <span class="comment"># 纠正sigmoid后的bias</span></span><br></pre></td></tr></table></figure>
<h3 id="（2）校准算法—保序回归"><a href="#（2）校准算法—保序回归" class="headerlink" title="（2）校准算法—保序回归"></a>（2）校准算法—保序回归</h3><blockquote>
<p>解决模型高低估、模型over-confidence等问题。</p>
</blockquote>
<p><img src="..\imgs\保序回归.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sir_calibration_model</span><span class="params">(pctr_list = [<span class="number">0.01</span>,<span class="number">0.02</span>,<span class="number">0.03</span>,<span class="number">0.04</span>], ctr_list = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>], bin_size = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">	sort_ctr_list = sorted(list(zip(pctr_list, ctr_list)), key = <span class="keyword">lambda</span> x:x[<span class="number">0</span>])</span><br><span class="line">    bin_index = <span class="number">0</span></span><br><span class="line">    ind = <span class="number">0</span></span><br><span class="line">    n = len(ctr_list)</span><br><span class="line">    cctr_res = []</span><br><span class="line">    <span class="keyword">while</span> ind &lt; n:</span><br><span class="line">        <span class="keyword">while</span> (bin_index + bin_size) &lt; sort_ctr_list[ind][<span class="number">0</span>]:</span><br><span class="line">            cctr_res.append(sort_ctr_list[ind][<span class="number">1</span>])</span><br><span class="line">            bin_index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (bin_index + bin_size) == sort_ctr_list[ind][<span class="number">0</span>]:</span><br><span class="line">            cctr_res.append(sort_ctr_list[ind][<span class="number">1</span>])</span><br><span class="line">            ind += <span class="number">1</span></span><br><span class="line">            bin_index += bin_size</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> ind + <span class="number">1</span>== n:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        tmp_cctr = sort_ctr_list[ind + <span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">        tmp_num = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> ind + <span class="number">1</span> &lt; n <span class="keyword">and</span> bin_index + bin_size &gt; sort_ctr_list[ind][<span class="number">0</span>]:</span><br><span class="line">            tmp_cctr += sort_ctr_list[ind][<span class="number">1</span>]</span><br><span class="line">            tmp_num += <span class="number">1</span></span><br><span class="line">            ind += <span class="number">1</span></span><br><span class="line">        cctr_res.append(tmp_cctr/tmp_num)</span><br><span class="line">        bin_index += bin_size</span><br></pre></td></tr></table></figure>
<h3 id="（3）校准算法3-—-SIR（保序回归平滑校准算法）"><a href="#（3）校准算法3-—-SIR（保序回归平滑校准算法）" class="headerlink" title="（3）校准算法3 —-SIR（保序回归平滑校准算法）"></a>（3）校准算法3 —-SIR（保序回归平滑校准算法）</h3><blockquote>
<p>解决桶间数据稀疏问题</p>
</blockquote>
<p><img src="..\imgs\保序回归平滑校准1.jpg" alt></p>
<blockquote>
<p> SIR算法是18年提出的，如上图所示，结合了Binning、Isotonic Regression和线性Scaling方法。</p>
</blockquote>
<p>具体思想为：</p>
<ol>
<li><p>进行保序回归。</p>
</li>
<li><p>使用单调平滑函数拟合模型预估值和实际点击率的映射关系（线性Scaling）就得到了校准函数。</p>
</li>
</ol>
<p>该算法的优势在于充分利用了<strong>保序和平滑</strong>思想缓解了<strong>数据稀疏</strong>的问题。</p>
<p>（详细可见论文：﻿Calibrating user response predictions in online advertising）。</p>
<h3 id="（4）校准算法4—贝叶斯平滑SIR校准算法（Bayes-SIR"><a href="#（4）校准算法4—贝叶斯平滑SIR校准算法（Bayes-SIR" class="headerlink" title="（4）校准算法4—贝叶斯平滑SIR校准算法（Bayes-SIR)"></a>（4）校准算法4—贝叶斯平滑SIR校准算法（Bayes-SIR)</h3><blockquote>
<ul>
<li><p>Bayes-SIR解决冷启动问题。</p>
</li>
<li><p>beta分布：可以看作一个概率的概率分布</p>
<p><a href="https://blog.csdn.net/a358463121/article/details/52562940" target="_blank" rel="noopener">带你理解beta分布</a></p>
</li>
<li><p>贝叶斯平滑方法:（最早在雅虎的一篇论文里面中提出，用于解决数据稀疏问题下的点击率预估优化）。</p>
</li>
</ul>
</blockquote>
<p>在SIR算法应用中，发现广告计划投放初期校准效果明显差于平均水平，并在实际业务中造成以下问题：</p>
<p>1）影响新建计划初始阶段的投放表现；</p>
<p>2）影响强时效性广告的全生命周期效果；</p>
<p>3）小客户在整个投放周期里数据一直稀疏，得不到准确的校准，影响竞价公平性。</p>
<p>这是SIR校准算法的冷启动问题，采用了Bayes平滑的思想进行优化.</p>
<p><a href><img src="..\imgs\贝叶斯SIR.jpg" alt></a></p>
<p>Bayes-SIR的算法思想：如上图所示，</p>
<ol>
<li><p>从丰富的先验数据中估计出每个广告计划的点击率先验分布，</p>
</li>
<li><p>依据该先验知识求解出belta分布的参数α和β。</p>
</li>
<li>依据α和β和新观测到的少量数据，计算得到更准确的后验点击率。</li>
</ol>
<p>这种估计方法能充分利用先验知识，具备置信程度过渡平滑的特点。</p>
<p>将贝叶斯平滑CTR估计过程替换掉SIR算法的朴素CTR统计逻辑即构成了具有冷启动问题优化效果的校准方法。</p>
<p>实际上线后，新广告的投放效果得到明显的提升。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/梯度下降/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/梯度下降/" itemprop="url">梯度下降</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-21T00:00:00+08:00">
                2023-02-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/梯度下降/" itemprop="url" rel="index">
                    <span itemprop="name">梯度下降</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<p>[TOC]</p>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><h2 id="1-重点"><a href="#1-重点" class="headerlink" title="1. 重点"></a>1. 重点</h2><blockquote>
<p>(1) 常用的优化方法：牛顿法、GD、拟牛顿法、共轭梯度法，之间的区别</p>
<p>(2) GD三种变形：BGD、SGD、MBGD</p>
<p>(3) 多种改进方法：Momentum、NAG、Adagrad、Adadelta、RMSProp、Adam</p>
</blockquote>
<h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2. 概念"></a>2. 概念</h2><h3 id="（1）常用的优化方法：直接法、迭代法（梯度下降、牛顿法、拟牛顿法、共轭梯度法）"><a href="#（1）常用的优化方法：直接法、迭代法（梯度下降、牛顿法、拟牛顿法、共轭梯度法）" class="headerlink" title="（1）常用的优化方法：直接法、迭代法（梯度下降、牛顿法、拟牛顿法、共轭梯度法）"></a>（1）常用的优化方法：直接法、迭代法（梯度下降、牛顿法、拟牛顿法、共轭梯度法）</h3><h3 id="（2）直接法（求解析解）："><a href="#（2）直接法（求解析解）：" class="headerlink" title="（2）直接法（求解析解）："></a>（2）直接法（求解析解）：</h3><p>求梯度，令梯度为0</p>
<h3 id="（3）牛顿法："><a href="#（3）牛顿法：" class="headerlink" title="（3）牛顿法："></a>（3）牛顿法：</h3><p><img src="..\imgs\牛顿法.jpg" alt></p>
<p><img src="..\imgs\牛顿法2.jpg" alt></p>
<h3 id="（4）-GD-梯度下降"><a href="#（4）-GD-梯度下降" class="headerlink" title="（4） GD 梯度下降:"></a>（4） GD 梯度下降:</h3><p>让变量沿着目标函数负梯度的方向移动，直到移动到极小值点。</p>
<p>从拉格朗日中值定理 / 泰勒展开一阶公式都可以推出损失函数下降最大的方向是梯度方向。</p>
<p><img src="..\imgs\梯度下降方法解释.jpg" alt></p>
<p><img src="..\imgs\梯度下降方法1.jpg" alt></p>
<h2 id="3-梯度下降法和牛顿法的区别"><a href="#3-梯度下降法和牛顿法的区别" class="headerlink" title="3. 梯度下降法和牛顿法的区别"></a>3. 梯度下降法和牛顿法的区别</h2><p><strong>牛顿法和梯度下降法对比：</strong></p>
<ul>
<li><strong>从公式上看</strong>，牛顿法是二阶收敛，梯度下降是一阶收敛（局部最优），所以牛顿法就更快。</li>
</ul>
<p>【如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。】</p>
<ul>
<li><strong>从几何上看</strong>，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</li>
</ul>
<h2 id="4-三种变形"><a href="#4-三种变形" class="headerlink" title="4. 三种变形"></a>4. 三种变形</h2><p>GD的三种变形：BGD、SGD、MBGD</p>
<p>这三种形式的区别就是取决于我们用多少数据来计算目标函数的梯度。</p>
<h3 id="（1）Batch-gradient-descend【批量梯度下降】"><a href="#（1）Batch-gradient-descend【批量梯度下降】" class="headerlink" title="（1）Batch gradient descend【批量梯度下降】"></a>（1）Batch gradient descend【批量梯度下降】</h3><ul>
<li><p>定义：【采用整个训练数据集的数据计算损失函数对参数的梯度】<br><img src="..\imgs\BGD.jpg" alt></p>
</li>
<li><p>优点：全局最优解；易于并行实现；</p>
</li>
<li>缺点：大样本数据计算速度非常慢，不能投入新数据实时更新模型。</li>
<li>收敛：对于凸函数可以收敛到全局最优，对于非凸函数可以收敛到局部最优。</li>
</ul>
<h3 id="（2）-Stochastic-gradient-descent【随机梯度下降】【适合在线更新】"><a href="#（2）-Stochastic-gradient-descent【随机梯度下降】【适合在线更新】" class="headerlink" title="（2） Stochastic gradient descent【随机梯度下降】【适合在线更新】"></a>（2） Stochastic gradient descent【随机梯度下降】【适合在线更新】</h3><ul>
<li>定义：SGD  每次更新时对一个样本进行梯度更新。<br> <img src="..\imgs\SGD.jpg" alt></li>
<li>优点：对于很大的数据集来说，可能会有相似的样本，这样 BGD在计算梯度时会出现冗余， 而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。</li>
<li>缺点：但是 SGD 因为更新比较频繁，会造成 cost  function 有严重的震荡。准确度下降，不易于并行实现。<br> <img src="..\imgs\SGD缺点.jpg" alt></li>
<li>收敛性：不一定每次更新朝着最优值方向，因为存在噪音点，局部最优。</li>
</ul>
<h3 id="（3）mini-batch-GD【小批量梯度下降】：降低随机梯度的方差"><a href="#（3）mini-batch-GD【小批量梯度下降】：降低随机梯度的方差" class="headerlink" title="（3）mini-batch GD【小批量梯度下降】：降低随机梯度的方差"></a>（3）mini-batch GD【小批量梯度下降】：降低随机梯度的方差</h3><p>【在更新每一参数时都使用一部分样本来进行更新】</p>
<p><img src="..\imgs\MBGD&#39;.jpg" alt>‘</p>
<ul>
<li>缺点：需要指定batch大小，收敛性不好。</li>
<li>一般batch取2的幂次能充分利用矩阵运算操作（32,64,128……）</li>
<li><img src="..\imgs\MBGD.jpg" alt></li>
</ul>
<p><strong>三种方法的使用情况：</strong></p>
<p>如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法。</p>
<p><strong>GD具有的几个问题：</strong></p>
<p>1、学习率选择【太小，收敛速度慢，太大，在最优值附近震荡】</p>
<p>2、学习率不固定【稀疏数据，学习率可增大】</p>
<p>3、（尤其SGD）对于非凸函数，易陷于局部最优值/鞍点。</p>
<h2 id="5、优化方法"><a href="#5、优化方法" class="headerlink" title="5、优化方法"></a>5、优化方法</h2><h3 id="（1）动量Momentum【一般为SGD-momentum】"><a href="#（1）动量Momentum【一般为SGD-momentum】" class="headerlink" title="（1）动量Momentum【一般为SGD+momentum】"></a>（1）动量Momentum【一般为SGD+momentum】</h3><blockquote>
<p><strong>原理：</strong>因为SGD易陷于局部最优点或鞍点，一种帮助SGD在相关方向进行加速并抑制振荡的方法</p>
</blockquote>
<p><img src="..\imgs\Momentum_SGD.jpg" alt></p>
<p><img src="..\imgs\Momentum_SGD_1.jpg" alt></p>
<p>momentum表示要在多大程度上保留原来的更新方向，这个值在0-1之间<strong>，在训练开始时，由于梯度可能会很大，所以初始值一般选为0.5；当梯度不那么大时</strong>，一般改为0.9。<br>α是学习率，即当前batch的梯度多大程度上影响最终更新方向，跟普通的SGD含义相同。</p>
<ul>
<li>优点：因此获得了更快的收敛性和减少了震荡。</li>
<li>缺点：这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/向量乘积/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/向量乘积/" itemprop="url">向量乘积</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-21T00:00:00+08:00">
                2023-02-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/" itemprop="url" rel="index">
                    <span itemprop="name">先导知识</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/向量乘积/" itemprop="url" rel="index">
                    <span itemprop="name">向量乘积</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="向量乘积"><a href="#向量乘积" class="headerlink" title="向量乘积"></a>向量乘积</h1><h2 id="1-内积"><a href="#1-内积" class="headerlink" title="1. 内积"></a>1. 内积</h2><blockquote>
<p> 含义：两个向量的相似度/向量的夹角， 标量。</p>
</blockquote>
<p>向量a和向量b的余弦的相似度: 对内积进行了归一化</p>
<p>​    cosθ = 内积 / (|a| * |b| )</p>
<h2 id="2-哈达玛积"><a href="#2-哈达玛积" class="headerlink" title="2. 哈达玛积"></a>2. 哈达玛积</h2><blockquote>
<p>元素两两相乘。</p>
</blockquote>
<p>[a1, b1, c1] * [a2, b2, c2] = [a1a2, b1b2, c1c2]</p>
<p><strong>元素集的交互：</strong> [w1<em> a1a2, w2 </em> b1b2, w3 *c1c2]</p>
<p><strong>向量级别的交互：</strong>w [a1a2, b1b2, c1c2]</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/逻辑回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/逻辑回归/" itemprop="url">逻辑回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-20T00:00:00+08:00">
                2023-02-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/逻辑回归/" itemprop="url" rel="index">
                    <span itemprop="name">逻辑回归</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<p>[TOC]</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="1-LR重点"><a href="#1-LR重点" class="headerlink" title="1. LR重点"></a>1. LR重点</h2><p><strong>LR的重点：</strong></p>
<ul>
<li><strong>目的</strong>：LR解决二分类问题。</li>
<li><strong>输出结果</strong>：(0,1)之间的数值。</li>
<li><strong>模型</strong>：线性模型 + sigmoid函数<ul>
<li>y_pred = sigmoid(W0+W1 <em> x1 + … + Wn </em> xn) = sigmoid(∑W^TX)</li>
</ul>
</li>
<li><strong>LR为何用sigmoid函数？</strong> sigmoid的性质 </li>
<li><strong>损失函数</strong>：loss = -(∑y) log y_pred_i + (1-yi) log(1 - y_pred_i))</li>
<li><strong>损失函数的推导：</strong></li>
<li><strong>求解方法</strong>：梯度下降</li>
<li></li>
</ul>
<h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2. 概念"></a>2. 概念</h2><blockquote>
<p><strong>二分类：</strong>假设有一批训练样本集合X={x1,x2,…,xn},其中xi有a个属性，对这些样本分类属于0还是1？</p>
<p><strong>伯努利分布：</strong>离散型概率分布，成功则随机变量取值为1，失败则为0，设置成功的概率为p,则失败的概率为1-p，N次实验后，成功期望是Np，方差为 Np(1-p)</p>
<ul>
<li><p>​    期望/均值：实验中每次可能结果的概率乘其结果的总和。E(X)=∑xP(X), x表示随机变量的取值，P(X)表示随机变量X=x的概率。</p>
</li>
<li><p>​    方差：概率分布的数据期望，反映了随机变量取值的变异程度。D(x ) = E{[X-E(X)]^2}=E(X^2) - [ E(X)]^2</p>
</li>
</ul>
<p><strong>似然函数（联合概率密度函数）：</strong></p>
<ul>
<li><p>先验概率：根据以往经验和分析得到的概率。主观上的经验估计p(x)</p>
</li>
<li><p>似然函数：在给定参数θ情况下得到结果 X 的概率分布p(x|θ) 。</p>
<p>（给定输出x时，使得函数得到X概率最大的关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率， 即L(θ|x) = P(X=x|θ)）</p>
</li>
<li><p>后验概率：在给定结果信息X的情况下得到参数 θ 的概率： p(θ|x) 。</p>
</li>
</ul>
<p><strong>梯度下降：</strong>一种求解模型的方法，后面细看。</p>
<p><strong>LR：</strong>假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，达到将数据二分类的目的。</p>
</blockquote>
<h2 id="3-LR模型和损失函数"><a href="#3-LR模型和损失函数" class="headerlink" title="3. LR模型和损失函数"></a>3. LR模型和损失函数</h2><h3 id="模型和损失函数得到的过程："><a href="#模型和损失函数得到的过程：" class="headerlink" title="模型和损失函数得到的过程："></a>模型和损失函数得到的过程：</h3><p>第①步：引入参数θ=（θ1，θ2……），对于样本加权θ^TX</p>
<p>第②步：引入logit函数：g(z)=1/(1+e^-z)，令z=θ^TX，故<strong>LR的模型</strong>：</p>
<p>​        <img src="..\imgs\LR模型.jpg"></p>
<p>​        <strong>假设样本服从的是伯努利分布（0-1分布）:</strong></p>
<p>​        以上h的含义是：表示样本x属于类别1的概率，即P（y=1|x;θ）。则样本x属于类别0的概率为1-hθ(x)。</p>
<p>​        联合两个概率即为似然函数。</p>
<p>第③步：似然函数（联合概率密度函数）：</p>
<p><img src="..\imgs\LR最大似然函数.jpg" alt></p>
<p>​        所有样本的似然函数：（令概率最大化）</p>
<p>​                    <img src="..\imgs\LR似然函数1.jpg" alt></p>
<p>第④步：<strong>损失函数</strong>（对数似然函数取反min）：<strong>交叉熵</strong>损失函数</p>
<p><img src="..\imgs\LR损失函数.jpg" alt></p>
<p>​        </p>
<h2 id="4-损失函数的来源推导"><a href="#4-损失函数的来源推导" class="headerlink" title="4. 损失函数的来源推导"></a>4. 损失函数的来源推导</h2><h3 id="法一：假设服从伯努利分布，伯努利分布的极大似然估计"><a href="#法一：假设服从伯努利分布，伯努利分布的极大似然估计" class="headerlink" title="法一：假设服从伯努利分布，伯努利分布的极大似然估计"></a><strong>法一：假设服从伯努利分布，伯努利分布的极大似然估计</strong></h3><ul>
<li>假设样本服从伯努利(0-1)分布,有：</li>
</ul>
<p>​                    <img src="..\imgs\LR损失函数推导.jpg" alt></p>
<h3 id="法二：熵角度确定损失函数"><a href="#法二：熵角度确定损失函数" class="headerlink" title="法二：熵角度确定损失函数"></a><strong>法二：熵角度确定损失函数</strong></h3><blockquote>
<p><strong>KL散度（相对熵）：KL散度可以用来<code>衡量两个分布之间的差异程度</code>。若两者差异越小，KL散度越小，反之亦反。当两分布一致时，其KL散度为0。</strong></p>
<p>​    相关概念请看：熵的章节</p>
<p>​    如果是两个随机变量P,Q，且其概率分布分别为p(x),q(x),则p相对q的相对熵为：</p>
<p>​        <img src="..\imgs\KL散度.jpg" alt></p>
</blockquote>
<p>模型预估值分布和实际值分布的KL散度：（KL散度越小，表明两个分布越相似，即预估值越接近实际值）。</p>
<blockquote>
<ul>
<li><p>p <em> log(p/q) = p</em> logp - p* logq</p>
<p>其中， p<em> logp是一个定值（实际值分布p = y_i，是固定的，0或1），- p</em> logq是交叉熵，只要交叉熵较大，则KL散度就越小，预测值和实际值越相似。</p>
</li>
<li><p>交叉熵：令-p <em> log q = -y_i </em> log y_pred_i . (可扩展为多个： -（y1 <em> log y_pred_1 + y2 </em> log y_pred_2 + y3 * log y_pred_3.。。。)</p>
</li>
</ul>
</blockquote>
<h3 id="法三、贝叶斯学派确定损失函数"><a href="#法三、贝叶斯学派确定损失函数" class="headerlink" title="法三、贝叶斯学派确定损失函数"></a><strong>法三、贝叶斯学派确定损失函数</strong></h3><p>贝叶斯公式：</p>
<p><img src="..\imgs\贝叶斯推导损失函数.jpg" alt></p>
<p>贝叶斯学派有先验函数，频率学派仅考虑似然函数。</p>
<p>贝叶斯用上一次的后验当成下一次的先验与似然函数相乘，计算下一次的后验，不断迭代。</p>
<h2 id="5-LR为何用sigmoid函数？"><a href="#5-LR为何用sigmoid函数？" class="headerlink" title="5. LR为何用sigmoid函数？"></a>5. LR为何用sigmoid函数？</h2><h3 id="（1）数学上"><a href="#（1）数学上" class="headerlink" title="（1）数学上"></a>（1）数学上</h3><pre><code>1. 因为LR服从伯努利分布，伯努利分布转化为广义线性模型指数分布族形式里可推导出sigmoid函数
</code></pre><h3 id="（2）概率上："><a href="#（2）概率上：" class="headerlink" title="（2）概率上："></a>（2）概率上：</h3><p>sigmoid自身性质，值域在（0,1）之间，满足概率的要求</p>
<h3 id="（3）单调性："><a href="#（3）单调性：" class="headerlink" title="（3）单调性："></a>（3）单调性：</h3><p>若sigmoid不具备单调性，LR不可用其作为映射函数。</p>
<ul>
<li><p>下图的g1 &lt; g2的x指的是线性中的g=wx的值。若LR的映射函数不具备单调性，则同一个y值会对应两个g值。</p>
<p>因为x是给定的特征，则w没办法学习，因为LR是个记忆函数，在g1之前当学习w发现增大w1能让y接近高点（即在g1和g2之间）时，突然发现当w1更大时（大于g2），y越来越小，没办法沿着某些方向学习w值。</p>
</li>
</ul>
<p><img src="..\imgs\sigmoid单调性.jpg" alt></p>
<ul>
<li>sigmoid具备单调性，则在使用redis存储参数w的key和value时，若推荐最终结果只涉及排序，则无需进行sigmoid计算，且若是离散特征，只需将x特征值为1对应的参数w相加得到最终值进行排序即可。又可以提高效率了。</li>
</ul>
<h3 id="sigmoid性质："><a href="#sigmoid性质：" class="headerlink" title="sigmoid性质："></a>sigmoid性质：</h3><blockquote>
<ol>
<li>定义域：（-∞，+∞）</li>
<li>将任意input压缩到(0,1)之间，即值域为（0,1），符合概率定义</li>
<li>设f(x) = sigmoid(x), f(x)导数为f(x)*(1-f(x)),  对梯度下降有好处，前向传递时计算结果可以保留，BP时可以直接用来计算，加快速度</li>
<li>1/2处导数最大， 两边梯度趋于饱和，容易发生梯度消失（作为激活函数的弊端）<ol>
<li><img src="https://pic2.zhimg.com/v2-729148803fc62ea5ee2327f28b6db301_b.jpg" alt="img"></li>
</ol>
</li>
<li>不以原点为中心，BP时梯度下降更新慢，更新时以zigzag（折线）方式更新。（作为激活函数的弊端）</li>
<li>单调性，LR的映射函数如果没有单调性不行。</li>
</ol>
</blockquote>
<h2 id="6-【为什么参数的梯度方向一致容易造成zigzag现象】"><a href="#6-【为什么参数的梯度方向一致容易造成zigzag现象】" class="headerlink" title="6.【为什么参数的梯度方向一致容易造成zigzag现象】"></a>6.【为什么参数的梯度方向一致容易造成zigzag现象】</h2><p>当所有梯度同为正或者负时，参数在梯度更新时容易出现zigzag现象。</p>
<p>zigzag现象如图4所示，不妨假设一共两个参数， w0 和 w1 ，紫色点为参数的最优解，蓝色箭头表示梯度最优方向，红色箭头表示实际梯度更新方向。</p>
<p>由于参数的梯度方向一致，要么同正，要么同负，因此更新方向只能为第三象限角度或第一象限角度，而梯度的最优方向为第四象限角度，也就是参数 w0 要向着变小的方向， w1 要向着变大的方向，在这种情况下，每更新一次梯度，不管是同时变小(第三象限角度)还是同时变大(第四象限角度)，总是一个参数更接近最优状态，另一个参数远离最优状态，因此为了使参数尽快收敛到最优状态，出现交替向最优状态更新的现象，也就是zigzag现象。</p>
<p><img src="https://pic3.zhimg.com/80/v2-bedaefcfbff2453f003365c10fc86356_720w.webp" alt="img"></p>
<h2 id="7-梯度下降求解损失函数"><a href="#7-梯度下降求解损失函数" class="headerlink" title="7. 梯度下降求解损失函数"></a>7. 梯度下降求解损失函数</h2><p>梯度下降概念：根据loss求导，往</p>
<p><img src="..\imgs\梯度下降.jpg" alt></p>
<p><img src="..\imgs\LR的梯度下降求导.jpg" alt></p>
<p>注意：这里梯度更新时用加号是因为max loss。 非min loss的值。</p>
<h2 id="8-LR的优缺点"><a href="#8-LR的优缺点" class="headerlink" title="8. LR的优缺点"></a>8. LR的优缺点</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ul>
<li>1、<strong>形式简单，模型的可解释性非常好</strong>。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</li>
<li>2、<strong>模型效果不错。</strong>在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</li>
<li>3、<strong>训练速度较快。</strong>分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</li>
<li>4、<strong>资源占用小,尤其是内存</strong>。因为只需要存储各个维度的特征值。</li>
<li>5、<strong>方便输出结果调整。</strong>逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</li>
<li>6、<strong>对稀疏数据比较友好。</strong>一个特征通过 one-hot : [0,0,1,……,0]  ，LR是个记忆模型，将该特征扔进LR之后，会将x3=1（w3）记得很好，别的都为0了，但这样也容易过拟合。（gbm，深度模型这些都不太喜欢稀疏数据）</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点:"></a>缺点:</h3><ul>
<li>准<strong>确率并不是很高</strong>。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</li>
<li><strong>很难处理数据不平衡的问题。</strong>举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比      10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</li>
<li><strong>处理非线性数据较麻烦。</strong>逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题      。</li>
<li><strong>逻辑回归本身无法筛选特征。</strong>有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</li>
<li><strong>对模型中自变量多重共线性较为敏感</strong>，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；</li>
<li><strong>必须对缺失值处理</strong></li>
</ul>
<h2 id="9-扩展问题"><a href="#9-扩展问题" class="headerlink" title="9. 扩展问题"></a>9. 扩展问题</h2><blockquote>
<ol>
<li><strong>线性回归与LR的本质区别</strong>：</li>
</ol>
<p>【线形回归与 逻辑回归的本质区别是前者用于回归后者用于分类，二者都是线形模型。】</p>
<ol start="2">
<li><p><strong>什么情况下用LR</strong>【大规模样本下的线性二分类】</p>
</li>
<li><p><strong>LR的损失函数的公式和函数</strong></p>
</li>
<li><p>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？ </p>
<p>【如果激活函数为sigmoid函数，使用均方误差作为损失函数的话为非凸函数，而使用对数损失函数为凸函数】</p>
<p>【损失函数一般有四种，平方损失函数、对数损失函数、铰链损失函数和绝对值损失函数。极大似然取对数之后相当于对数损失，在LR模型下，对数损失函数的训练求解参数速度是比较快的，不选平方损失函数是因为梯度更新的速度和sigmoid函数本身的梯度是相关的，这样训练会比较慢】</p>
</li>
<li><p>LR的推导过程</p>
</li>
<li><p>LR如何解决共线性，为什么深度学习不强调</p>
<p>【法1：岭回归（L2正则化）、</p>
<p>法2：主成分分析，</p>
<p>法3：共线性诊断常用的统计量：将所有回归中要用到的变量依次作为因变量、其他变量作为自变量进行回归分析，可以得到各个变量的膨胀系数VIF以及容忍度tolerance，如果容忍度越接近0，则共线性问题越严重，而VIF是越大共线性越严重，通常VIF小于5可以认为共线性不严重，宽泛一点的标准小于10即可。      】</p>
<p>【因为深度学习不需要处理数据特征，自身可以学习】</p>
<p>共线性问题有如下几种检验方法：</p>
<ul>
<li><strong>相关性分析</strong>。检验变量之间的相关系数；</li>
<li><strong>方差膨胀因子VIF</strong>。当VIF大于5或10时，代表模型存在严重的共线性问题；</li>
<li><strong>条件数检验</strong>。当条件数大于100、1000时，代表模型存在严重的共线性问题</li>
</ul>
</li>
<li><p>为什么不能将线性或逻辑回归模型的系数绝对值解释为特征重要性？</p>
<p>【因为很多现有线性回归量为每个系数返回P值，对于线性模型，许多实践者认为，系数绝对值越大，其对应特征越重要。事实很少如此，因为：</p>
<p>(a)改变变量尺度就会改变系数绝对值；</p>
<p>(b)如果特征是线性相关的，则系数可以从一个特征转移到另一个特征。此外，数据集特征越多，特征间越可能线性相关，用系数解释特征重要性就越不可靠。】</p>
</li>
<li><p>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</p>
<p>结论：如果在损失函数最终收敛的情况下，其实就算<strong>有很多特征高度相关也不会影响分类器的效果</strong>。原因：<strong>因为损失函数为凸函数，最终它会收敛到全局最小值，不再变化。</strong>因为LR是线性模型，其特征重复仅在每个特征分配权重缩小为百分之一，最后得到总结果和一个特征是一样的。】</p>
</li>
<li><p>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</p>
<p>【形式简单，模型可解释性好。训练速度快。资源占用内存小】</p>
</li>
<li><p>LR如何防止过拟合：</p>
<p>【增加正则化项，L1、L2】</p>
</li>
<li><p>LR分布式训练怎么做</p>
<p>【LR的并行化最主要的就是对目标函数梯度计算的并行化。梯度计算就是向量的点乘和相加，可通过按行或按列切分样本数据并行计算梯度，最后合并结果】</p>
</li>
<li><p>LR如何解决线性不可分问题？或者问LR为何常常要做特征组合？</p>
<p>【逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过2种方式帮助LR实现：（1）利用特殊核函数，对特征进行变换：把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。（2）扩展LR算法，提出FM算法。】</p>
</li>
<li><p>LR为什么使用Sigmoid，</p>
<p>【数学上：因为LR服从伯努利分布，伯努利分布转化成广义线性模型指数分布族形式里可推导出sigmoid函数；概率上：sigmoid自身性质，值域在（0,1）之间，满足概率的要求】</p>
</li>
<li><p>LR为什么要对连续数值特征进行离散化？</p>
<p>【计算速度快，简单化模型，降低过拟合，可以做特征组合，对异常数据有强鲁棒性】</p>
<p>①离散特征的增加和减少都很容易，易于模型的快速迭代；</p>
<p>②稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</p>
<p>③离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</p>
<p>④逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</p>
<p>⑤离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</p>
<p>⑥特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</p>
<p>⑦特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</p>
<p>李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p>
</li>
<li><p>LR与SVM的联系与区别：</p>
<p>联系：</p>
<p>1、<strong>LR和SVM都可以处理分类问题</strong>，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）</p>
<p>2、<strong>两个方法都可以增加不同的正则化项，</strong>如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。</p>
<p>区别：</p>
<p>1、<strong>LR是参数模型</strong>[逻辑回归是假设y服从Bernoulli分布]，<strong>SVM是非参数模型</strong>，LR对异常值更敏感。</p>
<p>2、<strong>从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss</strong>，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</p>
<p>3、<strong>SVM的处理方法是只考虑support vectors</strong>，也就是和分类最相关的少数点，去学习分类器。<strong>而逻辑回归通过非线性映射</strong>，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</p>
<p>4、<strong>逻辑回归相对来说模型更简单，好理解，</strong>特别是<strong>大规模</strong>线性分类时比较方便。而<strong>SVM的理解和优化相对来说复杂一些</strong>，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</p>
<p>5、logic 能做的 svm能做，但可能在<strong>准确率</strong>上有问题，svm能做的logic有的做不了。</p>
</li>
<li><p>如何选择LR与SVM？</p>
<p>非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。</p>
<p>①如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</p>
<p>②如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</p>
<p>③如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。</p>
<p>模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝</p>
<p>损失函数：SVM hinge loss; LR L2正则化;</p>
<p> adaboost 指数损失数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感</p>
<p>数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核</p>
</li>
<li><p>什么是参数模型（LR）与非参数模型（SVM）？</p>
<p>在统计学中，参数模型通常假设总体（随机变量）服从某一个分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；</p>
<p>非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。</p>
</li>
<li><p>逻辑回归怎么多分类</p>
<p>【把Sigmoid函数换成softmax函数，即可适用于多分类的场景。】</p>
</li>
<li><p>softmax公式，为什么用sigmoid函数进行非线性映射（从二项分布的伯努利方程角度）</p>
</li>
<li><p>Logistic回归能处理浮点数吗？</p>
</li>
</ol>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/熵/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/熵/" itemprop="url">熵</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-20T00:00:00+08:00">
                2023-02-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/" itemprop="url" rel="index">
                    <span itemprop="name">先导知识</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/熵/" itemprop="url" rel="index">
                    <span itemprop="name">熵</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247599777&amp;idx=2&amp;sn=982e734b8de12d67ae64476c66f704bc&amp;chksm=fb54a44dcc232d5ba3fbf8f40c75801b0728beffb1b63f053b0c76209bbf54e47655dd421b25&amp;scene=27" target="_blank" rel="noopener">详解机器学习中的熵、条件熵、相对熵、交叉熵</a></p>
<h1 id="1-信息量："><a href="#1-信息量：" class="headerlink" title="1. 信息量："></a>1. 信息量：</h1><p>一个事件发生的概率越小，信息量越大，所以信息量应该为概率的<strong>减函数</strong>，对于相互独立的两个事有<em>p</em>(<em>xy</em>)=<em>p</em>(<em>x</em>)<em>p</em>(<em>y</em>)，对于这两个事件信息量应满足<em>h</em>(<em>xy</em>)=<em>h</em>(<em>x</em>)+<em>h</em>(<em>y</em>)，那么信息量应为对数函数：</p>
<p>​    h(x) = -ln p(x)</p>
<p>​    对于一个随机变量可以以不同的概率发生，那么通过<strong>信息量期望</strong>的方式衡量，即<strong>信息熵</strong>。 </p>
<h1 id="2-熵"><a href="#2-熵" class="headerlink" title="2. 熵"></a>2. 熵</h1><blockquote>
<p> 熵：信息不确定性度量（信息量与不确定性相关）。事件发生的概率越小则携带的信息越大。</p>
</blockquote>
<p>​    一个离散随机变量X的可能取值为X=x1,x2,…,xn，而对应的概率为pi=p(X=xi),则随机变量的熵定义为： </p>
<p><img src="..\imgs\熵.jpg" alt></p>
<p><img src="..\imgs\熵.jpg" style="zoom:100%"></p>
<p>​    每个<em>xi</em>表示一种特征。 <em>H</em>(<em>X</em>)在每个<em>p</em>(<em>xi</em>) = 1/<em>N</em>是最大，<em>N</em>为信息的个数。在概率为1/<em>N</em>时信息是最不确定的。</p>
<p>​    规定当p(xi)=0时，p(xi)log⁡(p(xi)=0</p>
<p>【小思考：为何公式是这样子的？其实只需熵和概率P成反比，1/P , 但是有个量纲缺点：地震发生的概率很小（P = 1/百万)，则信息量为一百万。抛硬币概率1/2，则信息量为2。两者量纲差太大，取log之后，使得低范围的值稍微放大，高范围的值稍微放小。】</p>
<h1 id="3-联合熵H-p-q"><a href="#3-联合熵H-p-q" class="headerlink" title="3. 联合熵H(p,q)"></a>3. 联合熵H(p,q)</h1><p>两个随机变量的p与q的联合分布形成的熵称为联合熵，记为<em>H</em>(p, q)。 </p>
<p><img src="..\imgs\交叉熵.jpg" style="zoom:100%"></p>
<h1 id="4-条件熵H-q-p"><a href="#4-条件熵H-q-p" class="headerlink" title="4. 条件熵H(q|p)"></a>4. 条件熵H(q|p)</h1><p><em>X</em>给定的条件下，<em>Y</em>的信息熵，即<em>H</em> (<em>Y</em> |X )。公式为：</p>
<p><img src="..\imgs\条件熵.jpg" style="zoom:100%"></p>
<p><strong>推导过程：</strong></p>
<p><img src="..\imgs\条件熵推导过程.jpg" style="zoom:100%"></p>
<h1 id="5-KL散度（相对熵）："><a href="#5-KL散度（相对熵）：" class="headerlink" title="5. KL散度（相对熵）："></a>5. KL散度（相对熵）：</h1><blockquote>
<p> 交叉熵：两个概率分布之间的一个比较，如果两个分布越匹配，交叉熵就越低；如果两个概率分布完全比配，那么交叉熵就为 0。</p>
</blockquote>
<p>如果是两个随机变量P,Q，且其概率分布分别为p(x),q(x),则p相对q的相对熵为：</p>
<p><img src="..\imgs\KL散度.jpg" style="zoom:100%"></p>
<h1 id="6-几种熵之间的关系："><a href="#6-几种熵之间的关系：" class="headerlink" title="6. 几种熵之间的关系："></a>6. 几种熵之间的关系：</h1><p><img src="..\imgs\熵之间的关系.jpg" style="zoom:100%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/MYSQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/MYSQL/" itemprop="url">SQL</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-17T00:00:00+08:00">
                2023-02-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/数据库/" itemprop="url" rel="index">
                    <span itemprop="name">数据库</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/数据库/Mysql/" itemprop="url" rel="index">
                    <span itemprop="name">Mysql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h1><h2 id="1-相关概念："><a href="#1-相关概念：" class="headerlink" title="1. 相关概念："></a>1. 相关概念：</h2><blockquote>
<ul>
<li>sql语句必须以分号结尾</li>
<li>数据库：文件夹。</li>
<li>表：文件夹里的文件。</li>
<li>记录：文件中的一行行数据。</li>
</ul>
</blockquote>
<h2 id="2-数据库"><a href="#2-数据库" class="headerlink" title="2. 数据库"></a>2. 数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">'''基于库的增删改查'''</span><br><span class="line">1.创建库</span><br><span class="line">	<span class="keyword">create</span> <span class="keyword">database</span> 库名;</span><br><span class="line">2.查看库</span><br><span class="line">	<span class="keyword">show</span> <span class="keyword">databases</span>;  查看所有的库名称</span><br><span class="line"> 	<span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">database</span> 库名;  查看指定库信息</span><br><span class="line"> 	<span class="keyword">select</span> <span class="keyword">database</span>(); 查看当前使用的数据库</span><br><span class="line">3.编辑库</span><br><span class="line">	<span class="keyword">alter</span> <span class="keyword">database</span> 库名 <span class="keyword">charset</span>=<span class="string">'utf8'</span>;</span><br><span class="line">4.删除库</span><br><span class="line">	<span class="keyword">drop</span> <span class="keyword">database</span> 库名;</span><br><span class="line">5.查看数据库使用端口</span><br><span class="line">	<span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'port'</span>;</span><br></pre></td></tr></table></figure>
<h2 id="3-表"><a href="#3-表" class="headerlink" title="3. 表"></a>3. 表</h2><h3 id="（1-构成"><a href="#（1-构成" class="headerlink" title="（1)构成"></a>（1)构成</h3><ul>
<li>字段：事物的所有属性</li>
<li>列：事物的某一个属性的集合 </li>
<li>记录：行</li>
<li>主键：区分表里的每一条数据行。唯一标识。</li>
<li>NULL值：</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">操作表之前需要先确定库</span><br><span class="line">	<span class="keyword">create</span> <span class="keyword">database</span> db1;</span><br><span class="line">切换操作库</span><br><span class="line">	<span class="keyword">use</span> db1;</span><br><span class="line">	</span><br><span class="line">1.创建表</span><br><span class="line">	<span class="keyword">create</span> <span class="keyword">table</span> 表名(字段名 字段类型,字段名 字段类型);</span><br><span class="line">2.查看表</span><br><span class="line">	<span class="keyword">show</span> <span class="keyword">tables</span>;  查看库下所有的表名称</span><br><span class="line">	<span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> 表名;  查看指定表信息</span><br><span class="line">    <span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> &lt;表名\G&gt; (加上\G，有较高的易读性)</span><br><span class="line">	<span class="keyword">describe</span> 表名;  查看表结构</span><br><span class="line">	desc 表名;</span><br><span class="line"> 	ps:如果想跨库操作其他表 只需要在表名前加库名即可</span><br><span class="line">        desc mysql.user;</span><br><span class="line">3.编辑表</span><br><span class="line">	<span class="keyword">alter</span> <span class="keyword">table</span> 表名 <span class="keyword">rename</span> 新表名;</span><br><span class="line">4.删除表</span><br><span class="line">	<span class="keyword">drop</span> <span class="keyword">table</span> 表名;</span><br></pre></td></tr></table></figure>
<h2 id="4-记录"><a href="#4-记录" class="headerlink" title="4. 记录"></a>4. 记录</h2><h3 id="（1）数据类型"><a href="#（1）数据类型" class="headerlink" title="（1）数据类型"></a>（1）数据类型</h3><ul>
<li>定长字符串：CHAR(n)，n为最多保存字符数量。如果字符长度是10，而输入的数据只有5位，则用5位空格填充。</li>
<li>变长字符串：VARCHAR(n)， n为最多字符数量</li>
<li>大对象类型：TEXT，大VARCHAR字段，可用于存储大字符集，如JHTML输入。</li>
<li>数值类型：NUMERIC，通用的数值类型</li>
<li>小数类型：DECIMAL(p,s)，p表示数值总位数，s表示小数点后位数。DECIMAL（4，2）:12.449存为12.45。</li>
<li>整数：</li>
<li>浮点数：FLOAT，总位数和小数点位数都可变。</li>
<li>日期和时间：DATE、TIME、DATETIME（YEAR、MONTH、DAY、HOUR、MINUTE、SECOND）、TIMESTAMP</li>
<li>直义字符串：？？？</li>
<li>NULL</li>
<li>布尔值：TRUE、FALSE、NULL</li>
<li>自定义类型：采用语句创建类型。create type PERSON AS OBJECT（NAME VARCHAR（30））</li>
<li>域：有效数据类型的集合。create domain MONEY_D AS NUMBER(8,2) ？？？</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">'''基于记录的增删改查'''</span><br><span class="line">1.插入数据</span><br><span class="line">	<span class="keyword">insert</span> <span class="keyword">into</span> 表名 <span class="keyword">values</span>(数据值<span class="number">1</span>,数据值<span class="number">2</span>);</span><br><span class="line">2.查询数据</span><br><span class="line">	<span class="keyword">select</span> * <span class="keyword">from</span> 表名;  		查询表中所有的数据</span><br><span class="line">3.编辑数据</span><br><span class="line">	<span class="keyword">update</span> 表名 <span class="keyword">set</span> 字段名=新数据 <span class="keyword">where</span> 筛选条件;</span><br><span class="line">4.删除数据</span><br><span class="line">	<span class="keyword">delete</span> <span class="keyword">from</span> 表名;</span><br><span class="line">	<span class="keyword">delete</span> <span class="keyword">from</span> 表名 <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">2</span>;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Lee_yl</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lee_yl</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
