<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Lee_yl&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Lee_yl&#39;s blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lee_yl&#39;s blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>Lee_yl's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lee_yl's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/7.Spark处理不同的数据类型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/7.Spark处理不同的数据类型/" itemprop="url">7. 处理不同的数据类型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-18T00:00:00+08:00">
                2023-03-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="一、处理不同的数据类型"><a href="#一、处理不同的数据类型" class="headerlink" title="一、处理不同的数据类型"></a>一、处理不同的数据类型</h1><h2 id="1-数据类型"><a href="#1-数据类型" class="headerlink" title="1. 数据类型"></a>1. 数据类型</h2><ul>
<li>布尔型</li>
<li>数值型</li>
<li>字符串型</li>
<li>日期和时间戳类型</li>
<li>空值处理</li>
<li>复杂类型</li>
<li>自定义函数</li>
</ul>
<h2 id="2-处理布尔类型"><a href="#2-处理布尔类型" class="headerlink" title="2. 处理布尔类型"></a>2. 处理布尔类型</h2><p>（1）and、or、true 和false</p>
<p>（2）&amp;、|、1、0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.functions <span class="keyword">as</span> F</span><br><span class="line">filter1 = F.col(<span class="string">'aa'</span>) &gt; <span class="number">600</span></span><br><span class="line">filter2 = F.col(<span class="string">'bb'</span>) &gt; <span class="number">2</span></span><br><span class="line">df.where(filter1 &amp; filter2).where(F.col(<span class="string">'cc'</span>).isin(<span class="string">"DOT"</span>)).show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：创建布尔表达式时注意空值处理！！！</p>
<p>将df.where(F.col(‘a’) != ‘hello’)改写成df.where (F.col(‘a’).eqNullSafe(“hello”))可以保证空值安全。</p>
</blockquote>
<h2 id="3-处理数值类型"><a href="#3-处理数值类型" class="headerlink" title="3. 处理数值类型"></a>3. 处理数值类型</h2><ul>
<li>pow：平方</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例子</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pow</span><br><span class="line">f = pow(F.col(<span class="string">'a'</span>) * F.col(<span class="string">'b'</span>), <span class="number">2</span>) + <span class="number">5</span>  <span class="comment"># (a * b)^2 + 5</span></span><br><span class="line">df.select(f.alias(<span class="string">'c'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># selectExpr</span></span><br><span class="line">df.selectExpr(<span class="string">'a'</span>, <span class="string">'pow(F.col('</span>a<span class="string">') * F.col('</span><span class="string">b'), 2) + 5'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>round：向上取整，bround：向下取整</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from pyspark.sql.functions import lit, round, bround</span></span><br><span class="line">df.select(round(lit(<span class="string">"2.5"</span>)), bround(lit(<span class="string">"2.5"</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li>corr：计算两列的相关性</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> corr</span><br><span class="line">df.stat.corr(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line">df.select(corr(<span class="string">'a'</span>, <span class="string">'b'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>describe：计算一列或一组列的汇总统计信息，可以用describe来实现。</li>
<li>count：计数</li>
<li>mean：平均值</li>
<li>stddev_pop：标准差</li>
<li>min：最小值</li>
<li><p>max：最大值</p>
</li>
<li><p>StatFunctions包中封装了许多可使用的统计函数</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">quantileProbs = [<span class="number">0.5</span>]</span><br><span class="line">relError = <span class="number">0.05</span></span><br><span class="line">df.stat.approxQuantile(<span class="string">'a'</span>, quantileProbs, relError)</span><br></pre></td></tr></table></figure>
<ul>
<li>monotonically_increasing_id函数：从0开始，为每行添加一个唯一的ID。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> monotonically_increasing_id</span><br><span class="line">df.select(monotonically_increasing_id())</span><br></pre></td></tr></table></figure>
<h2 id="4-处理字符串类型"><a href="#4-处理字符串类型" class="headerlink" title="4. 处理字符串类型"></a>4. 处理字符串类型</h2><ul>
<li>initcap：将给定字符串中空格分隔的每个单词首字母大写。</li>
<li>lower：全部小写</li>
<li>upper：全部大写</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> initcap</span><br><span class="line">df.select(initcap(F.col(<span class="string">'aaa'</span>))， lower(F.col(<span class="string">'bbb'</span>)), upper(F.col(<span class="string">'ccc'</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li>lpad：从左边对字符串使用指定的字符进行填充。</li>
<li>ltrim：从左边对字符串使用指定的字符进行删除空格。</li>
<li>rpad: 从右边对字符串使用指定的字符进行填充。</li>
<li>trim：从右边对字符串使用指定的字符进行删除空格。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, ltrim, rtim, rpad, lpad, trim</span><br><span class="line">df.select(ltrim(lit(<span class="string">"    HELLO    "</span>)).alias(<span class="string">"ltrim"</span>),  </span><br><span class="line">		  rtrim(lit(<span class="string">"    HELLO    "</span>)).alias(<span class="string">"rtrim"</span>),</span><br><span class="line">		  trim(lit(<span class="string">"    HELLO    "</span>)).alias(<span class="string">"trim"</span>),</span><br><span class="line">		  lpad(lit(<span class="string">"HELLO"</span>, <span class="number">3</span>, <span class="string">" "</span>)).alias(<span class="string">"lpad"</span>),</span><br><span class="line">		  rpad(lit(<span class="string">"HELLO"</span>, <span class="number">10</span>, <span class="string">" "</span>)).alias(<span class="string">"rpad"</span>)</span><br><span class="line">)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">ltrim: "HELLO     "</span></span><br><span class="line"><span class="string">rtrim："     HELLO"</span></span><br><span class="line"><span class="string">trim:"HELLO"</span></span><br><span class="line"><span class="string">lpad："HEL"</span></span><br><span class="line"><span class="string">rpad: "HELLO     "</span></span><br><span class="line"><span class="string">注意 lpad或rpad输入的数值小于字符串长度，它将从字符串的右侧删除字符</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h2 id="5、正则表达式"><a href="#5、正则表达式" class="headerlink" title="5、正则表达式"></a>5、正则表达式</h2><ul>
<li>regexp_extract（列名，正则表达式，第几个）：提取值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_extract</span><br><span class="line">regex_string = <span class="string">"BLACK|WHITE|RED|GREEN|BLUE"</span></span><br><span class="line">df.select(regexp_extract(F.col(<span class="string">'a'</span>, regex_string, <span class="number">1</span>))) <span class="comment"># 将列名为a的字段中出现包含在正则表达式的第一个单词取出来。</span></span><br><span class="line"><span class="comment"># 如 df['a'] = "WHITE HANGING HEA", 结果为WHITE</span></span><br></pre></td></tr></table></figure>
<ul>
<li>regexp_replace：替换值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_replace</span><br><span class="line">regex_string = <span class="string">"BLACK|WHITE|RED|GREEN|BLUE"</span>  <span class="comment"># |在正则中表示或的意思</span></span><br><span class="line">df.select(</span><br><span class="line">	regexp_replace(F.col(<span class="string">'a'</span>), regex_string, <span class="string">'color'</span>).alias(<span class="string">'color_clean'</span>),</span><br><span class="line">) <span class="comment"># 将字段a中包含regex_string这些字段换成color。</span></span><br></pre></td></tr></table></figure>
<ul>
<li>translate：替换，将给定字符串替换掉所有出现的某字符串。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> translate</span><br><span class="line">df.select(translate(F.col(<span class="string">'a'</span>), <span class="string">'LEET'</span>, <span class="string">'1337'</span>)) <span class="comment"># L替换成1， E替换成3， T替换成7.</span></span><br><span class="line"><span class="comment"># 所以WHITE 会被替换成WHI73.</span></span><br></pre></td></tr></table></figure>
<ul>
<li>instr：是否存在（类似contains)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> instr</span><br><span class="line">containsBlack = instr(F.col(<span class="string">'a'</span>), <span class="string">'BLACK'</span>) &gt;= <span class="number">1</span></span><br><span class="line">df.withColumn(<span class="string">'b'</span>, containsBlack)</span><br></pre></td></tr></table></figure>
<ul>
<li>locate(substr, str, pos=1)：在位置 pos 之后定位字符串列中第一次出现 substr 的位置。<ul>
<li>该位置不是基于零的，而是基于 1 的索引。如果在 str 中找不到 substr，则返回 0。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame([(<span class="string">'abcd'</span>,)], [<span class="string">'s'</span>,])</span><br><span class="line">df.select(locate(<span class="string">'b'</span>, df.s, <span class="number">1</span>).alias(<span class="string">'s'</span>)).collect()</span><br><span class="line">[Row(s=<span class="number">2</span>)]</span><br></pre></td></tr></table></figure>
<h2 id="6、处理日期和时间戳类型"><a href="#6、处理日期和时间戳类型" class="headerlink" title="6、处理日期和时间戳类型"></a>6、处理日期和时间戳类型</h2><p><a href="https://zhuanlan.zhihu.com/p/450636026" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/450636026</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<h3 id="（1）-示例数据"><a href="#（1）-示例数据" class="headerlink" title="（1） 示例数据"></a>（1） 示例数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data=[[<span class="string">"1"</span>,<span class="string">"2020-02-01"</span>],[<span class="string">"2"</span>,<span class="string">"2019-03-01"</span>],[<span class="string">"3"</span>,<span class="string">"2021-03-01"</span>]]</span><br><span class="line">df=spark.createDataFrame(data, [<span class="string">"id"</span>,<span class="string">"time"</span>])</span><br><span class="line">df.show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+---+----------+</span><br><span class="line">| id|      time|</span><br><span class="line">+---+----------+</span><br><span class="line">|  <span class="number">1</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">|  <span class="number">3</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">+---+----------+</span><br></pre></td></tr></table></figure>
<h3 id="（2）-日期"><a href="#（2）-日期" class="headerlink" title="（2）. 日期"></a>（2）. 日期</h3><h4 id="2-1-当前日期-current-date"><a href="#2-1-当前日期-current-date" class="headerlink" title="2.1 当前日期 current_date()"></a>2.1 当前日期 <code>current_date()</code></h4><ul>
<li>获取当前系统日期。默认情况下，数据将以<code>yyyy-dd-mm</code>格式返回。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.current_date().alias(<span class="string">"current_date"</span>)).show(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+------------+</span><br><span class="line">|current_date|</span><br><span class="line">+------------+</span><br><span class="line">|  <span class="number">2021</span><span class="number">-12</span><span class="number">-28</span>|</span><br><span class="line">+------------+</span><br><span class="line">only showing top <span class="number">1</span> row</span><br></pre></td></tr></table></figure>
<h4 id="2-2-日期格式-date-format"><a href="#2-2-日期格式-date-format" class="headerlink" title="2.2 日期格式 date_format()"></a>2.2 日期格式 <code>date_format()</code></h4><ul>
<li>解析日期并转换<code>yyyy-dd-mm</code>为<code>MM-dd-yyyy</code>格式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.date_format(F.col(<span class="string">"time"</span>), <span class="string">"MM-dd-yyyy"</span>).alias(<span class="string">"date_format"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-----------+</span><br><span class="line">|      time|date_format|</span><br><span class="line">+----------+-----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>| <span class="number">02</span><span class="number">-01</span><span class="number">-2020</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">03</span><span class="number">-01</span><span class="number">-2019</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">03</span><span class="number">-01</span><span class="number">-2021</span>|</span><br><span class="line">+----------+-----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-3-使用to-date-将日期格式字符串yyyy-MM-dd转换为DateType-yyyy-MM-dd"><a href="#2-3-使用to-date-将日期格式字符串yyyy-MM-dd转换为DateType-yyyy-MM-dd" class="headerlink" title="2.3 使用to_date()将日期格式字符串yyyy-MM-dd转换为DateType yyyy-MM-dd"></a>2.3 使用<code>to_date()</code>将日期格式字符串<code>yyyy-MM-dd</code>转换为<code>DateType yyyy-MM-dd</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.to_date(F.col(<span class="string">"time"</span>), <span class="string">"yyy-MM-dd"</span>).alias(<span class="string">"to_date"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----------+</span><br><span class="line">|      time|   to_date|</span><br><span class="line">+----------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">+----------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-4-两个日期之间的日差datediff"><a href="#2-4-两个日期之间的日差datediff" class="headerlink" title="2.4 两个日期之间的日差datediff()"></a>2.4 两个日期之间的日差<code>datediff()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.datediff(F.current_date(), F.col(<span class="string">"time"</span>)).alias(<span class="string">"datediff"</span>)  </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+--------+</span><br><span class="line">|      time|datediff|</span><br><span class="line">+----------+--------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|     <span class="number">696</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|    <span class="number">1033</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|     <span class="number">302</span>|</span><br><span class="line">+----------+--------+</span><br></pre></td></tr></table></figure>
<h4 id="2-5-两个日期之间的月份months-between"><a href="#2-5-两个日期之间的月份months-between" class="headerlink" title="2.5 两个日期之间的月份months_between()"></a>2.5 两个日期之间的月份<code>months_between()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.months_between(F.current_date(),F.col(<span class="string">"time"</span>)).alias(<span class="string">"months_between"</span>)  </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+--------------+</span><br><span class="line">|      time|months_between|</span><br><span class="line">+----------+--------------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|   <span class="number">22.87096774</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|   <span class="number">33.87096774</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|    <span class="number">9.87096774</span>|</span><br><span class="line">+----------+--------------+</span><br></pre></td></tr></table></figure>
<h4 id="2-6-截断指定单位的日期trunc"><a href="#2-6-截断指定单位的日期trunc" class="headerlink" title="2.6 截断指定单位的日期trunc()"></a>2.6 截断指定单位的日期<code>trunc()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.trunc(F.col(<span class="string">"time"</span>),<span class="string">"Month"</span>).alias(<span class="string">"Month_Trunc"</span>), </span><br><span class="line">    F.trunc(F.col(<span class="string">"time"</span>),<span class="string">"Year"</span>).alias(<span class="string">"Month_Year"</span>), </span><br><span class="line">    F.trunc(F.col(<span class="string">"time"</span>),<span class="string">"Month"</span>).alias(<span class="string">"Month_Trunc"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-----------+----------+-----------+</span><br><span class="line">|      time|Month_Trunc|Month_Year|Month_Trunc|</span><br><span class="line">+----------+-----------+----------+-----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>| <span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-01</span><span class="number">-01</span>| <span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-01</span><span class="number">-01</span>| <span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-01</span><span class="number">-01</span>| <span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">+----------+-----------+----------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># pyspark.sql.functions.date_trunc(format, timestamp)</span></span><br><span class="line"><span class="comment"># 返回截断为格式指定单位的时间戳。</span></span><br><span class="line"><span class="comment"># 2.3.0 版中的新函数。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = spark.createDataFrame([(<span class="string">'1997-02-28 05:02:11'</span>,)], [<span class="string">'t'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(date_trunc(<span class="string">'year'</span>, df.t).alias(<span class="string">'year'</span>)).collect()</span><br><span class="line">[Row(year=datetime.datetime(<span class="number">1997</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(date_trunc(<span class="string">'mon'</span>, df.t).alias(<span class="string">'month'</span>)).collect()</span><br><span class="line">[Row(month=datetime.datetime(<span class="number">1997</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))]</span><br></pre></td></tr></table></figure>
<h4 id="2-7-月、日加减法"><a href="#2-7-月、日加减法" class="headerlink" title="2.7 月、日加减法"></a>2.7 月、日加减法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.add_months(F.col(<span class="string">"time"</span>),<span class="number">3</span>).alias(<span class="string">"add_months"</span>), </span><br><span class="line">    F.add_months(F.col(<span class="string">"time"</span>),<span class="number">-3</span>).alias(<span class="string">"sub_months"</span>), </span><br><span class="line">    F.date_add(F.col(<span class="string">"time"</span>),<span class="number">4</span>).alias(<span class="string">"date_add"</span>), </span><br><span class="line">    F.date_sub(F.col(<span class="string">"time"</span>),<span class="number">4</span>).alias(<span class="string">"date_sub"</span>) </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----------+----------+----------+----------+</span><br><span class="line">|      time|add_months|sub_months|  date_add|  date_sub|</span><br><span class="line">+----------+----------+----------+----------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-05</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-11</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-05</span>|<span class="number">2020</span><span class="number">-01</span><span class="number">-28</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-06</span><span class="number">-01</span>|<span class="number">2018</span><span class="number">-12</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-05</span>|<span class="number">2019</span><span class="number">-02</span><span class="number">-25</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-06</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-12</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-05</span>|<span class="number">2021</span><span class="number">-02</span><span class="number">-25</span>|</span><br><span class="line">+----------+----------+----------+----------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-8-年、月、下一天、一年中第几个星期"><a href="#2-8-年、月、下一天、一年中第几个星期" class="headerlink" title="2.8 年、月、下一天、一年中第几个星期"></a>2.8 年、月、下一天、一年中第几个星期</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">     F.year(F.col(<span class="string">"time"</span>)).alias(<span class="string">"year"</span>), </span><br><span class="line">     F.month(F.col(<span class="string">"time"</span>)).alias(<span class="string">"month"</span>), </span><br><span class="line">     F.next_day(F.col(<span class="string">"time"</span>),<span class="string">"Sunday"</span>).alias(<span class="string">"next_day"</span>), </span><br><span class="line">     F.weekofyear(F.col(<span class="string">"time"</span>)).alias(<span class="string">"weekofyear"</span>) </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----+-----+----------+----------+</span><br><span class="line">|      time|year|month|  next_day|weekofyear|</span><br><span class="line">+----------+----+-----+----------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span>|    <span class="number">2</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>|         <span class="number">5</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span>|    <span class="number">3</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-03</span>|         <span class="number">9</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span>|    <span class="number">3</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-07</span>|         <span class="number">9</span>|</span><br><span class="line">+----------+----+-----+----------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-9-星期几、月日、年日"><a href="#2-9-星期几、月日、年日" class="headerlink" title="2.9 星期几、月日、年日"></a>2.9 星期几、月日、年日</h4><ul>
<li>查询星期几</li>
<li>一个月中的第几天</li>
<li>一年中的第几天</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>),  </span><br><span class="line">     F.dayofweek(F.col(<span class="string">"time"</span>)).alias(<span class="string">"dayofweek"</span>), </span><br><span class="line">     F.dayofmonth(F.col(<span class="string">"time"</span>)).alias(<span class="string">"dayofmonth"</span>), </span><br><span class="line">     F.dayofyear(F.col(<span class="string">"time"</span>)).alias(<span class="string">"dayofyear"</span>), </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+---------+----------+---------+</span><br><span class="line">|      time|dayofweek|dayofmonth|dayofyear|</span><br><span class="line">+----------+---------+----------+---------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|        <span class="number">7</span>|         <span class="number">1</span>|       <span class="number">32</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|        <span class="number">6</span>|         <span class="number">1</span>|       <span class="number">60</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|        <span class="number">2</span>|         <span class="number">1</span>|       <span class="number">60</span>|</span><br><span class="line">+----------+---------+----------+---------+</span><br></pre></td></tr></table></figure>
<h3 id="（3）-时间"><a href="#（3）-时间" class="headerlink" title="（3）. 时间"></a>（3）. 时间</h3><h4 id="3-1-创建一个测试数据"><a href="#3-1-创建一个测试数据" class="headerlink" title="3.1 创建一个测试数据"></a>3.1 创建一个测试数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data=[</span><br><span class="line">    [<span class="string">"1"</span>,<span class="string">"02-01-2020 11 01 19 06"</span>],</span><br><span class="line">    [<span class="string">"2"</span>,<span class="string">"03-01-2019 12 01 19 406"</span>],</span><br><span class="line">    [<span class="string">"3"</span>,<span class="string">"03-01-2021 12 01 19 406"</span>]]</span><br><span class="line">df2=spark.createDataFrame(data,[<span class="string">"id"</span>,<span class="string">"time"</span>])</span><br><span class="line">df2.show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+---+-----------------------+</span><br><span class="line">|id |time                   |</span><br><span class="line">+---+-----------------------+</span><br><span class="line">|<span class="number">1</span>  |<span class="number">02</span><span class="number">-01</span><span class="number">-2020</span> <span class="number">11</span> <span class="number">01</span> <span class="number">19</span> <span class="number">06</span> |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">03</span><span class="number">-01</span><span class="number">-2019</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|</span><br><span class="line">|<span class="number">3</span>  |<span class="number">03</span><span class="number">-01</span><span class="number">-2021</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|</span><br><span class="line">+---+-----------------------+</span><br></pre></td></tr></table></figure>
<h4 id="3-2-以-spark-默认格式yyyy-MM-dd-HH-mm-ss返回当前时间戳"><a href="#3-2-以-spark-默认格式yyyy-MM-dd-HH-mm-ss返回当前时间戳" class="headerlink" title="3.2 以 spark 默认格式yyyy-MM-dd HH:mm:ss返回当前时间戳"></a>3.2 以 spark 默认格式<code>yyyy-MM-dd HH:mm:ss</code>返回当前时间戳</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">df2.select(F.current_timestamp().alias(<span class="string">"current_timestamp"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+--------------------+</span><br><span class="line">|   current_timestamp|</span><br><span class="line">+--------------------+</span><br><span class="line">|<span class="number">2021</span><span class="number">-12</span><span class="number">-28</span> <span class="number">09</span>:<span class="number">31</span>:...|</span><br><span class="line">|<span class="number">2021</span><span class="number">-12</span><span class="number">-28</span> <span class="number">09</span>:<span class="number">31</span>:...|</span><br><span class="line">|<span class="number">2021</span><span class="number">-12</span><span class="number">-28</span> <span class="number">09</span>:<span class="number">31</span>:...|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
<h4 id="3-3-将字符串时间戳转换为时间戳类型格式-to-timestamp"><a href="#3-3-将字符串时间戳转换为时间戳类型格式-to-timestamp" class="headerlink" title="3.3 将字符串时间戳转换为时间戳类型格式 to_timestamp()"></a>3.3 将字符串时间戳转换为时间戳类型格式 <code>to_timestamp()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df2.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.to_timestamp(F.col(<span class="string">"time"</span>), <span class="string">"MM-dd-yyyy HH mm ss SSS"</span>).alias(<span class="string">"to_timestamp"</span>) </span><br><span class="line">    ).show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-----------------------+-----------------------+</span><br><span class="line">|time                   |to_timestamp           |</span><br><span class="line">+-----------------------+-----------------------+</span><br><span class="line">|<span class="number">02</span><span class="number">-01</span><span class="number">-2020</span> <span class="number">11</span> <span class="number">01</span> <span class="number">19</span> <span class="number">06</span> |<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span> <span class="number">11</span>:<span class="number">01</span>:<span class="number">19.06</span> |</span><br><span class="line">|<span class="number">03</span><span class="number">-01</span><span class="number">-2019</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|</span><br><span class="line">|<span class="number">03</span><span class="number">-01</span><span class="number">-2021</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|</span><br><span class="line">+-----------------------+-----------------------+</span><br></pre></td></tr></table></figure>
<h4 id="3-4-获取小时、分钟、秒"><a href="#3-4-获取小时、分钟、秒" class="headerlink" title="3.4 获取小时、分钟、秒"></a>3.4 获取<code>小时</code>、<code>分钟</code>、<code>秒</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">data=[</span><br><span class="line">    [<span class="string">"1"</span>,<span class="string">"2020-02-01 11:01:19.06"</span>],</span><br><span class="line">    [<span class="string">"2"</span>,<span class="string">"2019-03-01 12:01:19.406"</span>],</span><br><span class="line">    [<span class="string">"3"</span>,<span class="string">"2021-03-01 12:01:19.406"</span>]]</span><br><span class="line">df3=spark.createDataFrame(data,[<span class="string">"id"</span>,<span class="string">"time"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取小时、分钟、秒</span></span><br><span class="line">df3.select(</span><br><span class="line">    F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.hour(F.col(<span class="string">"time"</span>)).alias(<span class="string">"hour"</span>), </span><br><span class="line">    F.minute(F.col(<span class="string">"time"</span>)).alias(<span class="string">"minute"</span>),</span><br><span class="line">    F.second(F.col(<span class="string">"time"</span>)).alias(<span class="string">"second"</span>) </span><br><span class="line">    ).show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-----------------------+----+------+------+</span><br><span class="line">|time                   |hour|minute|second|</span><br><span class="line">+-----------------------+----+------+------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span> <span class="number">11</span>:<span class="number">01</span>:<span class="number">19.06</span> |<span class="number">11</span>  |<span class="number">1</span>     |<span class="number">19</span>    |</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|<span class="number">12</span>  |<span class="number">1</span>     |<span class="number">19</span>    |</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|<span class="number">12</span>  |<span class="number">1</span>     |<span class="number">19</span>    |</span><br><span class="line">+-----------------------+----+------+------+</span><br></pre></td></tr></table></figure>
<h2 id="7-处理数据中的空值"><a href="#7-处理数据中的空值" class="headerlink" title="7. 处理数据中的空值"></a>7. 处理数据中的空值</h2><h2 id="8-复杂类型"><a href="#8-复杂类型" class="headerlink" title="8. 复杂类型"></a>8. 复杂类型</h2><ul>
<li>结构体: struct</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> struct</span><br><span class="line">df1 = df.select(struct(<span class="string">'a'</span>, <span class="string">'b'</span>).alias(<span class="string">'c'</span>))</span><br><span class="line"><span class="comment"># 可以通过"."来访问或列方法getField来实现：</span></span><br><span class="line">df1.select(<span class="string">"c.a"</span>)</span><br><span class="line">df1.select(F.col(<span class="string">"c"</span>).getField(<span class="string">"a"</span>))</span><br><span class="line"><span class="comment"># 可以通过 ".*"来查询结构体中所哟就值</span></span><br><span class="line">df1.select(<span class="string">"c.*"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>数组<ul>
<li>split：指定分隔符</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split</span><br><span class="line">df.select(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>)) </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">列名：split(a,)</span></span><br><span class="line"><span class="string">结果：[WHITE, HANGING, ...]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df.select(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>).alias(<span class="string">"array_col"</span>)) </span><br><span class="line">.selectExpr(<span class="string">"array_col[0]"</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">结果</span></span><br><span class="line"><span class="string">列名：array_col[0]</span></span><br><span class="line"><span class="string">结果：WHITE</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>​    数组长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> size</span><br><span class="line">df.select(size(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>array_contains：数组是否包含某个值
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> array_contains</span><br><span class="line">df.select(array_contains(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>), <span class="string">"WHITE"</span>))</span><br><span class="line"><span class="comment"># 结果为true</span></span><br></pre></td></tr></table></figure>
<p>​    explode：一行拆分成多行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">"sub_str"</span>, explode(split(df[<span class="string">"str_col"</span>], <span class="string">"_"</span>))) </span><br><span class="line"><span class="comment"># 将str_col按-拆分成list，list中的每一个元素成为sub_str,与原行中的其他列一起组成新的行</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">eg:</span></span><br><span class="line"><span class="string">"hello world","other column"</span></span><br><span class="line"><span class="string">split ===&gt; ["hello", "world"], "other column"</span></span><br><span class="line"><span class="string">explode ===&gt; </span></span><br><span class="line"><span class="string">		"hello", "other column"; </span></span><br><span class="line"><span class="string">		"world", "other column"</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>Map</p>
<ul>
<li>create_map：键值对</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split</span><br><span class="line">df.select(create_map(F.col(<span class="string">"a"</span>), F.col(<span class="string">"b"</span>)).alias(<span class="string">"c_map"</span>))</span><br></pre></td></tr></table></figure>
<p>​    根据key值取value</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split</span><br><span class="line">df.select(create_map(F.col(<span class="string">"a"</span>), F.col(<span class="string">"b"</span>)).alias(<span class="string">"c_map"</span>))\</span><br><span class="line">.selectExpt(<span class="string">"c_map['WHILE METAL LANTERN']"</span>)</span><br></pre></td></tr></table></figure>
<p>​    展开map类型，将其转换成列:explode</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.select(create_map(F.col(<span class="string">"a"</span>), F.col(<span class="string">"b"</span>)).alias(<span class="string">"c_map"</span>))\</span><br><span class="line">.selectExpt(<span class="string">"explod('c_map')"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="8、处理Json类型"><a href="#8、处理Json类型" class="headerlink" title="8、处理Json类型"></a>8、处理Json类型</h2><h3 id="（1）创建一个Json类型的列："><a href="#（1）创建一个Json类型的列：" class="headerlink" title="（1）创建一个Json类型的列："></a>（1）创建一个Json类型的列：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jsonDF = spark.range(<span class="number">1</span>).selectExpr(<span class="string">"""</span></span><br><span class="line"><span class="string">	'&#123;</span></span><br><span class="line"><span class="string">        "a": &#123;</span></span><br><span class="line"><span class="string">            "aa": [1,2,3]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">	&#125;' as jsonString</span></span><br><span class="line"><span class="string">"""</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-get-json-object：查询JSON对象"><a href="#2-get-json-object：查询JSON对象" class="headerlink" title="(2) get_json_object：查询JSON对象"></a>(2) get_json_object：查询JSON对象</h3><p>若此查询的JSON对象仅有一层嵌套，则可使用json_tuple</p>
<h3 id="3）to-json：将StructType转换成JSON字符串"><a href="#3）to-json：将StructType转换成JSON字符串" class="headerlink" title="(3）to_json：将StructType转换成JSON字符串"></a>(3）to_json：将StructType转换成JSON字符串</h3><h3 id="4-from-json：解析JSON数据，需指定模式"><a href="#4-from-json：解析JSON数据，需指定模式" class="headerlink" title="(4) from_json：解析JSON数据，需指定模式"></a>(4) from_json：解析JSON数据，需指定模式</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Spark的Join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark的Join/" itemprop="url">Join</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-16T00:00:00+08:00">
                2023-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="PySpark的Join"><a href="#PySpark的Join" class="headerlink" title="PySpark的Join"></a>PySpark的Join</h1><p><a href="https://zhuanlan.zhihu.com/p/344080090" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/344080090</a></p>
<h2 id="一、Join方式："><a href="#一、Join方式：" class="headerlink" title="一、Join方式："></a>一、Join方式：</h2><p>pyspark主要分为以下几种join方式：</p>
<ul>
<li><strong>Inner joins</strong> （内连接)： 两边都有的保持</li>
<li><strong>Outer joins</strong> (外连接)：两边任意一边有的保持</li>
<li><strong>Left outer joins</strong> (左外连接)：只保留左边有的records</li>
<li><strong>Right outer joins</strong> (右外连接)：只保留右边有的records</li>
<li><strong>Left semi joins</strong> (左半连接)：只保留在右边记录里出现的左边的records</li>
<li><strong>Left anti joins</strong> (左反连接)：只保留没出现在右边记录里的左边records（可以用来做过滤）</li>
<li><strong>natural join</strong>（自然连接）：通过隐式匹配两个数据集之间具有相同名称的列来执行连接）</li>
<li><strong>cross join</strong>（笛卡尔连接）：将左侧数据集的每一行与右侧数据集中的每一行匹配，结果行数很多。</li>
</ul>
<p>##二、使用方式和例子</p>
<p>下面造个数据集来看看这些join的例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">person = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Bill Chambers"</span>, <span class="number">0</span>, [<span class="number">100</span>]),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"Matei Zaharia"</span>, <span class="number">1</span>, [<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Michael Armbrust"</span>, <span class="number">1</span>, [<span class="number">250</span>, <span class="number">100</span>])])\</span><br><span class="line">  .toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"graduate_program"</span>, <span class="string">"spark_status"</span>)</span><br><span class="line">graduateProgram = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Masters"</span>, <span class="string">"School of Information"</span>, <span class="string">"UC Berkeley"</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Masters"</span>, <span class="string">"EECS"</span>, <span class="string">"UC Berkeley"</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"Ph.D."</span>, <span class="string">"EECS"</span>, <span class="string">"UC Berkeley"</span>)])\</span><br><span class="line">  .toDF(<span class="string">"id"</span>, <span class="string">"degree"</span>, <span class="string">"department"</span>, <span class="string">"school"</span>)</span><br><span class="line">sparkStatus = spark.createDataFrame([</span><br><span class="line">    (<span class="number">500</span>, <span class="string">"Vice President"</span>),</span><br><span class="line">    (<span class="number">250</span>, <span class="string">"PMC Member"</span>),</span><br><span class="line">    (<span class="number">100</span>, <span class="string">"Contributor"</span>)])\</span><br><span class="line">  .toDF(<span class="string">"id"</span>, <span class="string">"status"</span>)</span><br></pre></td></tr></table></figure>
<p>Inner Joins</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">joinExpression = person[<span class="string">"graduate_program"</span>] == graduateProgram[<span class="string">'id'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># in Python</span></span><br><span class="line">wrongJoinExpression = person[<span class="string">"name"</span>] == graduateProgram[<span class="string">"school"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># default so no need to specify</span></span><br><span class="line">person.join(graduateProgram, joinExpression).show()</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"></span><br><span class="line">+---+----------------+----------------+---------------+---+-------+----------+---</span><br><span class="line">| id|            name|graduate_program|   spark_status| id| degree|department|...</span><br><span class="line">+---+----------------+----------------+---------------+---+-------+----------+---</span><br><span class="line">|  <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|          [<span class="number">100</span>]|  <span class="number">0</span>|Masters| School...|...</span><br><span class="line">|  <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|[<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|      EECS|...</span><br><span class="line">|  <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|     [<span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|      EECS|...</span><br><span class="line">+---+----------------+----------------+---------------+---+-------+----------+---</span><br></pre></td></tr></table></figure>
<p><strong>Outer Joins</strong></p>
<blockquote>
<p>Outerjoins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. If there is no equivalent row in either the left or right DataFrame, Spark will insert<code>null</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"outer"</span></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+----+----------------+----------------+---------------+---+-------+-------------</span><br><span class="line">|  id|            name|graduate_program|   spark_status| id| degree| departmen...</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+-------------</span><br><span class="line">|   <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|[<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|       EEC...</span><br><span class="line">|   <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|     [<span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|       EEC...</span><br><span class="line">|null|            null|            null|           null|  <span class="number">2</span>|Masters|       EEC...</span><br><span class="line">|   <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|          [<span class="number">100</span>]|  <span class="number">0</span>|Masters|    School...</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+-------------</span><br></pre></td></tr></table></figure>
<p><strong>Left Outer Joins</strong></p>
<blockquote>
<p>Leftouter joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert<code>null</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"left_outer"</span></span><br><span class="line"></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+---+-------+----------+-----------+----+----------------+----------------+---</span><br><span class="line">| id| degree|department|     school|  id|            name|graduate_program|...</span><br><span class="line">+---+-------+----------+-----------+----+----------------+----------------+---</span><br><span class="line">|  <span class="number">0</span>|Masters| School...|UC Berkeley|   <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|...</span><br><span class="line">|  <span class="number">2</span>|Masters|      EECS|UC Berkeley|null|            null|            null|...</span><br><span class="line">|  <span class="number">1</span>|  Ph.D.|      EECS|UC Berkeley|   <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|...</span><br><span class="line">|  <span class="number">1</span>|  Ph.D.|      EECS|UC Berkeley|   <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|...</span><br><span class="line">+---+-------+----------+-----------+----+----------------+----------------+---</span><br></pre></td></tr></table></figure>
<p><strong>Right Outer Joins</strong></p>
<blockquote>
<p>Rightouter joins evaluate the keys in both of the DataFrames or tables and includes all rows from the right DataFrame as well as any rows in the left DataFrame that have a match in the right DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert<code>null</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"right_outer"</span></span><br><span class="line"></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+----+----------------+----------------+---------------+---+-------+------------+</span><br><span class="line">|  id|            name|graduate_program|   spark_status| id| degree|  department|</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+------------+</span><br><span class="line">|   <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|          [<span class="number">100</span>]|  <span class="number">0</span>|Masters|School of...|</span><br><span class="line">|null|            null|            null|           null|  <span class="number">2</span>|Masters|        EECS|</span><br><span class="line">|   <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|     [<span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|        EECS|</span><br><span class="line">|   <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|[<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|        EECS|</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+------------+</span><br></pre></td></tr></table></figure>
<p><strong>Left Semi Joins 用作数据筛选（include 方式）</strong></p>
<blockquote>
<p>Semijoins are a bit of a departure from the other joins. They do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame. Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"left_semi"</span></span><br><span class="line"></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+---+-------+--------------------+-----------+</span><br><span class="line">| id| degree|          department|     school|</span><br><span class="line">+---+-------+--------------------+-----------+</span><br><span class="line">|  <span class="number">0</span>|Masters|School of Informa...|UC Berkeley|</span><br><span class="line">|  <span class="number">1</span>|  Ph.D.|                EECS|UC Berkeley|</span><br><span class="line">+---+-------+--------------------+-----------+</span><br></pre></td></tr></table></figure>
<p><strong>Left Anti Joins 用作数据筛选（exclude的方式）</strong></p>
<blockquote>
<p>Leftanti joins are the opposite of left semi joins. Like left semi joins, they do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that<em>do not</em>have a corresponding key in the second DataFrame. Think of anti joins as a<code>NOT IN</code>SQL-style filter</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"left_anti"</span></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+---+-------+----------+-----------+</span><br><span class="line">| id| degree|department|     school|</span><br><span class="line">+---+-------+----------+-----------+</span><br><span class="line">|  <span class="number">2</span>|Masters|      EECS|UC Berkeley|</span><br><span class="line">+---+-------+----------+-----------+</span><br></pre></td></tr></table></figure>
<h2 id="三、常见问题和解决方案"><a href="#三、常见问题和解决方案" class="headerlink" title="三、常见问题和解决方案"></a>三、常见问题和解决方案</h2><h3 id="1-处理重复列名：两张表如果存在相同列名？？"><a href="#1-处理重复列名：两张表如果存在相同列名？？" class="headerlink" title="1. 处理重复列名：两张表如果存在相同列名？？"></a>1. 处理重复列名：两张表如果存在相同列名？？</h3><p>方法1：采用不同的连接表达式：</p>
<p>​    当有两个同名的键时，将连接表达式从布尔表达式更改为字符串或序列，这会在连接过程中自动删除其中一个列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.join(b, <span class="string">'id'</span>) <span class="comment"># 不写成a.join(b , a['id']==b['id'])</span></span><br></pre></td></tr></table></figure>
<p>方法2：连接后删除列，采用drop</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.join(b, a[<span class="string">'id'</span>]==b[<span class="string">'id'</span>]).drop(a.id)</span><br></pre></td></tr></table></figure>
<p>方法3：在连接前重命名列,采用withColumnRenamed</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = a.withColumnRenamed(<span class="string">'id1'</span>, F.col(<span class="string">'id'</span>))</span><br><span class="line">a.join(b, a.id1=b.id)</span><br></pre></td></tr></table></figure>
<h1 id="四、Spark如何执行连接"><a href="#四、Spark如何执行连接" class="headerlink" title="四、Spark如何执行连接"></a>四、Spark如何执行连接</h1><p><strong>两个核心模块</strong>：点对点通信模式和逐点计算模式</p>
<ul>
<li><p>大表和大表的连接</p>
<ul>
<li>shuffle join ：每个节点都与所有其他节点进行通信，并根据哪个节点具有某些键来共享数据。由于网络会因通信量而阻塞，所以这种方式很耗时，特殊是如果数据没有合理分区的情况下。</li>
</ul>
</li>
<li><p>大表与小表连接</p>
<ul>
<li><p>broadcast join：当表的大小足够小以便能够放入单个节点内存中且还有空闲空间的时候，可优化join。</p>
<p>把数据量较小的DataFrame复制到集群中的所有工作节点上，只需在开始时执行一次，然后让每个工作节点独立执行作业，而无需等待其他工作节点，也无需与其他工作节点通信。</p>
<p><img src="..\imgs\Spark_broadcast.jpg" alt></p>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/广播变量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/广播变量/" itemprop="url">广播变量</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-14T00:00:00+08:00">
                2023-03-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p>[TOC]</p>
<p>如果想在节点之间共享一份变量，spark提供了两种特定的共享变量，来完成节点之间的变量共享。</p>
<p>（1）广播变量（2）累加器</p>
<h1 id="一、广播变量"><a href="#一、广播变量" class="headerlink" title="一、广播变量"></a>一、广播变量</h1><h2 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h2><p>广播变量允许程序员缓存一个<strong>只读的变量</strong>在每台机器上(worker)，而不是每个任务(task)保存一个拷贝。例如，利用广播变量，我们能够以一种更有效率的方式将一个大数据量输入集合的副本分配给每个节点。</p>
<p>广播变量用于跨所有节点保存数据副本。 此变量缓存在所有计算机上，<strong>而不是在具有任务的计算机上发送。</strong></p>
<p>一个广播变量可以通过调用<strong>SparkContext.broadcast(v</strong>)方法从一个初始变量v中创建。广播变量是v的一个包装变量，它的值可以通过value方法访问。</p>
<p>用途：比如一个配置文件，可以共享给所有节点。比如一个Node的计算结果需要共享给其他节点。</p>
<ul>
<li>可以通过广播变量, 通知当前worker上所有的task, 来共享这个数据,避免数据的多次复制,可以大大降低内存的开销</li>
<li>sparkContext.broadcast(要共享的数据)</li>
</ul>
<h2 id="声明：broadcast"><a href="#声明：broadcast" class="headerlink" title="声明：broadcast"></a>声明：broadcast</h2><p>调用broadcast，Scala中一切可序列化的对象都可以进行广播。</p>
<p>sc.broadcast(xxx)</p>
<h2 id="引用广播变量数据：value"><a href="#引用广播变量数据：value" class="headerlink" title="引用广播变量数据：value"></a>引用广播变量数据：value</h2><p>可在各个计算节点中通过 bc.value来引用广播的数据。</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012131609444-530646160.png" alt="img"></p>
<h2 id="更新广播变量：unpersist"><a href="#更新广播变量：unpersist" class="headerlink" title="更新广播变量：unpersist"></a>更新广播变量：unpersist</h2><p>由于广播变量是只读的，即广播出去的变量没法再修改，</p>
<p>利用unpersist函数将老的广播变量删除，然后重新广播一遍新的广播变量。</p>
<p>bc.unpersist()</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012131623395-1393879813.png" alt="img"></p>
<h2 id="销毁广播变量：destroy"><a href="#销毁广播变量：destroy" class="headerlink" title="销毁广播变量：destroy"></a>销毁广播变量：destroy</h2><p>bc.destroy()可将广播变量的数据和元数据一同销毁，销毁之后就不能再使用了。</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012131656566-939265172.png" alt="img"></p>
<h1 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h1><h2 id="概念：-1"><a href="#概念：-1" class="headerlink" title="概念："></a>概念：</h2><p>累加器是一种只能利用关联操作做“加”操作的变数，因此他能够快速执行并行操作。而且其能够操作counters和sums。Spark原本支援数值类型的累加器，程序员可以自行增加可被支援的类型。如果建立一个具体的累加器，其可在spark UI上显示。</p>
<p><img src="..\imgs\Spark累加器.jpg" alt></p>
<h2 id="用途："><a href="#用途：" class="headerlink" title="用途："></a>用途：</h2><p>对信息进行聚合，累加器的一个常见的用途是在调试时对作业的执行过程中事件进行计数。</p>
<h2 id="创建累加器：accumulator"><a href="#创建累加器：accumulator" class="headerlink" title="创建累加器：accumulator"></a>创建累加器：accumulator</h2><p>调用SparkContext.accumulator(v)方法从一个初始变量v中创建。</p>
<p>运行在集群上的任务可以通过add方法或者使用+=操作来给它加值。然而，它们无法读取这个值。和广播变量相反，累加器是一种add only的变项。</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012134901872-1936239419.png" alt="img"></p>
<h2 id="累加器的陷阱"><a href="#累加器的陷阱" class="headerlink" title="累加器的陷阱"></a>累加器的陷阱</h2><p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012134924591-641324349.png" alt="img"></p>
<h2 id="打破累加器陷阱：persist函数"><a href="#打破累加器陷阱：persist函数" class="headerlink" title="打破累加器陷阱：persist函数"></a>打破累加器陷阱：persist函数</h2><p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012135059496-1976934299.png" alt="img"></p>
<p>存累加器初始值：</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012135113704-339134018.png" alt="img"></p>
<p><img src="..\imgs\Spark累加器1.jpg" alt></p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012135002352-2093815302.png" alt="img"></p>
<p> 累加器实现一些基本的功能：</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012141358473-580698126.png" alt="img"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/窗口函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/窗口函数/" itemprop="url">窗口函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-13T00:00:00+08:00">
                2023-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p>[TOC]</p>
<h1 id="一、窗口函数的概念"><a href="#一、窗口函数的概念" class="headerlink" title="一、窗口函数的概念"></a>一、窗口函数的概念</h1><blockquote>
<p> 能返回整个dataframe，也能进行聚合运算。Spark支持三种窗口函数：排名函数、解析函数和聚合函数。</p>
</blockquote>
<p>例子：找到每年当中最冷那一天的温度 /  最冷那一天的日期。</p>
<ul>
<li>groupBy实现 </li>
</ul>
<p>【每年当中最冷的那一天的温度】：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = gsod.groupby(<span class="string">'year'</span>).agg(F.min(<span class="string">'temp'</span>).alias(<span class="string">'temp'</span>))</span><br><span class="line">a.orderBy(<span class="string">'temp'</span>)</span><br></pre></td></tr></table></figure>
<p>将上面的结果join回原来的dataframe得到【最冷那一天的日期】：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gsod.join(a, how = <span class="string">'left'</span>, on = [<span class="string">"year"</span>, <span class="string">"temp"</span>]).select(<span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'day'</span>, <span class="string">'temp'</span>)</span><br></pre></td></tr></table></figure>
<p>但是上面做法影响效率，left join！</p>
<ul>
<li>窗口函数的过程</li>
</ul>
<ol>
<li>根据某个条件对数据进行分组，PartitionBy</li>
<li>根据需求计算聚合函数</li>
<li>将计算结果Join回一个大dataframe</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line">each_year = Window.partitionBy(<span class="string">"year"</span>)</span><br><span class="line">gsod.withColumn(<span class="string">'min_temp'</span>,F.min(<span class="string">"temp"</span>).over(each_year))\</span><br><span class="line">.where(<span class="string">"temp=min_temp"</span>)\</span><br><span class="line">.select(<span class="string">"year"</span>, <span class="string">"month"</span>, <span class="string">"day"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="二、窗口函数"><a href="#二、窗口函数" class="headerlink" title="二、窗口函数"></a>二、窗口函数</h1><p>对于一个数据集，<code>map</code> 是对每行进行操作，为每行得到一个结果；<code>reduce</code> 则是对多行进行操作，得到一个结果；而 <code>window</code> 函数则是对多行进行操作，得到多个结果（每行一个）。</p>
<p>窗口函数是什么？来源于数据库，窗口函数是用与当前行<strong>有关</strong>的数据行参与计算。Mysql中：</p>
<ul>
<li>partition by：用于对全量数据表进行切分（与SQL中的groupby功能类似，但功能完全不同），直接体现的是前面窗口函数定义中的“有关”，即切分到同一组的即为有关，否则就是无关；</li>
<li>order by：用于指定对partition后各组内的数据进行排序；</li>
<li>rows between：用于对切分后的数据进一步限定“有关”行的数量，此种情景下即使partition后分到一组，也可能是跟当前行的计算无关。</li>
</ul>
<p><img src="https://ask.qcloudimg.com/http-save/yehe-7131597/hixjnsj96z.png?imageView2/2/w/1620" alt="img"></p>
<p>需求：组内按分数排序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select($<span class="string">"uid"</span>, $<span class="string">"date"</span>, $<span class="string">"score"</span>, row_number().over(Window.partitionBy(<span class="string">"uid"</span>).orderBy($<span class="string">"score"</span>.desc)).<span class="keyword">as</span>(<span class="string">"rank"</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://ask.qcloudimg.com/http-save/yehe-7131597/qi0imvjt5y.png?imageView2/2/w/1620" alt="img"></p>
<h2 id="udf-窗口函数"><a href="#udf-窗口函数" class="headerlink" title="udf + 窗口函数"></a>udf + 窗口函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Window</span><br><span class="line"></span><br><span class="line"><span class="meta">@pandas_udf("double")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_udf</span><span class="params">(v: pd.Series)</span> -&gt; float:</span></span><br><span class="line">    <span class="keyword">return</span> v.mean()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">1</span>, <span class="number">1.0</span>), (<span class="number">1</span>, <span class="number">2.0</span>), (<span class="number">2</span>, <span class="number">3.0</span>), (<span class="number">2</span>, <span class="number">5.0</span>), (<span class="number">2</span>, <span class="number">10.0</span>)], (<span class="string">"id"</span>, <span class="string">"v"</span>))</span><br><span class="line">w = Window.partitionBy(<span class="string">'id'</span>).orderBy(<span class="string">'v'</span>).rowsBetween(<span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line">df.withColumn(<span class="string">'mean_v'</span>, mean_udf(<span class="string">"v"</span>).over(w)).show()</span><br><span class="line"></span><br><span class="line">+---+----+------+</span><br><span class="line">| id|   v|mean_v|</span><br><span class="line">+---+----+------+</span><br><span class="line">|  <span class="number">1</span>| <span class="number">1.0</span>|   <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">1</span>| <span class="number">2.0</span>|   <span class="number">1.5</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">3.0</span>|   <span class="number">3.0</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">5.0</span>|   <span class="number">4.0</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="number">10.0</span>|   <span class="number">7.5</span>|</span><br><span class="line">+---+----+------+</span><br></pre></td></tr></table></figure>
<h1 id="三、-同时groupby-两个key"><a href="#三、-同时groupby-两个key" class="headerlink" title="三、 同时groupby 两个key"></a>三、 同时groupby 两个key</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成首次付费时间(FirstPayDate), 首次付费当天的付费金额(FirstPayPrice), 总付费金额(AllPayPrice). 首次单天付费频次(FirstPayFreq), 总付费频次(AllPayFreq)</span></span><br><span class="line">w1 = Window.partitionBy(<span class="string">"OrderId"</span>).orderBy(col(<span class="string">"PayDate"</span>))</span><br><span class="line">w2 = Window.partitionBy(<span class="string">"OrderId"</span>)</span><br><span class="line">df_pay_data = df_order_all.filter((F.col(<span class="string">"OrderType"</span>) == <span class="number">0</span>) &amp; ((F.col(<span class="string">"IsPay"</span>) == <span class="number">1</span>)))\</span><br><span class="line">    .withColumnRenamed(<span class="string">"OrderTime"</span>, <span class="string">"PayTime"</span>) \</span><br><span class="line">    .withColumnRenamed(<span class="string">"OrderPrice"</span>, <span class="string">"PayPrice"</span>) \</span><br><span class="line">    .withColumn(<span class="string">"PayDate"</span>, date_trunc(<span class="string">'day'</span>, to_timestamp(F.col(<span class="string">"PayTime"</span>)/<span class="number">1000</span>)))\</span><br><span class="line">    .withColumn(<span class="string">"row"</span>,row_number().over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"AllPayPrice"</span>, sum(col(<span class="string">"PayPrice"</span>)).over(w2)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayDate"</span>, min(col(<span class="string">"PayDate"</span>)).over(w2)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayPrice"</span>, sum(col(<span class="string">"PayPrice"</span>)).over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayFreq"</span>, count(col(<span class="string">"IsPay"</span>)).over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"AllPayFreq"</span>, count(col(<span class="string">"IsPay"</span>)).over(w2)) \</span><br><span class="line">    .where(col(<span class="string">"row"</span>) == <span class="number">1</span>).select(<span class="string">"AllPayPrice"</span>, <span class="string">"FirstPayPrice"</span>, <span class="string">"FirstPayDate"</span>,<span class="string">"OrderId"</span>, <span class="string">"FirstPayFreq"</span>, <span class="string">"AllPayFreq"</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/" target="_blank" rel="noopener">https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/</a></p>
<p><a href="https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe" target="_blank" rel="noopener">https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe</a> (相加)</p>
<h1 id="四-groupby-sort-list"><a href="#四-groupby-sort-list" class="headerlink" title="四. groupby  + sort + list"></a>四. groupby  + sort + list</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group_df = qds_com.groupby([<span class="string">'company'</span>]) \</span><br><span class="line">    .agg(F.sort_array(F.collect_list(F.struct(<span class="string">"features"</span>, <span class="string">"label"</span>, <span class="string">"samples_count"</span>))) \</span><br><span class="line">         .alias(<span class="string">"pair"</span>))</span><br></pre></td></tr></table></figure>
<h1 id="五、求众数"><a href="#五、求众数" class="headerlink" title="五、求众数"></a>五、求众数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_com_cookie.groupBy([<span class="string">'company'</span>,<span class="string">'price'</span>]).agg(F.count(<span class="string">'price'</span>).alias(<span class="string">'count_price'</span>)).orderBy([<span class="string">'company'</span>,<span class="string">'count_price'</span>], ascending=<span class="literal">False</span>).drop_duplicates(subset=[<span class="string">'company'</span>]).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">window = Window.partitionBy([<span class="string">'a'</span>]).orderBy([<span class="string">'a'</span>])</span><br><span class="line">df.withColumn(<span class="string">'rank'</span>,F.rank().over(window)).filter(<span class="string">"rank = '1'"</span>).drop(<span class="string">'rank'</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/DataFrame/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/DataFrame/" itemprop="url">DataFrame</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-12T00:00:00+08:00">
                2023-03-12
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><h2 id="一、Spark-SQL"><a href="#一、Spark-SQL" class="headerlink" title="一、Spark SQL"></a>一、Spark SQL</h2><blockquote>
<p><strong>Spark SQL用于对结构化数据进行处理，它提供了DataFrame的抽象</strong>，作为分布式平台数据查询引擎，可以在此组件上构建大数据仓库。</p>
<p><strong>DataFrame是一个分布式数据集，在概念上类似于传统数据库的表结构</strong>，数据被组织成命名的列，DataFrame的数据源可以是结构化的数据文件，也可以是Hive中的表或外部数据库，也还可以是现有的RDD。</p>
<p>DataFrame的一个主要优点是，Spark引擎一开始就构建了一个逻辑执行计划，而且执行生成的代码是基于成本优化程序确定的物理计划。与Java或者Scala相比，<strong>Python中的RDD是非常慢的，而DataFrame的引入则使性能在各种语言中都保持稳定。</strong></p>
</blockquote>
<h2 id="二、初始化"><a href="#二、初始化" class="headerlink" title="二、初始化"></a>二、初始化</h2><blockquote>
<p>在过去，你可能会使用SparkConf、SparkContext、SQLContext和HiveContext来分别执行配置、Spark环境、SQL环境和Hive环境的各种Spark查询。</p>
<p><strong>SparkSession现在是读取数据、处理元数据、配置会话和管理集群资源的入口。SparkSession本质上是这些环境的组合</strong>，包括StreamingContext。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark=SparkSession \</span><br><span class="line">   .builder \</span><br><span class="line">   .appName(<span class="string">'test'</span>) \</span><br><span class="line">   .config(<span class="string">'master'</span>,<span class="string">'yarn'</span>) \</span><br><span class="line">   .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>Spark 交互式环境下，默认已经创建了名为 spark 的 SparkSession 对象，不需要自行创建。</p>
<p><strong>从RDD创建DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推断schema</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">lines = sc.textFile(<span class="string">"users.txt"</span>)</span><br><span class="line">parts = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line">data = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>],age=p[<span class="number">1</span>],city=p[<span class="number">2</span>]))</span><br><span class="line">df=createDataFrame(data)</span><br><span class="line"><span class="comment"># 指定schema</span></span><br><span class="line">data = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>],age=int(p[<span class="number">1</span>]),city=p[<span class="number">2</span>]))</span><br><span class="line">df=createDataFrame(data)</span><br><span class="line"><span class="comment"># StructType指定schema</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">schema = StructType([</span><br><span class="line">    StructField(<span class="string">'name'</span>,StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">'age'</span>,LongType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">'city'</span>,StringType(),<span class="literal">True</span>)</span><br><span class="line">    ])</span><br><span class="line">df=createDataFrame(parts, schema)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>StructField包括以下方面的内容：<br>name：字段名<br>dataType：数据类型<br>nullable：此字段的值是否为空</p>
</blockquote>
<p><strong>从文件系统创建DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.json(<span class="string">"customer.json"</span>)</span><br><span class="line">df = spark.read.load(<span class="string">"customer.json"</span>, format=<span class="string">"json"</span>)</span><br><span class="line">df = spark.read.load(<span class="string">"users.parquet"</span>)</span><br><span class="line">df = spark.read.text(<span class="string">"users.txt"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输出和保存</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.rdd <span class="comment"># df转化为RDD</span></span><br><span class="line">df.toJSON() <span class="comment"># df转化为RDD字符串</span></span><br><span class="line">df.toPandas() <span class="comment"># df转化为pandas</span></span><br><span class="line">df.write.save(<span class="string">"customer.json"</span>, format=<span class="string">"json"</span>)</span><br><span class="line">df.write.save(<span class="string">"users.parquet"</span>)</span><br><span class="line">df.write.json(<span class="string">"users.json"</span>)</span><br><span class="line">df.write.text(<span class="string">"users.txt"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>数据库读写</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.sql(<span class="string">'select name,age,city from users'</span>) </span><br><span class="line">df.createOrReplaceTempView(name) <span class="comment"># 创建临时视图</span></span><br><span class="line">df.write.saveAsTable(name,mode=<span class="string">'overwrite'</span>,partitionBy=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p><strong>操作hive表</strong><br><code>df.write</code> 有两种方法操作hive表</p>
<ul>
<li><code>saveAsTable()</code><br>如果hive中不存在该表，则spark会自动创建此表匹。<br>如果表已存在，则匹配插入数据和原表 schema(数据格式，分区等)，只要有区别就会报错<br>若是分区表可以调用<code>partitionBy</code>指定分区，使用<code>mode</code>方法调整数据插入方式：</li>
</ul>
<blockquote>
<p>Specifies the behavior when data or table already exists. Options include:</p>
<ul>
<li><code>overwrite</code>: 覆盖原始数据(包括原表的格式，注释等)</li>
<li><code>append</code>: 追加数据(需要严格匹配)</li>
<li><code>ignore</code>: ignore the operation (i.e. no-op).</li>
<li><code>error</code> or <code>errorifexists</code>: default option, throw an exception at runtime.</li>
</ul>
</blockquote>
<ul>
<li><code>df.write.partitionBy(&#39;dt&#39;).mode(&#39;append&#39;).saveAsTable(&#39;tb2&#39;)</code></li>
<li><code>insertInto()</code></li>
</ul>
<p>无关schema，只按数据的顺序插入，如果原表不存在则会报错<br>对于分区表，先开启Hive动态分区，则不需要指定分区字段，如果有一个分区，那么默认为数据中最后一列为分区字段，有两个分区则为最后两列为分区字段，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition"</span>, <span class="string">"true"</span>)</span><br><span class="line">sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition.mode"</span>, <span class="string">"nonstrict"</span>)</span><br><span class="line">df.write.insertInto(<span class="string">'tb2'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>同样也可以先开启Hive动态分区，用SQL语句直接运行<br><code>sql(&quot;insert into tb2 select * from tb1&quot;)</code></li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">DataFrame信息</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.show(n)</code></td>
<td style="text-align:left">预览前 n 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.collect()</code></td>
<td style="text-align:left">列表形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>df.dtypes</code></td>
<td style="text-align:left">列名与数据类型</td>
</tr>
<tr>
<td style="text-align:left"><code>df.head(n)</code></td>
<td style="text-align:left">返回前 n 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.first()</code></td>
<td style="text-align:left">返回第 1 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.take(n)</code></td>
<td style="text-align:left">返回前 n 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.printSchema()</code></td>
<td style="text-align:left">打印模式信息</td>
</tr>
<tr>
<td style="text-align:left"><code>df.columns</code></td>
<td style="text-align:left">列名</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">查询语句</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.select(*cols)</code></td>
<td style="text-align:left"><code>SELECT</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>df.union(other)</code></td>
<td style="text-align:left"><code>UNION ALL</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>df.when(condition,value)</code></td>
<td style="text-align:left"><code>CASE WHEN</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>df.alias(*alias,**kwargs)</code></td>
<td style="text-align:left"><code>as</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>F.cast(dataType)</code></td>
<td style="text-align:left">数据类型转换（函数）</td>
</tr>
<tr>
<td style="text-align:left"><code>F.lit(col)</code></td>
<td style="text-align:left">常数列（函数）</td>
</tr>
<tr>
<td style="text-align:left">selectExpr</td>
<td style="text-align:left">表查询</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line">df.select(<span class="string">'*'</span>)</span><br><span class="line">df.select(<span class="string">'name'</span>,<span class="string">'age'</span>) <span class="comment"># 字段名查询</span></span><br><span class="line">df.select([<span class="string">'name'</span>,<span class="string">'age'</span>]) <span class="comment"># 字段列表查询</span></span><br><span class="line">df.select(df[<span class="string">'name'</span>],df[<span class="string">'age'</span>]+<span class="number">1</span>) <span class="comment"># 表达式查询</span></span><br><span class="line">df.select(<span class="string">'name'</span>,df.mobile.alias(<span class="string">'phone'</span>)) <span class="comment"># 重命名列</span></span><br><span class="line">df.select(<span class="string">'name'</span>,<span class="string">'age'</span>,F.lit(<span class="string">'2020'</span>).alias(<span class="string">'update'</span>))  <span class="comment"># 常数</span></span><br><span class="line">df.select(<span class="string">'name'</span>,</span><br><span class="line">          F.when(df.age &gt; <span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">           .when(df.age &lt; <span class="number">0</span>,<span class="number">-1</span>)</span><br><span class="line">           .otherwise(df.age)</span><br><span class="line">          ).show()</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">df.select(<span class="string">'name'</span>,df.age.cast(<span class="string">'float'</span>))</span><br><span class="line">df.select(<span class="string">'name'</span>,df.age.cast(FloatType()))</span><br><span class="line"><span class="comment"># selectExpr接口支持并行计算</span></span><br><span class="line">expr=[<span class="string">'count(&#123;&#125;)'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> df.columns]</span><br><span class="line">df.selectExpr(*expr).collect()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#表查询selectExpr,可以使用UDF函数，指定别名等</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">spark.udf.register(<span class="string">"getBirthYear"</span>,<span class="keyword">lambda</span> age:datetime.datetime.now().year-age)</span><br><span class="line">dftest = df.selectExpr(<span class="string">"name"</span>, <span class="string">"getBirthYear(age) as birth_year"</span> , <span class="string">"UPPER(gender) as gender"</span> )</span><br><span class="line">dftest.show()</span><br><span class="line"></span><br><span class="line">---------------------------</span><br><span class="line"><span class="comment">#窗口函数</span></span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([(<span class="string">"LiLei"</span>,<span class="number">78</span>,<span class="string">"class1"</span>),(<span class="string">"HanMeiMei"</span>,<span class="number">87</span>,<span class="string">"class1"</span>),</span><br><span class="line">                           (<span class="string">"DaChui"</span>,<span class="number">65</span>,<span class="string">"class2"</span>),(<span class="string">"RuHua"</span>,<span class="number">55</span>,<span class="string">"class2"</span>)]) \</span><br><span class="line">    .toDF(<span class="string">"name"</span>,<span class="string">"score"</span>,<span class="string">"class"</span>)</span><br><span class="line"></span><br><span class="line">df.show()</span><br><span class="line">dforder = df.selectExpr(<span class="string">"name"</span>,<span class="string">"score"</span>,<span class="string">"class"</span>,</span><br><span class="line">         <span class="string">"row_number() over (partition by class order by score desc) as order"</span>)</span><br><span class="line"></span><br><span class="line">dforder.show()</span><br><span class="line">+---------+-----+------+</span><br><span class="line">|     name|score| <span class="class"><span class="keyword">class</span>|</span></span><br><span class="line"><span class="class">+---------+-----+------+</span></span><br><span class="line"><span class="class">|    <span class="title">LiLei</span>|   78|<span class="title">class1</span>|</span></span><br><span class="line"><span class="class">|<span class="title">HanMeiMei</span>|   87|<span class="title">class1</span>|</span></span><br><span class="line"><span class="class">|   <span class="title">DaChui</span>|   65|<span class="title">class2</span>|</span></span><br><span class="line"><span class="class">|    <span class="title">RuHua</span>|   55|<span class="title">class2</span>|</span></span><br><span class="line"><span class="class">+---------+-----+------+</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">+---------+-----+------+-----+</span></span><br><span class="line"><span class="class">|     <span class="title">name</span>|<span class="title">score</span>| <span class="title">class</span>|<span class="title">order</span>|</span></span><br><span class="line"><span class="class">+---------+-----+------+-----+</span></span><br><span class="line"><span class="class">|   <span class="title">DaChui</span>|   65|<span class="title">class2</span>|    1|</span></span><br><span class="line"><span class="class">|    <span class="title">RuHua</span>|   55|<span class="title">class2</span>|    2|</span></span><br><span class="line"><span class="class">|<span class="title">HanMeiMei</span>|   87|<span class="title">class1</span>|    1|</span></span><br><span class="line"><span class="class">|    <span class="title">LiLei</span>|   78|<span class="title">class1</span>|    2|</span></span><br><span class="line"><span class="class">+---------+-----+------+-----+</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">排序</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.sort(*col,**kwargs)</code></td>
<td style="text-align:left">排序</td>
</tr>
<tr>
<td style="text-align:left"><code>df.orderBy(*col,**kwargs)</code></td>
<td style="text-align:left">排序(用法同sort)</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.sort(df.age.desc()).show()</span><br><span class="line">df.sort(<span class="string">'age'</span>,ascending=<span class="literal">True</span>).show()</span><br><span class="line">df.sort(desc(<span class="string">'age'</span>),<span class="string">'name'</span>).show()</span><br><span class="line">df.sort([<span class="string">'age'</span>,<span class="string">'name'</span>],ascending=[<span class="number">0</span>,<span class="number">1</span>]).show()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">筛选方法</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.filter(condition)</code></td>
<td style="text-align:left">筛选</td>
</tr>
<tr>
<td style="text-align:left"><code>column.isin(*cols)</code></td>
<td style="text-align:left"><code>in (...)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>column.like(pattern)</code></td>
<td style="text-align:left">SQL通配符匹配</td>
</tr>
<tr>
<td style="text-align:left"><code>column.rlike(pattern)</code></td>
<td style="text-align:left">正则表达式匹配</td>
</tr>
<tr>
<td style="text-align:left"><code>column.startswith(pattern)</code></td>
<td style="text-align:left">匹配开始</td>
</tr>
<tr>
<td style="text-align:left"><code>column.endswith(pattern)</code></td>
<td style="text-align:left">匹配结尾</td>
</tr>
<tr>
<td style="text-align:left"><code>column.substr(start,length)</code></td>
<td style="text-align:left">截取字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>column.between(lower,upper)</code></td>
<td style="text-align:left"><code>between ... and ...</code></td>
</tr>
<tr>
<td style="text-align:left">column.where</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.filter(<span class="string">"age = 22"</span>).show()</span><br><span class="line">df.filter(df.age == <span class="number">22</span>).show()</span><br><span class="line">df.select(df[<span class="string">'age'</span>] == <span class="number">22</span>).show()</span><br><span class="line">df.select(df.name.isin(<span class="string">'Bill'</span>,<span class="string">'Elon'</span>)).show()</span><br><span class="line">df.filter(<span class="string">"name like Elon%"</span>).show()</span><br><span class="line">df.filter(df.name.rlike(<span class="string">"Musk$"</span>).show()</span><br><span class="line">          </span><br><span class="line">df.where(<span class="string">"gender='male' and age &gt; 15"</span>).show()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">统计信息</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.describe()</code></td>
<td style="text-align:left">描述性统计</td>
</tr>
<tr>
<td style="text-align:left"><code>df.count()</code></td>
<td style="text-align:left">行数</td>
</tr>
<tr>
<td style="text-align:left"><code>df.approxQuantile(col,prob,relativeError)</code></td>
<td style="text-align:left">百分位数</td>
</tr>
<tr>
<td style="text-align:left"><code>df.corr(col1,col2,method=None)</code></td>
<td style="text-align:left">相关系数</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异常值处理</span></span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    quantiles = df.approxQuantile(col,[<span class="number">0.25</span>,<span class="number">0.75</span>],<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># 第三个参数relativeError代表可接受的错误程度，越小精度越高</span></span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [quantiles[<span class="number">0</span>]<span class="number">-1.5</span>*IQR, quantiles[<span class="number">1</span>]+<span class="number">1.5</span>*IQR]</span><br><span class="line">    <span class="comment"># bounds保存了每个特征的上下限</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">分组和聚合</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.groupBy(*cols)</code></td>
<td style="text-align:left">分组，返回GroupedData</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.count()</code></td>
<td style="text-align:left">计数</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.sum(*cols)</code></td>
<td style="text-align:left">求和</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.avg(*cols)</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.mean(*cols)</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.max(*cols)</code></td>
<td style="text-align:left">最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.min(*cols)</code></td>
<td style="text-align:left">最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.agg(*exprs)</code></td>
<td style="text-align:left">应用表达式</td>
</tr>
</tbody>
</table>
<blockquote>
<p> 聚合函数还包括 countDistinct, kurtosis, skewness, stddev, sumDistinct, variance 等</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">'city'</span>).count().collect()</span><br><span class="line">df.groupBy(df.city).avg(<span class="string">'age'</span>).collect()</span><br><span class="line">df.groupBy(<span class="string">'city'</span>,df.age).count().collect()</span><br><span class="line">df.groupBy(<span class="string">'city'</span>).agg(&#123;<span class="string">'age'</span>:<span class="string">'mean'</span>&#125;).collect() <span class="comment"># 字典形式给出</span></span><br><span class="line">df.groupBy(<span class="string">'city'</span>).agg(&#123;<span class="string">'*'</span>:<span class="string">'count'</span>&#125;).collect() </span><br><span class="line">df.groupBy(<span class="string">'city'</span>).agg(F.mean(df.age)).collect() </span><br><span class="line"><span class="comment"># groupBy + collect_list</span></span><br><span class="line">df.groupBy(<span class="string">"gender"</span>).agg(F.expr(<span class="string">"avg(age)"</span>),F.expr(<span class="string">"collect_list(name)"</span>)).show()</span><br><span class="line">+------+--------+------------------+</span><br><span class="line">|gender|avg(age)|collect_list(name)|</span><br><span class="line">+------+--------+------------------+</span><br><span class="line">|  null|    <span class="number">16.0</span>|           [RuHua]|</span><br><span class="line">|female|    <span class="number">16.0</span>|       [HanMeiMei]|</span><br><span class="line">|  male|    <span class="number">16.0</span>|   [LiLei, DaChui]|</span><br><span class="line">+------+--------+------------------+</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">去重</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.distinct()</code></td>
<td style="text-align:left">唯一值（整行去重）</td>
</tr>
<tr>
<td style="text-align:left"><code>df.dropDuplicates(subset=None)</code></td>
<td style="text-align:left">删除重复项（可以指定字段）</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">添加、修改、删除列</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.withColumnRenamed(existing,new)</code></td>
<td style="text-align:left">重命名</td>
</tr>
<tr>
<td style="text-align:left"><code>df.withColumn(colname,new)</code></td>
<td style="text-align:left">修改列</td>
</tr>
<tr>
<td style="text-align:left"><code>df.drop(*cols)</code></td>
<td style="text-align:left">删除列</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df=df.withColumn(<span class="string">'age'</span>,df.age+<span class="number">1</span>)</span><br><span class="line">df=df.drop(<span class="string">'age'</span>)</span><br><span class="line">df=df.drop(df.age)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">缺失值处理</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.na.fill(value,subset=None)</code></td>
<td style="text-align:left">缺失值填充</td>
</tr>
<tr>
<td style="text-align:left"><code>df.na.drop(how=&#39;any&#39;,thresh=None,subset=None)</code></td>
<td style="text-align:left">缺失值删除</td>
</tr>
<tr>
<td style="text-align:left"><code>df.na.replace(to_teplace,value,subset=None)</code></td>
<td style="text-align:left">替换</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df=df.na.fill(<span class="number">0</span>)</span><br><span class="line">df=df.na.fill(&#123;<span class="string">'age'</span>:<span class="number">50</span>,<span class="string">'name'</span>:<span class="string">'unknow'</span>&#125;)</span><br><span class="line">df=df.na.drop()</span><br><span class="line">df = df.dropna() <span class="comment"># 跟上面那种方式是一样的</span></span><br><span class="line">df=df.na.replace([<span class="string">'Alice'</span>,<span class="string">'Bob'</span>],[<span class="string">'A'</span>,<span class="string">'B'</span>],<span class="string">'name'</span>)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">分区和缓存</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.repartition(n)</code></td>
<td style="text-align:left">将df拆分为10个分区</td>
</tr>
<tr>
<td style="text-align:left"><code>df.coalesce(n)</code></td>
<td style="text-align:left">将df合并为n个分区</td>
</tr>
<tr>
<td style="text-align:left"><code>df.cache()</code></td>
<td style="text-align:left">缓存</td>
</tr>
</tbody>
</table>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/理解RDD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/理解RDD/" itemprop="url">RDD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-11T00:00:00+08:00">
                2023-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="一、理解RDD"><a href="#一、理解RDD" class="headerlink" title="一、理解RDD"></a>一、理解RDD</h1><blockquote>
<p> RDD可以被抽象地理解为一个大的数组，但是这个数组是分布在集群上的。</p>
<p><img src="..\imgs\RDD的理解.jpg" alt></p>
</blockquote>
<h1 id="二、RDD的生命周期"><a href="#二、RDD的生命周期" class="headerlink" title="二、RDD的生命周期"></a>二、RDD的生命周期</h1><p>创建—变换—动作（结束）</p>
<h1 id="三、RDD依赖"><a href="#三、RDD依赖" class="headerlink" title="三、RDD依赖"></a>三、RDD依赖</h1><p>1、窄依赖（RDD）—–原地变换，不需要shuffle【即各个RDD之间不需要统计】</p>
<p><img src="..\imgs\RDD1.jpg" alt></p>
<p>2、宽依赖（RDD）—–需要shuffle，与其他RDD交换资料，时间消耗长。</p>
<p><img src="..\imgs\RDD2.jpg" alt></p>
<p>3、任务优化，如</p>
<p><img src="..\imgs\RDD3.jpg" alt></p>
<h1 id="四、RDD的基本操作"><a href="#四、RDD的基本操作" class="headerlink" title="四、RDD的基本操作"></a>四、RDD的基本操作</h1><p>RDD可以有两种计算操作算子：Transformation（变换）与Action（行动）。</p>
<p><img src="..\imgs\RDD4.jpg" alt></p>
<h2 id="1、基本的RDD"><a href="#1、基本的RDD" class="headerlink" title="1、基本的RDD"></a>1、基本的RDD</h2><p>（1）建立RDD</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wordsList =  [<span class="string">'cat'</span>,<span class="string">'ele'</span>,<span class="string">'rat'</span>,<span class="string">'rat'</span>,<span class="string">'cat'</span>]</span><br><span class="line">wordsRDD =  sc.parallelize(wordsList , <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>   #建立RDD   <code>wordsList =   [&#39;cat&#39;,&#39;ele&#39;,&#39;rat&#39;,&#39;rat&#39;,&#39;cat&#39;]</code>   <code>wordsRDD =   sc.parallelize(wordsList , 4)</code>   </p>
<p>   #计算法每个单词的长度   <code>wordsRDD.map(len).collect()</code>   </p>
<ul>
<li>转换操作</li>
</ul>
<p><img src="..\imgs\RDD5.jpg" alt></p>
<ul>
<li>动作操作</li>
</ul>
<p><img src="..\imgs\RDD6.jpg" alt></p>
<h2 id="键值对PairRDD："><a href="#键值对PairRDD：" class="headerlink" title="键值对PairRDD："></a>键值对PairRDD：</h2><p>是一种以（key,value)方式存储的RDD。</p>
<p>   <code>pairRDD =   wordsRDD.map(lambda x : (x , 1))</code>   </p>
<ul>
<li><p>转换操作</p>
<p><img src="..\imgs\RDD7.jpg" alt></p>
</li>
<li><p>动作操作</p>
<p><img src="..\imgs\RDD8.jpg" alt></p>
</li>
</ul>
<p>一些小问题：</p>
<p>1、spark中的RDD是什么</p>
<p>概念：<strong>分布式数据集，**</strong>spark<strong>**中基本的数据抽象，代表一个不可变、可分区、元素可并行计算的集合。</strong></p>
<p>2、RDD的五大特性：</p>
<p>①有一个<strong>分区</strong>列表，即能被切分，可并行。</p>
<p>②由一个<strong>函数</strong>计算每一个分片</p>
<p>③<strong>容错机制</strong>，对其他RDD的<strong>依赖</strong>（宽依赖和窄依赖），但并非所有RDD都要依赖。</p>
<p>RDD每次transformations（转换）都会生成一个新的RDD，两者之间会形成依赖关系。在部分分区数据丢失时，可通过依赖关系重新计算丢失的数据。</p>
<p>④key-value型的RDD是根据<strong>哈希</strong>来分区的，控制Key分到哪个reduce。</p>
<p>⑤每一分片<strong>计算优先位置</strong>，比如HDFS的block的所在位置应该是优先计算的位置。</p>
<p>3、概述一下spark中的常用算子区别（map、mapPartitions、foreach、foreachPartition）</p>
<table>
<thead>
<tr>
<th>map</th>
<th>遍历RDD，将函数f应用于每一个元素，返回新的RDD</th>
<th>transformation算子</th>
</tr>
</thead>
<tbody>
<tr>
<td>mapPartitions</td>
<td>用于遍历操作RDD中的每一个分区，返回生成一个新的RDD</td>
<td>transformation</td>
</tr>
<tr>
<td>collect</td>
<td>将RDD元素送回Master并返回List类型</td>
<td>Action</td>
</tr>
<tr>
<td>foreach</td>
<td>用于遍历RDD,将函数f应用于每一个元素，无返回值</td>
<td>action算子</td>
</tr>
<tr>
<td>foreachPartition</td>
<td>用于遍历操作RDD中的每一个分区。无返回值</td>
<td>action算子</td>
</tr>
<tr>
<td>总结</td>
<td>一般使用mapPartitions或者foreachPartition算子比map和foreach更加高效，推荐使用。</td>
</tr>
</tbody>
</table>
<p>4、谈谈spark中的宽窄依赖</p>
<ul>
<li>RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖和宽依赖。</li>
<li>宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition分区【需要shuffle，与其他RDD交换资料】 例如 groupByKey、 reduceByKey、 sortByKey等操作会产生宽依赖，会产生shuffle      </li>
<li>窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个Partition分区使用。【原地变换，不需要shuffle】      例如map、filter、union等操作会产生窄依赖 </li>
</ul>
<p>5、spark中如何划分stage</p>
<p>Stage划分的依据就是宽依赖，何时产生宽依赖，例如reduceByKey,groupByKey的算子，会导致宽依赖的产生。</p>
<table>
<thead>
<tr>
<th>先介绍什么是RDD中的宽窄依赖，</th>
</tr>
</thead>
<tbody>
<tr>
<td>然后在根据DAG有向无环图进行划分，从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage,</td>
</tr>
<tr>
<td>然后继续按照这种方式在继续往前推，如在遇到宽依赖，又划分成一个stage,一直到最前面的一个算子。</td>
</tr>
<tr>
<td>最后整个job会被划分成多个stage,而stage之间又存在依赖关系，后面的stage依赖于前面的stage。</td>
</tr>
</tbody>
</table>
<h1 id="五、代码学习"><a href="#五、代码学习" class="headerlink" title="五、代码学习"></a>五、代码学习</h1><h3 id="1-建立RDD"><a href="#1-建立RDD" class="headerlink" title="1. 建立RDD"></a>1. 建立RDD</h3><p><strong>创建RDD的两种方法：</strong></p>
<p>1 读取一个数据集(SparkContext.textFile()) : lines = sc.textFile(“README.md”)<br>2 读取一个集合(SparkContext.parallelize()) : lines = sc.paralelize(List(“pandas”,”i like pandas”))</p>
<p><img src="E:\GitHub_learn\blog\source\imgs\rdd_1.jpg" alt></p>
<p>#take操作将前若干个数据汇集到Driver，相比collect安全</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建RDD</span></span><br><span class="line"><span class="comment"># 从并行集合创建</span></span><br><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#collect操作将数据汇集到Driver,数据过大时有超内存风险</span></span><br><span class="line">all_data = rdd.collect()</span><br><span class="line">all_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#take操作将前若干个数据汇集到Driver，相比collect安全</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>) </span><br><span class="line">part_data = rdd.take(<span class="number">4</span>)</span><br><span class="line">part_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#takeSample可以随机取若干个到Driver,第一个参数设置是否放回抽样</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>) </span><br><span class="line">sample_data = rdd.takeSample(<span class="literal">False</span>,<span class="number">10</span>,<span class="number">0</span>)</span><br><span class="line">sample_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#first取第一个数据</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>) </span><br><span class="line">first_data = rdd.first()</span><br><span class="line">print(first_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#count查看RDD元素数量</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>)</span><br><span class="line">data_count = rdd.count()</span><br><span class="line">print(data_count)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>从text中读取，read.text</strong></li>
</ul>
<p><img src="..\imgs\rdd_2.jpg" alt></p>
<ul>
<li><strong>从csv中读取:read.csv</strong></li>
</ul>
<p><img src="..\imgs\rdd_3.jpg" alt></p>
<ul>
<li><strong>从json中读取：read.json</strong></li>
</ul>
<p><img src="..\imgs\rdd_4.jpg" alt></p>
<h3 id="2-RDD与Dataframe的转换"><a href="#2-RDD与Dataframe的转换" class="headerlink" title="2. RDD与Dataframe的转换"></a>2. RDD与Dataframe的转换</h3><p><strong>（1）dataframe转换成rdd：</strong></p>
<p><strong>法一：datardd = dataDataframe.rdd</strong></p>
<p><strong>法二：datardd = sc.parallelize(_)</strong></p>
<p><strong>（2）rdd转换成dataframe：</strong></p>
<p><strong>dataDataFrame = spark.createDataFrame(datardd)</strong></p>
<p><img src="..\imgs\rdd_5.jpg" alt></p>
<p><img src="..\imgs\rdd_6.jpg" alt></p>
<p><img src="..\imgs\rdd_7.jpg" alt></p>
<h3 id="3-rdd函数"><a href="#3-rdd函数" class="headerlink" title="3. rdd函数"></a>3. rdd函数</h3><table>
<thead>
<tr>
<th style="text-align:left">mapReduce</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.map(func)</code></td>
<td style="text-align:left">将<a href="http://www.aisouwen.com/tags_38.html" target="_blank" rel="noopener">函数</a>应用于RDD中的每个元素并返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.mapValues(func)</code></td>
<td style="text-align:left">不改变key，只对value执行map</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.flatMap(func)</code></td>
<td style="text-align:left">先map后扁平化返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.flatMapValues(func)</code></td>
<td style="text-align:left">不改变key，只对value执行flatMap</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.reduce(func)</code></td>
<td style="text-align:left">合并RDD的元素返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.reduceByKey(func)</code></td>
<td style="text-align:left">合并每个key的value</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.foreach(func)</code></td>
<td style="text-align:left">用迭代的方法将函数应用于每个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.keyBy(func)</code></td>
<td style="text-align:left">执行函数于每个元素创建key-value对RDD</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.map(<span class="keyword">lambda</span> x:x+<span class="number">1</span>).collect()</span><br><span class="line">[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.reduce(<span class="keyword">lambda</span> x,y : x+y)</span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.keyBy(<span class="keyword">lambda</span> x:x%<span class="number">2</span>).collect()</span><br><span class="line">[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">0</span>,<span class="number">4</span>),(<span class="number">1</span>,<span class="number">5</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.mapValues(<span class="keyword">lambda</span> x:x+<span class="number">1</span>).collect()</span><br><span class="line">[(<span class="string">'a'</span>,<span class="number">8</span>),(<span class="string">'a'</span>,<span class="number">3</span>),(<span class="string">'b'</span>,<span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.reduceByKey(<span class="keyword">lambda</span> x,y : x+y).collect()</span><br><span class="line">[(<span class="string">'a'</span>,<span class="number">9</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names=sc.parallelize([<span class="string">'Elon Musk'</span>,<span class="string">'Bill Gates'</span>,<span class="string">'Jim Green'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names.map(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>)).collect()</span><br><span class="line">[(<span class="string">'Elon'</span>,<span class="string">'Musk'</span>),(<span class="string">'Bill'</span>,<span class="string">'Gates'</span>),(<span class="string">'Jim'</span>,<span class="string">'Green'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names.flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>)).collect()</span><br><span class="line">[<span class="string">'Elon'</span>,<span class="string">'Musk'</span>,<span class="string">'Bill'</span>,<span class="string">'Gates'</span>,<span class="string">'Jim'</span>,<span class="string">'Green'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>（1）map操作</strong></p>
<p><img src="..\imgs\rdd_8.jpg" alt></p>
<p><strong>（2）collect操作</strong></p>
<p><img src="..\imgs\rdd_10.jpg" alt></p>
<table>
<thead>
<tr>
<th style="text-align:left">提取</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.collect()</code></td>
<td style="text-align:left">将RDD以列表形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.collectAsMap()</code></td>
<td style="text-align:left">将RDD以字典形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.take(n)</code></td>
<td style="text-align:left">提取前n个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.takeSample(replace,n,seed)</code></td>
<td style="text-align:left">随机提取n个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.first()</code></td>
<td style="text-align:left">提取第1名</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.top(n)</code></td>
<td style="text-align:left">提取前n名</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.keys()</code></td>
<td style="text-align:left">返回RDD的keys</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.values()</code></td>
<td style="text-align:left">返回RDD的values</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.isEmpty()</code></td>
<td style="text-align:left">检查RDD是否为空</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.collectAsMap()</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">2</span>,<span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.keys().collect()</span><br><span class="line">[<span class="string">'a'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.values().collect()</span><br><span class="line">[<span class="number">7</span>,<span class="number">2</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">分组和聚合</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.groupBy(func)</code></td>
<td style="text-align:left">将RDD元素通过函数变换分组为key-iterable集</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.groupByKey()</code></td>
<td style="text-align:left">将key-value元素集分组为key-iterable集</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.aggregate(zeroValue,seqOp,combOp)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.aggregateByKey(zeroValue,seqOp,combOp)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.fold(zeroValue,func)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.foldByKey(zeroValue,func)</code></td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.groupBy(<span class="keyword">lambda</span> x: x % <span class="number">2</span>).mapValues(list).collect()</span><br><span class="line">[(<span class="number">0</span>,[<span class="number">2</span>,<span class="number">4</span>]),(<span class="number">1</span>,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>])]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.groupByKey().mapValues(list).collect()</span><br><span class="line">[(<span class="string">'a'</span>,[<span class="number">7</span>,<span class="number">2</span>]),(<span class="string">'b'</span>,[<span class="number">2</span>])]</span><br></pre></td></tr></table></figure>
<blockquote>
<h3 id="更好的解决方案：reduceByKey-非groupBykey"><a href="#更好的解决方案：reduceByKey-非groupBykey" class="headerlink" title="更好的解决方案：reduceByKey,非groupBykey"></a>更好的解决方案：reduceByKey,非groupBykey</h3><p>reduceByKey能够直接将资料根据key值聚合，减少多余的交换（shuffle）动作。</p>
<p>避免使用groupbykey，如果数据量过大，会造成内存溢出。</p>
</blockquote>
<ol>
<li><strong>spark中groupByKey 、aggregateByKey、reduceByKey 有什么区别？使用上需要注意什么？</strong></li>
</ol>
<p>（1）groupByKey()是对RDD中的所有数据做shuffle,根据不同的Key映射到不同的partition中再进行aggregate。</p>
<p>（3） distinct()也是对RDD中的所有数据做shuffle进行aggregate后再去重。</p>
<p>（2）aggregateByKey()是先对每个partition中的数据根据不同的Key进行aggregate，然后将结果进行shuffle，完成各个partition之间的aggregate。因此，和groupByKey()相比，运算量小了很多。</p>
<p>（4）reduceByKey()也是先在单台机器中计算，再将结果进行shuffle，减小运算量 </p>
<p><img src="..\imgs\rdd_11.jpg" alt></p>
<table>
<thead>
<tr>
<th style="text-align:left">选择数据</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.sample(replace,frac,seed)</code></td>
<td style="text-align:left">抽样</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.filter(func)</code></td>
<td style="text-align:left">筛选满足函数的元素(变换)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.distinct()</code></td>
<td style="text-align:left">去重</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.sample(<span class="literal">False</span>,<span class="number">0.8</span>,seed=<span class="number">42</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>).collect()</span><br><span class="line">[<span class="number">2</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">排序</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.sortBy(func,ascending=True)</code></td>
<td style="text-align:left">按RDD元素变换后的值排序</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.sortByKey(ascending=True)</code></td>
<td style="text-align:left">按key排序</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">统计</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.count()</code></td>
<td style="text-align:left">返回RDD中的元素数</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.countByKey()</code></td>
<td style="text-align:left">按key计算RDD元素数量</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.countByValue()</code></td>
<td style="text-align:left">按RDD元素计算数量</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.sum()</code></td>
<td style="text-align:left">求和</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.mean()</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.max()</code></td>
<td style="text-align:left">最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.min()</code></td>
<td style="text-align:left">最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.stdev()</code></td>
<td style="text-align:left">标准差</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.variance()</code></td>
<td style="text-align:left">方差</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.histograme()</code></td>
<td style="text-align:left">分箱（Bin）生成直方图</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.stats()</code></td>
<td style="text-align:left">综合统计（计数、平均值、标准差、最大值和最小值）</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.count()</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.countByKey()</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,&#123;<span class="string">'a'</span>:<span class="number">2</span>,<span class="string">'b'</span>:<span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.countByValue()</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,&#123;(<span class="string">'b'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">7</span>):<span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.histogram(<span class="number">3</span>)</span><br><span class="line">([<span class="number">0</span>,<span class="number">33</span>,<span class="number">66</span>,<span class="number">99</span>],[<span class="number">33</span>,<span class="number">33</span>,<span class="number">34</span>])</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">连接运算</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.union(other)</code></td>
<td style="text-align:left">并集(不去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.intersection(other)</code></td>
<td style="text-align:left">交集(去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.subtract(other)</code></td>
<td style="text-align:left">差集(不去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.cartesian(other)</code></td>
<td style="text-align:left">笛卡尔积</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.subtractByKey(other)</code></td>
<td style="text-align:left">按key差集</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.join(other)</code></td>
<td style="text-align:left">内连接</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.leftOuterJoin(other)</code></td>
<td style="text-align:left">左连接</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.rightOuterJoin(other)</code></td>
<td style="text-align:left">右连接</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2=sc.parallelize([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.union(rdd1).collect()</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.intersection(rdd2).collect()</span><br><span class="line">[<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.subtract(rdd2).collect()</span><br><span class="line">[<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.cartesian(rdd2).collect()</span><br><span class="line">[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">1</span>),(<span class="number">3</span>,<span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2=sc.parallelize([(<span class="string">'b'</span>,<span class="string">'B'</span>),(<span class="string">'c'</span>,<span class="string">'C'</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.subtractByKey(rdd2).collect()</span><br><span class="line">[(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.join(rdd2).collect()</span><br><span class="line">[(<span class="string">'b'</span>,(<span class="number">2</span>,<span class="string">'B'</span>))]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">持久化</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.persist()</code></td>
<td style="text-align:left">标记为持久化</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.cache()</code></td>
<td style="text-align:left">等价于<code>rdd.persist(MEMORY_ONLY)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.unpersist()</code></td>
<td style="text-align:left">释放缓存</td>
</tr>
</tbody>
</table>
<ol>
<li><strong>cache后面能不能接其他算子,它是不是action操作？</strong></li>
</ol>
<p>Cache后可以接其他算子，但是接了算子之后，起不到缓存的作用，因为会重复出发cache。</p>
<p>Cache不是action操作。</p>
<ol>
<li><strong>cache和pesist有什么区别？</strong> </li>
</ol>
<p>（1）cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间； </p>
<p>（2）cache只有一个默认的缓存级别MEMORY_ONLY ，cache调用了persist，而persist可以根据情况设置其它的缓存级别； </p>
<p>（3）executor执行的时候，默认60%做cache，40%做task操作，persist最根本的函数，最底层的函数 </p>
<ol>
<li><strong>Spark为什么要持久化，一般什么场景下要进行persist操作？</strong></li>
</ol>
<p>为什么要进行持久化？</p>
<p>spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤</p>
<p>只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。</p>
<p>以下场景会使用persist</p>
<p>1）某个步骤计算非常耗时，需要进行persist持久化</p>
<p>2）计算链条非常长，重新恢复要算很多步骤，很好使，persist</p>
<p>3）checkpoint所在的rdd要持久化persist，</p>
<p>lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前</p>
<p>要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。</p>
<p>4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</p>
<p>5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</p>
<table>
<thead>
<tr>
<th style="text-align:left">分区</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.getNumPartitions()</code></td>
<td style="text-align:left">获取RDD分区数</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.repartition(n)</code></td>
<td style="text-align:left">新建一个含n个分区的RDD</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.coalesce(n)</code></td>
<td style="text-align:left">将RDD中的分区减至n个</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.partitionBy(key,func)</code></td>
<td style="text-align:left">自定义分区</td>
</tr>
</tbody>
</table>
<p><strong>文件系统读写</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取</span></span><br><span class="line">rdd=sc.textFile(<span class="string">'hdfs://file_path'</span>)  <span class="comment"># 从hdfs集群读取</span></span><br><span class="line">rdd=sc.textFile(<span class="string">'file_path'</span>) </span><br><span class="line">rdd=sc.textFile(<span class="string">'file:///local_file_path'</span>) <span class="comment"># 从本地文件读取</span></span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">'hdfs://file_path'</span>)</span><br><span class="line">rdd.saveAsTextFile(<span class="string">'file_path'</span>) <span class="comment"># hdfs路径</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">'file:///local_file_path'</span>)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Spark简介/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark简介/" itemprop="url">1.Spark简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-09T00:00:00+08:00">
                2023-03-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p><a href="https://blog.csdn.net/weixin_42331985/article/details/124126019" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42331985/article/details/124126019</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/396809439" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/396809439</a></p>
<p><a href="https://blog.csdn.net/czz1141979570/article/details/105877261/" target="_blank" rel="noopener">https://blog.csdn.net/czz1141979570/article/details/105877261/</a></p>
<p>[TOC]</p>
<h1 id="Spark生态架构图"><a href="#Spark生态架构图" class="headerlink" title="Spark生态架构图"></a>Spark生态架构图</h1><h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h2><p><img src="..\imgs\spark组件介绍.jpg" alt></p>
<blockquote>
<p><strong>Spark：</strong>基于内存的迭代式计算引擎。</p>
<p><strong>RDD：</strong>Resillient Distributed Dataset（弹性分布式数据集），是分布式内存的一个抽象概念。</p>
<p><strong>DAG：</strong>Directed Acyclic Graph（有向无环图），反映RDD之间的依赖关系。</p>
<p><img src="https://img-blog.csdnimg.cn/99c95c8e6e724185b86fa7e0ba42f7fc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAUmVsaWFu5ZOI5ZOI,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img"></p>
<p><strong>Executor</strong>：是运行在工作节点（WorkerNode）的一个进程，负责运行Task</p>
<p><strong>应用（Application）</strong>：用户编写的Spark应用程序</p>
<p><strong>任务（ Task ）</strong>：运行在Executor上的工作单元(线程)</p>
<p><strong>作业（ Job ）</strong>：一个作业包含多个RDD及作用于相应RDD上的各种操作</p>
<p><strong>阶段（ Stage ）</strong>：是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集, 下图为DAG划分Stage过程：</p>
<p><img src="https://img-blog.csdnimg.cn/70e593dbb2c54f53a01f239947f7e451.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAUmVsaWFu5ZOI5ZOI,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img"></p>
</blockquote>
<h2 id="2-组件关系"><a href="#2-组件关系" class="headerlink" title="2. 组件关系"></a>2. 组件关系</h2><p>当执行一个Application时，Driver会向Yarn申请资源，启动Executor（Worker），并向Executor发送代码和文件，执行任务，任务结束后执行结果会返回给任务控制节点，或者写到HDFS/Hive等。</p>
<p>1 Application = 1 Driver + 多 Job </p>
<p>1 Job（多个 RDD + RDD的操作） = 多个Stage </p>
<p>1 Stage = 多个Task</p>
<p><img src="..\imgs\Spark组件关系.jpg" alt></p>
<p><img src="..\imgs\Spark基本组件.jpg" alt></p>
<h2 id="3-运行流程"><a href="#3-运行流程" class="headerlink" title="3. 运行流程"></a>3. 运行流程</h2><h3 id="（1）概念层级"><a href="#（1）概念层级" class="headerlink" title="（1）概念层级"></a>（1）概念层级</h3><p>解释1：</p>
<blockquote>
<ol>
<li><p>一个Spark提交时，由Driver运行main方法创建一个SparkContext，由SparkContext负责和Yarn的通信、资源的申请、任务的分配和监控等。</p>
<p>SparkContext会向Yarn注册并申请运行Executor的资源。</p>
</li>
<li><p>Yarn为Executor分配资源，启动Executor进程，Executor发送心跳到Yarn上</p>
</li>
<li><p>SparkContext根据RDD的依赖关系构建DAG图，DAG调度解析后将图分解成多个Stage，并计算出之间的依赖关系，将这些Job集提交给Task调度器处理。Executor向SparkContext申请Task，Task调度器将Task分发给Executor运行，同时，SparkContext将Application代码发放给Executor。</p>
</li>
<li><p>任务在Executor上运行，结果反馈给Job调度器，再反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</p>
</li>
</ol>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/1115a2d3fe534bd8b172961157a56eb2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemt5Q29kZXI=,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>解释2：</p>
<blockquote>
<p>每个 worker 节点包含一个或者多个 executor，一个 executor 中又包含多个 task。task 是真正实现并行计算的最小工作单元。</p>
<ul>
<li><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Driver 是一个 Java 进程，负责执行 Spark 任务的 main 方法，它的职责有：</p>
<ul>
<li><p>执行用户提交的代码，创建 SparkContext 或者 SparkSession</p>
</li>
<li><p>将用户代码转化为Spark任务（Jobs）</p>
</li>
<li><ul>
<li>创建血缘（Lineage），逻辑计划（Logical Plan）和物理计划（Physical Plan)</li>
</ul>
</li>
<li><p>在 Cluster Manager 的辅助下，把 task 任务分发调度出去</p>
</li>
<li><p>跟踪任务的执行情况</p>
</li>
</ul>
</li>
<li><h3 id="Spark-Context-Session"><a href="#Spark-Context-Session" class="headerlink" title="Spark Context/Session"></a>Spark Context/Session</h3><p>它是由Spark driver创建，每个 Spark 应用对应一个。程序和集群交互的入口。可以连接到 Cluster Manager</p>
</li>
<li><h3 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager"></a>Cluster Manager</h3><p>负责部署整个Spark 集群，包括上面提到的 driver 和 executors。具有以下几种部署模式</p>
<ol>
<li>Standalone 模式</li>
<li>YARN</li>
<li>Mesos</li>
<li>Kubernetes</li>
</ol>
</li>
<li><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>一个创建在 worker 节点的进程。一个 Executor 有多个 slots(线程) 可以并发执行多个 tasks。</p>
<ul>
<li>负责执行spark任务，把结果返回给 Driver</li>
<li>可以将数据缓存到 worker 节点的内存</li>
<li>一个 slot 就是一个线程，对应了一个 task</li>
</ul>
</li>
</ul>
<p><img src="..\imgs\Spark架构.jpg" alt></p>
</blockquote>
<p><img src="..\imgs\Spark代码执行流程.jpg" alt></p>
<h3 id="（2）代码层级"><a href="#（2）代码层级" class="headerlink" title="（2）代码层级"></a>（2）代码层级</h3><p>Spark 有懒加载的特性，也就是说 Spark 计算按兵不动，直到遇到 action 类型的 operator 的时候才会触发一次计算。</p>
<blockquote>
<ul>
<li><p>DAG</p>
<ul>
<li>Spark Job如何执行，都是由这个 DAG 来管的，包括决定 task 运行在什么节点</li>
</ul>
</li>
<li><p>Spark Job</p>
<ul>
<li>每个Spark Job 对应一个action</li>
</ul>
</li>
<li><p>Stages</p>
<ul>
<li>每个 Spark Job 包含一系列 stages</li>
<li>Stages 按照数据是否需要 shuffle 来划分（宽依赖）</li>
<li>Stages 之间的执行是串行的（除非stage 间计算的RDD不同）</li>
<li>因为 Stages 是串行的，所以 shuffle 越少越好</li>
</ul>
</li>
<li><p>Tasks</p>
<ul>
<li>每个 stage 包含一系列的 tasks</li>
<li>Tasks 是并行计算的最小单元</li>
<li>一个 stage 中的所有 tasks 执行同一段代码逻辑，只是基于不同的数据块</li>
<li>一个 task 只能在一个executor中执行，不能是多个</li>
<li>一个 stage 输出的 partition 数量等于这个 stage 执行 tasks 的数量</li>
</ul>
</li>
<li><p>Partition</p>
<ul>
<li>Spark 中 partition（分区） 可以理解为内存中的一个数据集</li>
<li>一个 partition 对应一个 task，一个 task 对应 一个 executor 中的一个 slot，一个 slot 对应物理资源是一个线程 thread</li>
<li>1 partition = 1 task = 1 slot = 1 thread</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="3-spark中Master与Worker区别及Driver与Executor区别"><a href="#3-spark中Master与Worker区别及Driver与Executor区别" class="headerlink" title="(3) spark中Master与Worker区别及Driver与Executor区别"></a>(3) spark中Master与Worker区别及Driver与Executor区别</h3><p><img src="..\imgs\Master和Worker关系.jpg" alt></p>
<p><img src="..\imgs\Driver和Executor关系.jpg" alt></p>
<p> Master和Worker是Spark的守护进程，即Spark在特定模式下正常运行所必须的进程。Driver和Executor是临时程序，当有具体任务提交到Spark集群才会开启的程序。</p>
<p><a href="https://blog.csdn.net/nicole_33/article/details/122520395" target="_blank" rel="noopener">了解 Spark中的master、worker和Driver、Executor</a></p>
<p><img src="..\imgs\Spark的worker理解.jpg" alt></p>
<blockquote>
<p>每个Worker上存在一个或者多个ExecutorBackend 进程。每个进程包含一个Executor对象，该对象持有一个线程池，每个线程可以执行一个task。<br>每个application包含一个 driver 和多个 executors，每个 executor里面运行的tasks都属于同一个application。<br>每个Worker上存在一个或者多个ExecutorBackend 进程。<br>每个进程包含一个Executor对象，该对象持有一个线程池，每个线程可以执行一个task</p>
</blockquote>
<h2 id="4-DAGScheduler具体流程"><a href="#4-DAGScheduler具体流程" class="headerlink" title="4. DAGScheduler具体流程"></a>4. DAGScheduler具体流程</h2><p>DAG负责的是将RDD中的数据依赖划分为不同可以并行的宽依赖task， 这些不同的task集合统称为stage，最后将这些stage推送给TaskScheduler进行调度，DAG的具体划分过程如下所示：</p>
<p><img src="..\imgs\DAG流程.jpg" alt></p>
<blockquote>
<ul>
<li><code>窄依赖经历的是map、filter等操作没有进行相关的shuffle，而宽依赖则通常都是join等操作需要进行一定的shuffle意味着需要打散均匀等操作</code></li>
<li>1 stage是触发action的时候 <strong>从后往前划分</strong> 的，所以本图要从RDD_G开始划分。</li>
<li>2 RDD_G依赖于RDD_B和RDD_F，随机决定先判断哪一个依赖，但是对于结果无影响。</li>
<li>3 RDD_B与RDD_G属于窄依赖，所以他们属于同一个stage，RDD_B与老爹RDD_A之间是宽依赖的关系，所以他们不能划分在一起，所以RDD_A自己是一个stage1</li>
<li>4 RDD_F与RDD_G是属于宽依赖，他们不能划分在一起，所以最后一个stage的范围也就限定了，RDD_B和RDD_G组成了Stage3</li>
<li>5 RDD_F与两个爹RDD_D、RDD_E之间是窄依赖关系，RDD_D与爹RDD_C之间也是窄依赖关系，所以他们都属于同一个stage2</li>
<li>6 执行过程中stage1和stage2相互之间没有前后关系所以可以并行执行，相应的每个stage内部各个partition对应的task也并行执行</li>
<li>7 stage3依赖stage1和stage2执行结果的partition，只有等前两个stage执行结束后才可以启动stage3.</li>
<li>8 我们前面有介绍过Spark的Task有两种：ShuffleMapTask和ResultTask，其中后者在DAG最后一个阶段推送给Executor，其余所有阶段推送的都是ShuffleMapTask。在这个案例中stage1和stage2中产生的都是ShuffleMapTask，在stage3中产生的ResultTask。</li>
<li>9 虽然stage的划分是从后往前计算划分的，但是依赖逻辑判断等结束后真正创建stage是从前往后的。也就是说如果从stage的ID作为标识的话，先需要执行的stage的ID要小于后需要执行的ID。就本案例来说，stage1和stage2的ID要小于stage3，至于stage1和stage2的ID谁大谁小是随机的，是由前面第2步决定的。</li>
</ul>
</blockquote>
<h2 id="5-MR和spark区别"><a href="#5-MR和spark区别" class="headerlink" title="5. MR和spark区别"></a>5. MR和spark区别</h2><blockquote>
<h3 id="（1）中间结果输出："><a href="#（1）中间结果输出：" class="headerlink" title="（1）中间结果输出："></a>（1）中间结果输出：</h3><ul>
<li><p>MapReduce：读–处理–写磁盘–读–处理–写磁盘（<strong>中间结果落地，即存入磁盘</strong>）</p>
</li>
<li><p>spark：读–处理–处理–（需要的时候）写磁盘（<strong>中间结果存入内存</strong>）</p>
</li>
</ul>
<p><strong>减少落地时间，速度快</strong></p>
<p><img src="../imgs/Hadoop%E5%92%8Cspark%E5%8C%BA%E5%88%AB.jpg" alt></p>
<h3 id="（2）数据格式："><a href="#（2）数据格式：" class="headerlink" title="（2）数据格式："></a>（2）数据格式：</h3><ul>
<li><p>MapReduce<strong>：从</strong>DB中读取数据再处理</p>
</li>
<li><p>spark：采用弹性分布式数据结构RDD存储数据</p>
</li>
</ul>
<h3 id="（3）容错性："><a href="#（3）容错性：" class="headerlink" title="（3）容错性："></a>（3）容错性：</h3><ul>
<li>Spark：采用RDD存储数据，若数据集丢失，可重建。</li>
</ul>
<h3 id="（4）通用性："><a href="#（4）通用性：" class="headerlink" title="（4）通用性："></a>（4）通用性：</h3><ul>
<li><p>MapReduce：只提供map和reduce两种操作。</p>
</li>
<li><p>spark：提供很多数据集操作类型（transformations、actions）【transformations包括map\filter\</p>
</li>
</ul>
<p>Groupbykey\sort等，action包括reduce、save、collect、lookup等】</p>
<h3 id="（5）执行策略"><a href="#（5）执行策略" class="headerlink" title="（5）执行策略"></a>（5）执行策略</h3><ul>
<li><p>MapReduce：数据shuffle前需排序</p>
</li>
<li><p>spark：不是所有场景都要排序</p>
</li>
</ul>
</blockquote>
<h2 id="6、spark1-x和spark2-x的区别"><a href="#6、spark1-x和spark2-x的区别" class="headerlink" title="6、spark1.x和spark2.x的区别"></a>6、spark1.x和spark2.x的区别</h2><blockquote>
<ul>
<li><p>Spark1.x：采用SparkContext作为进入点</p>
</li>
<li><p>Spark2.x：SparkSession 是 Spark SQL 的入口。</p>
<ul>
<li>采用SparkSession作为进入点，SparkSession可直接读取各种资料源，可直接与Hive元数据沟通，同时包含设定以及资源管理功能。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="7-spark-应用执行模式"><a href="#7-spark-应用执行模式" class="headerlink" title="7. spark 应用执行模式"></a>7. spark 应用执行模式</h2><h3 id="（1）local模式"><a href="#（1）local模式" class="headerlink" title="（1）local模式"></a>（1）local模式</h3><p>local 模式主要是用于本地代码测试操作</p>
<p>本质上就是一个单进程程序, 在一个进程中运行多个线程</p>
<p>类似于pandas , 都是一个单进程程序, 无法处理大规模数据, 只需要处理小规模数据</p>
<p><img src="..\imgs\Spark环境1.jpg" alt></p>
<h3 id="（2）standalone："><a href="#（2）standalone：" class="headerlink" title="（2）standalone："></a>（2）standalone：</h3><blockquote>
<p> Spark Standalone模式：该模式是不借助于第三方资源管理框架的完全分布式模式。Spark 使用自己的 Master 进程对应用程序运行过程中所需的资源进行调度和管理；对于中小规模的 Spark 集群首选 Standalone 模式。目前Spark 在 Standalone 模式下主要是借助 Zookeeper 实现单点故障问题；思想也是类似于 Hbase Master 单点故障解决方案。</p>
</blockquote>
<h3 id="（3）YARN"><a href="#（3）YARN" class="headerlink" title="（3）YARN"></a>（3）YARN</h3><blockquote>
<p>该模式是借助于第三方资源管理框架 Yarn 的完全分布式模式。Spark 作为一个提交程序的客户端将 Job 任务提交到 Yarn 上；然后通过 Yarn 来调度和管理 Job 任务执行过程中所需的资源。需要此模式需要先搭建 Yarn 集群，然后将 Spark 作为 Hadoop 中的一个组件纳入到 Yarn 的调度管理下，这样将更有利于系统资源的共享。</p>
</blockquote>
<h2 id="8-提交任务方法"><a href="#8-提交任务方法" class="headerlink" title="8. 提交任务方法"></a>8. 提交任务方法</h2><blockquote>
<p><strong>（1）spark shell</strong></p>
<ul>
<li>spark-shell 是 Spark 自带的交互式 Shell 程序，方便用户进行交互式编程，用户可以在该命令行下用 <a href="https://so.csdn.net/so/search?q=Scala&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener">Scala</a> 编写 spark 程序。</li>
<li><p>应用场景</p>
<ul>
<li>通常是以测试为主</li>
<li>所以一般直接以<code>./spark-shell</code>启动，进入本地模式测试</li>
</ul>
</li>
<li>local方式启动：./spark-shell</li>
<li>standalone集群模式启动：./spark-shell –master spark://master:7077</li>
<li>yarn client模式启动：./spark-shell –master yarn-client</li>
</ul>
<p><strong>（2）spark submit</strong></p>
<p>使用spark 自带的spark-submit工具提交任务</p>
<p>程序一旦打包好，就可以使用 bin/spark-submit 脚本启动应用了。这个脚本负责设置 spark 使用的 classpath 和依赖，支持不同类型的集群管理器和发布模式。</p>
<p><strong>它主要是用于提交编译并打包好的Jar包到集群环境中来运行</strong>，和hadoop中的hadoop jar命令很类似，hadoop jar是提交一个MR-task,而spark-submit是提交一个spark任务，这个脚本 可以设置Spark类路径（classpath）和应用程序依赖包，并且可以设置不同的Spark所支持的集群管理和部署模式。 相对于spark-shell来讲它不具有REPL(交互式的编程环境)的，在运行前需要指定应用的启动类，jar包路径,参数等内容。</p>
</blockquote>
<h2 id="9-参数配置："><a href="#9-参数配置：" class="headerlink" title="9. 参数配置："></a>9. 参数配置：</h2><p>参数名    参数说明</p>
<ul>
<li>-class    应用程序的主类，仅针对 java 或 scala 应用</li>
<li>-master    master 的地址，提交任务到哪里执行，例如 local,spark://host:port, yarn, local</li>
<li>-deploy-mode    在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</li>
<li>-name    应用程序的名称，会显示在Spark的网页用户界面</li>
<li>-jars    用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</li>
<li>-packages    包含在driver 和executor 的 classpath 中的 jar 的 maven 坐标</li>
<li>-exclude-packages    为了避免冲突 而指定不包含的 package</li>
<li>-repositories    远程 repository</li>
<li>-conf PROP=VALUE    指定 spark 配置属性的值，例如 -conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=256m”</li>
<li>-properties-file    加载的配置文件，默认为 conf/spark-defaults.conf</li>
<li>-driver-memory    Driver内存，默认 1G</li>
<li>-driver-java-options    传给 driver 的额外的 Java 选项</li>
<li>-driver-library-path    传给 driver 的额外的库路径</li>
<li>-driver-class-path    传给 driver 的额外的类路径</li>
<li>-driver-cores    Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</li>
<li>-executor-memory    每个 executor 的内存，默认是1G</li>
<li>-total-executor-cores    所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用</li>
<li>-num-executors    启动的 executor 数量。默认为2。在 yarn 下使用</li>
<li>-executor-core    每个 executor 的核数。在yarn或者standalone下使用</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/正则化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/正则化/" itemprop="url">正则化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/正则化/" itemprop="url" rel="index">
                    <span itemprop="name">正则化</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="正则化-惩罚项"><a href="#正则化-惩罚项" class="headerlink" title="正则化/惩罚项"></a>正则化/惩罚项</h1><h2 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h2><blockquote>
<p> <strong>（1）范数：</strong><img src="..\imgs\范数.jpg" alt></p>
<p><strong>（2）方差和偏差：</strong></p>
<p>Error = Bias + Variance</p>
<ul>
<li>Error反映的是整个模型的准确度，</li>
<li>Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，</li>
<li>Variance反映的是模型每一次输出结果与模型输出期望之间的误差（描述的是样本上训练的模型在测试集上的表现。），即模型的稳定性。</li>
<li>欠拟合是高bias，过拟合是高variance。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>（3）正则化的目的</strong>：减少模型参数大小或者参数数量，缓解过拟合。</p>
<p>正则化的作用是给模型加一个先验，lasso(l1)认为模型是拉普拉斯分布，ridge(l2)认为是高斯分布，正则项对应参数的协方差，协方差越小，这个模型的variance越小，泛化 能力越强，也就抵抗了过拟合。</p>
<p><strong>（4）正则化通用形式：</strong></p>
<p>​        Loss_with_regularization = loss(w,x) + λf(w)</p>
<ul>
<li>正则化恒为非负</li>
<li>f(w)不能为负数，若其为负数，Loss(w,x)+λf(x)本来尽可能想让其变小，那f(x)为负数，f(x)绝对值会越学越大。</li>
</ul>
<p><strong>(5) 正则化方法：</strong>L1正则、L2正则、Dropout正则</p>
</blockquote>
<h2 id="二、-从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】"><a href="#二、-从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】" class="headerlink" title="二、 从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】"></a>二、 从数学角度解释正则化为什么能提升模型的泛化能力；【奥卡姆剃刀：简单就好】</h2><p><strong>过拟合就是模型在学习训练样本时将噪声异常值也学习得非常好，使得模型参数过多，模型较复杂，给参数加上一个先验约束，可降低过拟合。</strong></p>
<p> <img src="..\imgs\正则化1.jpg" alt></p>
<h2 id="三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；"><a href="#三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；" class="headerlink" title="三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；"></a>三、L1和L2范数各有什么特点以及相应的原因？L1和L2的区别与应用场景；</h2><p><strong>区别</strong>：L1假设参数服从拉普拉斯分布，L2则符合高斯分布；</p>
<p>L1范数更容易产生稀疏的权重，L2范数更容易产生分散的权重。</p>
<p><strong>原因</strong>：（L1稀疏的原因，L2不稀疏的原因）【几何、公式两个角度】</p>
<p><strong>场景</strong>：具有高维的数据特征时采用L1正则效果好一点。因为L1具有稀疏性。</p>
<h2 id="四、解释L1范数更容易产生稀疏的权重，L2不的原因："><a href="#四、解释L1范数更容易产生稀疏的权重，L2不的原因：" class="headerlink" title="四、解释L1范数更容易产生稀疏的权重，L2不的原因："></a>四、解释L1范数更容易产生稀疏的权重，L2不的原因：</h2><h3 id="（1）几何角度"><a href="#（1）几何角度" class="headerlink" title="（1）几何角度"></a>（1）几何角度</h3><p>L2正则项约束后的解空间是圆形，L1正则项约束后的解空间是多方形，L1易在角点发生交点，从而产生稀疏解。</p>
<blockquote>
<p>绿色等高线代表未施加正则化的代价函数，菱形和圆形分别代表L1和L2正则化约束，L1-ball 与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的”等高线”除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性。相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小</p>
</blockquote>
<p><img src="..\imgs\L1正则解释.jpg" alt></p>
<h3 id="（2）公式角度：（拉格朗日求导）"><a href="#（2）公式角度：（拉格朗日求导）" class="headerlink" title="（2）公式角度：（拉格朗日求导）"></a>（2）公式角度：（拉格朗日求导）</h3><blockquote>
<p>深度学习花书7.1节（202页左右）。带L1正则化的最优参数w=sign(w<em>) max{|w</em>|- a/H , 0}，其中w代表未正则化的目标函数的最优参数，H代表海森矩阵，a是正则化系数，只要a足够大，w就会在更大区间范围内使w变为0，而带L2正则化的最优参数w=H/(H+a)▪w,只要w不为0，w也不为0.</p>
</blockquote>
<p><strong>1、稀疏性的约束：</strong></p>
<p><img src="..\imgs\正则公式1.jpg" alt></p>
<p>​    L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</p>
<p><strong>2、不好求解，松弛为L1，L2：</strong></p>
<p><img src="..\imgs\正则公式2.jpg" alt></p>
<p><strong>3、拉格朗日</strong></p>
<p><img src="..\imgs\正则公式3.jpg" alt></p>
<h3 id="（3）贝叶斯先验"><a href="#（3）贝叶斯先验" class="headerlink" title="（3）贝叶斯先验"></a>（3）贝叶斯先验</h3><blockquote>
<p>L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验，而拉普拉斯先验使参数为0的可能性更大。</p>
</blockquote>
<p><strong>L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出。</strong></p>
<p><strong>L2正则化可通过假设权重w的先验分布为高斯分布，由最大后验概率估计导出。</strong></p>
<p><strong>详细解释：</strong> <a href="https://blog.csdn.net/m0_38045485/article/details/82147817" target="_blank" rel="noopener">https://blog.csdn.net/m0_38045485/article/details/82147817</a></p>
<p><img src="..\imgs\正则公式4.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/概率论1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/概率论1/" itemprop="url">概率论</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/" itemprop="url" rel="index">
                    <span itemprop="name">先导知识</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/先导知识/概率论基本知识/" itemprop="url" rel="index">
                    <span itemprop="name">概率论基本知识</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="一、均值、方差、协方差"><a href="#一、均值、方差、协方差" class="headerlink" title="一、均值、方差、协方差"></a>一、均值、方差、协方差</h1><ul>
<li>期望/均值：实验中每次可能结果的概率乘其结果的总和。<ul>
<li>E(X)=∑xP(X), x表示随机变量的取值，P(X)表示随机变量X=x的概率。</li>
</ul>
</li>
<li>方差：概率分布的数据期望，反映了随机变量取值的变异程度。<ul>
<li>D(x ) = E{[X-E(X)]^2} =E(X^2) - [ E(X)]^2</li>
</ul>
</li>
</ul>
<p><img src="..\imgs\协方差.jpg" alt></p>
<ul>
<li>协方差：度量两个随机变量关系的统计量<ul>
<li><img src="..\imgs\协方差2.jpg" alt></li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/过拟合欠拟合/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/过拟合欠拟合/" itemprop="url">过拟合</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">
                2023-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/过拟合/" itemprop="url" rel="index">
                    <span itemprop="name">过拟合</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本篇博客仅作为学习,如有侵权必删。</p>
<h1 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h1><h1 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h1><blockquote>
<p>过拟合：模型对于训练数拟合呈过当的情况，反映到评估指标上，就是模型在训练集上表现很好，测试集和新数据上表现较差。</p>
<p>欠拟合：模型在训练和和预测时表现都不好的情况。</p>
</blockquote>
<p><img src="..\imgs\过拟合欠拟合.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Lee_yl</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lee_yl</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
