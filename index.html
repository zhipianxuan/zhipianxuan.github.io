<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Lee_yl&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Lee_yl&#39;s blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lee_yl&#39;s blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>Lee_yl's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lee_yl's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Spark-聚合操作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark-聚合操作/" itemprop="url">8. 聚合操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-19T00:00:00+08:00">
                2023-03-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>本文仅做学习总结，如有侵权立删</p>
<p>[TOC]</p>
<h1 id="一、分组类型"><a href="#一、分组类型" class="headerlink" title="一、分组类型"></a>一、分组类型</h1><p><strong>！！！注意null值，如果不过滤空值，则可能会得到不正确的结果。尤其是grouping set、rollup和cube！</strong></p>
<ul>
<li><p>group by：指定一个或多个key和一个或多个聚合函数，对列进行转换操作</p>
</li>
<li><p>window：跟group by类似功能， 但能保留当前行</p>
</li>
<li>grouping set：SQL中的用法</li>
<li><p>rollup：跟groupby类似功能，并会针对指定的多个key进行分级分组汇总。</p>
</li>
<li><p>cube：跟rollup相同功能。</p>
</li>
</ul>
<h1 id="二、聚合函数"><a href="#二、聚合函数" class="headerlink" title="二、聚合函数"></a>二、聚合函数</h1><ul>
<li>count：统计个数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可用expr表达式来处理聚合函数</span></span><br><span class="line">df.groupBy(<span class="string">"a"</span>).agg(</span><br><span class="line">	count(<span class="string">"aa"</span>).alias(<span class="string">"aa"</span>),</span><br><span class="line">	expr(<span class="string">"count(aa)"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>countDistinct：去重统计</p>
</li>
<li><p>approx_count_distinct：近似统计个数, pyspark.sql.functions.approx_count_distinct(col, rsd=None)</p>
<ul>
<li>允许的最大相对标准偏差(默认 = 0.05)。对于 rsd <code>count_distinct()</code> 效率更高</li>
<li>对于大数据即，某种精度的近似值可以接受，但比countDistinct（）耗时更少，数据越大，效率提升越明显。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.agg(approx_count_distinct(df.age).alias(<span class="string">'distinct_ages'</span>)).collect()</span><br><span class="line">[Row(distinct_ages=<span class="number">2</span>)]</span><br></pre></td></tr></table></figure>
</li>
<li><p>first和last：</p>
</li>
<li><p>min和max</p>
</li>
<li><p>sum</p>
</li>
<li><p>sumDistinct</p>
</li>
<li><p>avg</p>
</li>
<li><p>variance和stddev：方差和标准差</p>
<ul>
<li>var_samp：样本方差</li>
<li>var_pop：总体方差</li>
</ul>
</li>
<li><p>skewness和kurtosis：偏度和峰度</p>
</li>
<li><p>cov和corr：协方差和相关性</p>
<ul>
<li>covar_samp：样本协方差</li>
<li>covar_pop：总体协方差</li>
</ul>
</li>
<li><p>collect_list和collect_set：聚合成一个list或者set</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.agg(F.collect_set(<span class="string">"a"</span>), F.collect_list(<span class="string">"a"</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>用户自定义聚合函数：UDAF：仅在Scala和Java中使用</p>
<ul>
<li><p>UDF：</p>
<blockquote>
<p>UDF（User-defined functions）用户自定义函数，简单说就是输入一行输出一行的自定义算子。<br>是大多数 SQL 环境的关键特性，用于扩展系统的内置功能。<strong>（一对一）</strong></p>
</blockquote>
</li>
<li><p>UDAF</p>
<blockquote>
<p>UDAF（User Defined Aggregate Function），即用户定义的聚合函数，聚合函数和普通函数的区别是什么呢，普通函数是接受一行输入产生一个输出，聚合函数是接受一组（一般是多行）输入然后产生一个输出，即将一组的值想办法聚合一下。<strong>（多对一）</strong></p>
<p>UDAF可以跟group by一起使用，也可以不跟group by一起使用，这个其实比较好理解，联想到mysql中的max、min等函数，可以:<br>select max(foo) from foobar group by bar;<br>表示根据bar字段分组，然后求每个分组的最大值，这时候的分组有很多个，使用这个函数对每个分组进行处理，也可以：<br>select max(foo) from foobar;<br>这种情况可以将整张表看做是一个分组，然后在这个分组（实际上就是一整张表）中求最大值。所以聚合函数实际上是对分组做处理，而不关心分组中记录的具体数量。</p>
</blockquote>
</li>
<li><p>UDTF</p>
<blockquote>
<p>UDTF(User-Defined Table-Generating Functions),用户自定义生成函数。它就是输入一行输出多行的自定义算子，可输出多行多列，又被称为 “表生成函数”。<strong>（一对多）</strong></p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h1 id="三、例子"><a href="#三、例子" class="headerlink" title="三、例子"></a>三、例子</h1><h2 id="创建数据"><a href="#创建数据" class="headerlink" title="创建数据"></a>创建数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line">data = [</span><br><span class="line">    (<span class="string">"James"</span>, <span class="string">"Sales"</span>, <span class="number">3000</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Michael"</span>, <span class="string">"Sales"</span>, <span class="number">4600</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Robert"</span>, <span class="string">"Sales"</span>, <span class="number">4100</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Maria"</span>, <span class="string">"Finance"</span>, <span class="number">3000</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"James"</span>, <span class="string">"Sales"</span>, <span class="number">3000</span>, <span class="string">'2019'</span>),</span><br><span class="line">    (<span class="string">"Scott"</span>, <span class="string">"Finance"</span>, <span class="number">3300</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Jen"</span>, <span class="string">"Finance"</span>, <span class="number">3900</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Jeff"</span>, <span class="string">"Marketing"</span>, <span class="number">3000</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Kumar"</span>, <span class="string">"Marketing"</span>, <span class="number">2000</span>, <span class="string">'2020'</span>),</span><br><span class="line">    (<span class="string">"Saif"</span>, <span class="string">"Sales"</span>, <span class="number">4100</span>, <span class="string">'2020'</span>)</span><br><span class="line">]</span><br><span class="line">schema = [<span class="string">"employee_name"</span>, <span class="string">"department"</span>, <span class="string">"salary"</span>, <span class="string">'year'</span>]</span><br><span class="line">  </span><br><span class="line">df = spark.createDataFrame(data=data, schema=schema)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">root</span><br><span class="line"> |-- employee_name: string (nullable = true)</span><br><span class="line"> |-- department: string (nullable = true)</span><br><span class="line"> |-- salary: long (nullable = true)</span><br><span class="line"> |-- year: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----+</span><br><span class="line">|employee_name|department|salary|year|</span><br><span class="line">+-------------+----------+------+----+</span><br><span class="line">|James        |Sales     |<span class="number">3000</span>  |<span class="number">2020</span>|</span><br><span class="line">|Michael      |Sales     |<span class="number">4600</span>  |<span class="number">2020</span>|</span><br><span class="line">|Robert       |Sales     |<span class="number">4100</span>  |<span class="number">2020</span>|</span><br><span class="line">|Maria        |Finance   |<span class="number">3000</span>  |<span class="number">2020</span>|</span><br><span class="line">|James        |Sales     |<span class="number">3000</span>  |<span class="number">2019</span>|</span><br><span class="line">|Scott        |Finance   |<span class="number">3300</span>  |<span class="number">2020</span>|</span><br><span class="line">|Jen          |Finance   |<span class="number">3900</span>  |<span class="number">2020</span>|</span><br><span class="line">|Jeff         |Marketing |<span class="number">3000</span>  |<span class="number">2020</span>|</span><br><span class="line">|Kumar        |Marketing |<span class="number">2000</span>  |<span class="number">2020</span>|</span><br><span class="line">|Saif         |Sales     |<span class="number">4100</span>  |<span class="number">2020</span>|</span><br><span class="line">+-------------+----------+------+----+</span><br></pre></td></tr></table></figure>
<h2 id="1、group-by：分组"><a href="#1、group-by：分组" class="headerlink" title="1、group by：分组"></a>1、group by：分组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按照department, year计算工资之和。</span></span><br><span class="line">df.groupBy(<span class="string">'department'</span>, <span class="string">'year'</span>).agg(</span><br><span class="line">    F.sum(<span class="string">'salary'</span>).alias(<span class="string">'salary'</span>)</span><br><span class="line">).orderBy(<span class="string">'department'</span>, <span class="string">'year'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----+------+</span><br><span class="line">|department|year|salary|</span><br><span class="line">+----------+----+------+</span><br><span class="line">|   Finance|2020| 10200|</span><br><span class="line">| Marketing|2020|  5000|</span><br><span class="line">|     Sales|2019|  3000|</span><br><span class="line">|     Sales|2020| 15800|</span><br><span class="line">+----------+----+------+</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>pivot：透视转换</strong></li>
</ul>
<p>使用pivot函数进行透视，透视过程中可以提供第二个参数来明确指定使用哪些数据项<br>注意：pivot只能跟在groupby之后</p>
<blockquote>
<p> <strong>利用pivot实现行转列</strong><br>pivot的第一个参数指定原数据列，第二个参数指定要新生成的数据列</p>
</blockquote>
<p><img src="..\imgs\Spark_pivot1.jpg" alt></p>
<p><img src="..\imgs\Spark_pivot.jpg" alt></p>
<h2 id="2-rollup：分级分组"><a href="#2-rollup：分级分组" class="headerlink" title="2. rollup：分级分组"></a>2. rollup：分级分组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">- 先按照 `department`、`employee_name`、`year`分组；</span></span><br><span class="line"><span class="string">- 然后按照`department`、`employee_name`分组；</span></span><br><span class="line"><span class="string">- 然后再按照 `department` 分组；</span></span><br><span class="line"><span class="string">- 最后进行全表分组。</span></span><br><span class="line"><span class="string">- 后面接聚合函数，此处使用的是`sum`。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">df.rollup(<span class="string">'department'</span>, <span class="string">'employee_name'</span>, <span class="string">'year'</span>).agg(</span><br><span class="line">    F.sum(<span class="string">'salary'</span>).alias(<span class="string">'salary'</span>)</span><br><span class="line">).orderBy(<span class="string">'department'</span>, <span class="string">'employee_name'</span>, <span class="string">'year'</span>).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-------------+----+------+</span><br><span class="line">|department|employee_name|year|salary|</span><br><span class="line">+----------+-------------+----+------+</span><br><span class="line">|      null|         null|null| <span class="number">34000</span>|</span><br><span class="line">|   Finance|         null|null| <span class="number">10200</span>|</span><br><span class="line">|   Finance|          Jen|null|  <span class="number">3900</span>|</span><br><span class="line">|   Finance|          Jen|<span class="number">2020</span>|  <span class="number">3900</span>|</span><br><span class="line">|   Finance|        Maria|null|  <span class="number">3000</span>|</span><br><span class="line">|   Finance|        Maria|<span class="number">2020</span>|  <span class="number">3000</span>|</span><br><span class="line">|   Finance|        Scott|null|  <span class="number">3300</span>|</span><br><span class="line">|   Finance|        Scott|<span class="number">2020</span>|  <span class="number">3300</span>|</span><br><span class="line">| Marketing|         null|null|  <span class="number">5000</span>|</span><br><span class="line">| Marketing|         Jeff|null|  <span class="number">3000</span>|</span><br><span class="line">| Marketing|         Jeff|<span class="number">2020</span>|  <span class="number">3000</span>|</span><br><span class="line">| Marketing|        Kumar|null|  <span class="number">2000</span>|</span><br><span class="line">| Marketing|        Kumar|<span class="number">2020</span>|  <span class="number">2000</span>|</span><br><span class="line">|     Sales|         null|null| <span class="number">18800</span>|</span><br><span class="line">|     Sales|        James|null|  <span class="number">6000</span>|</span><br><span class="line">|     Sales|        James|<span class="number">2019</span>|  <span class="number">3000</span>|</span><br><span class="line">|     Sales|        James|<span class="number">2020</span>|  <span class="number">3000</span>|</span><br><span class="line">|     Sales|      Michael|null|  <span class="number">4600</span>|</span><br><span class="line">|     Sales|      Michael|<span class="number">2020</span>|  <span class="number">4600</span>|</span><br><span class="line">|     Sales|       Robert|null|  <span class="number">4100</span>|</span><br><span class="line">+----------+-------------+----+------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br></pre></td></tr></table></figure>
<h2 id="3-cube：分级分组，比rollup组合的维度更全。"><a href="#3-cube：分级分组，比rollup组合的维度更全。" class="headerlink" title="3. cube：分级分组，比rollup组合的维度更全。"></a>3. <code>cube</code>：分级分组，比rollup组合的维度更全。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">1. cube` 先按照`department、employee_name、year`分组；</span></span><br><span class="line"><span class="string">2. 然后按照`(department, employee_name)`、`(department, year)`、`(year, employee_name)`分组；</span></span><br><span class="line"><span class="string">3. 然后按照`department`、`employee_name`、`year`分别分组；</span></span><br><span class="line"><span class="string">4. 最后进行全表分组。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df.cube(<span class="string">'department'</span>, <span class="string">'employee_name'</span>, <span class="string">'year'</span>).agg(</span><br><span class="line">    F.sum(<span class="string">'salary'</span>).alias(<span class="string">'salary'</span>)</span><br><span class="line">).orderBy(<span class="string">'department'</span>, <span class="string">'employee_name'</span>, <span class="string">'year'</span>).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-------------+----+------+</span><br><span class="line">|department|employee_name|year|salary|</span><br><span class="line">+----------+-------------+----+------+</span><br><span class="line">|      null|         null|null| <span class="number">34000</span>|</span><br><span class="line">|      null|         null|<span class="number">2019</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|         null|<span class="number">2020</span>| <span class="number">31000</span>|</span><br><span class="line">|      null|        James|null|  <span class="number">6000</span>|</span><br><span class="line">|      null|        James|<span class="number">2019</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|        James|<span class="number">2020</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|         Jeff|null|  <span class="number">3000</span>|</span><br><span class="line">|      null|         Jeff|<span class="number">2020</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|          Jen|null|  <span class="number">3900</span>|</span><br><span class="line">|      null|          Jen|<span class="number">2020</span>|  <span class="number">3900</span>|</span><br><span class="line">|      null|        Kumar|null|  <span class="number">2000</span>|</span><br><span class="line">|      null|        Kumar|<span class="number">2020</span>|  <span class="number">2000</span>|</span><br><span class="line">|      null|        Maria|null|  <span class="number">3000</span>|</span><br><span class="line">|      null|        Maria|<span class="number">2020</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|      Michael|null|  <span class="number">4600</span>|</span><br><span class="line">|      null|      Michael|<span class="number">2020</span>|  <span class="number">4600</span>|</span><br><span class="line">|      null|       Robert|null|  <span class="number">4100</span>|</span><br><span class="line">|      null|       Robert|<span class="number">2020</span>|  <span class="number">4100</span>|</span><br><span class="line">|      null|         Saif|null|  <span class="number">4100</span>|</span><br><span class="line">|      null|         Saif|<span class="number">2020</span>|  <span class="number">4100</span>|</span><br><span class="line">+----------+-------------+----+------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br></pre></td></tr></table></figure>
<h3 id="3-1-grouping"><a href="#3-1-grouping" class="headerlink" title="3.1 grouping"></a>3.1 <code>grouping</code></h3><p>指示 <code>GROUP BY</code> 列表中的指定列是否为空，在结果集中返回 1 表示空或 0 表示未空。</p>
<p>grouping(xx) = 0：表示维度出现在组合里面</p>
<p>grouping(xx) = 1: 表示维度不出现在组合里面</p>
<p>CUBE 操作所生成的空值带来一个问题：如何区分 CUBE 操作所生成的 NULL 值和从实际数据中返回的 NULL 值？这个问题可用 GROUPING 函数解决。<strong>如果列中的值来自事实数据，则 GROUPING 函数返回 0；如果列中的值是 CUBE 操作所生成的 NULL，则返回 1。</strong>在 CUBE 操作中，所生成的 NULL 代表全体值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df.cube(<span class="string">"department"</span>).agg(</span><br><span class="line">    F.grouping(<span class="string">"department"</span>).alias(<span class="string">'department'</span>), </span><br><span class="line">    F.sum(<span class="string">"salary"</span>).alias(<span class="string">'salary'</span>)</span><br><span class="line">).orderBy(<span class="string">"salary"</span>).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----------+------+</span><br><span class="line">|department|department|salary|</span><br><span class="line">+----------+----------+------+</span><br><span class="line">| Marketing|         <span class="number">0</span>|  <span class="number">5000</span>|</span><br><span class="line">|   Finance|         <span class="number">0</span>| <span class="number">10200</span>|</span><br><span class="line">|     Sales|         <span class="number">0</span>| <span class="number">18800</span>|</span><br><span class="line">|      null|         <span class="number">1</span>| <span class="number">34000</span>|</span><br><span class="line">+----------+----------+------+</span><br></pre></td></tr></table></figure>
<h3 id="3-2-grouping-id-帮助轻松找到自己想要的信息。"><a href="#3-2-grouping-id-帮助轻松找到自己想要的信息。" class="headerlink" title="3.2 grouping_id: 帮助轻松找到自己想要的信息。"></a>3.2 <code>grouping_id</code>: 帮助轻松找到自己想要的信息。</h3><p>返回分组级别: (分組(c1) &lt;&lt; (n-1)) + (分組(c2) &lt;&lt; (n-2)) + … + 分組(cn)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">df.cube(<span class="string">'department'</span>, <span class="string">'employee_name'</span>, <span class="string">'year'</span>).agg(</span><br><span class="line">    F.grouping_id().alias(<span class="string">'group_level'</span>), </span><br><span class="line">    F.sum(<span class="string">'salary'</span>).alias(<span class="string">'salary'</span>)</span><br><span class="line">).orderBy(F.desc(<span class="string">'group_level'</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-------------+----+-----------+------+</span><br><span class="line">|department|employee_name|year|group_level|salary|</span><br><span class="line">+----------+-------------+----+-----------+------+</span><br><span class="line">|      null|         null|null|          <span class="number">7</span>| <span class="number">34000</span>|</span><br><span class="line">|      null|         null|<span class="number">2020</span>|          <span class="number">6</span>| <span class="number">31000</span>|</span><br><span class="line">|      null|         null|<span class="number">2019</span>|          <span class="number">6</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|         Jeff|null|          <span class="number">5</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|      Michael|null|          <span class="number">5</span>|  <span class="number">4600</span>|</span><br><span class="line">|      null|        James|null|          <span class="number">5</span>|  <span class="number">6000</span>|</span><br><span class="line">|      null|        Kumar|null|          <span class="number">5</span>|  <span class="number">2000</span>|</span><br><span class="line">|      null|        Maria|null|          <span class="number">5</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|         Saif|null|          <span class="number">5</span>|  <span class="number">4100</span>|</span><br><span class="line">|      null|       Robert|null|          <span class="number">5</span>|  <span class="number">4100</span>|</span><br><span class="line">|      null|        Scott|null|          <span class="number">5</span>|  <span class="number">3300</span>|</span><br><span class="line">|      null|          Jen|null|          <span class="number">5</span>|  <span class="number">3900</span>|</span><br><span class="line">|      null|      Michael|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">4600</span>|</span><br><span class="line">|      null|       Robert|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">4100</span>|</span><br><span class="line">|      null|        Kumar|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">2000</span>|</span><br><span class="line">|      null|        James|<span class="number">2019</span>|          <span class="number">4</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|         Saif|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">4100</span>|</span><br><span class="line">|      null|        James|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|        Maria|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">3000</span>|</span><br><span class="line">|      null|        Scott|<span class="number">2020</span>|          <span class="number">4</span>|  <span class="number">3300</span>|</span><br><span class="line">+----------+-------------+----+-----------+------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br></pre></td></tr></table></figure>
<h2 id="4、grouping-set：分组集，跨多个组的聚合操作，仅SQL"><a href="#4、grouping-set：分组集，跨多个组的聚合操作，仅SQL" class="headerlink" title="4、grouping set：分组集，跨多个组的聚合操作，仅SQL"></a>4、grouping set：分组集，跨多个组的聚合操作，仅SQL</h2><p>希望获得所有用户的各种股票的数量</p>
<p>sql: </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> CustomerId, stockCode, <span class="keyword">sum</span>(Quantity) <span class="keyword">FROM</span> dfNoNull</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> CustomerId, stockCode</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId <span class="keyword">DESC</span>, stockCode <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure>
<p>grouping set也可以实现完全相同的操作：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> CustomerId, stockCode, <span class="keyword">sum</span>(Quantity) <span class="keyword">FROM</span> dfNoNull</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> CustomerId, stockCode </span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((CustomerId, stockCode))</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId <span class="keyword">DESC</span>, stockCode <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure>
<p>如果想不区分客户和股票来统计股票总数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> CustomerId, stockCode, <span class="keyword">sum</span>(Quantity) <span class="keyword">FROM</span> dfNoNull</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> CustomerId, stockCode </span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((CustomerId, stockCode), ())</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId <span class="keyword">DESC</span>, stockCode <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure>
<h1 id="四、窗口函数"><a href="#四、窗口函数" class="headerlink" title="四、窗口函数"></a>四、窗口函数</h1><h2 id="1-概念："><a href="#1-概念：" class="headerlink" title="1. 概念："></a>1. 概念：</h2><blockquote>
<p> 能返回整个dataframe，也能进行聚合运算。</p>
</blockquote>
<blockquote>
<p> 对于一个数据集，</p>
<ul>
<li><p><code>map</code> 是对每行进行操作，为每行得到一个结果；</p>
</li>
<li><p><code>reduce</code> 则是对多行进行操作，得到一个结果；</p>
</li>
<li><p><code>window</code> 函数则是对多行进行操作，得到多个结果（每行一个）。</p>
</li>
</ul>
<p>窗口函数使用时由“窗口函数”和over从句组成；</p>
<p>其中，over从句分为三部分：</p>
<ul>
<li>分组（partition by）、</li>
<li>排序（order by）、</li>
<li>frame选取（rangeBetween 和 rowsBetween）。</li>
</ul>
</blockquote>
<p>Window函数分类为三种：</p>
<ul>
<li>排名函数 <code>ranking functions</code>包括:<ul>
<li>row_number()：连续不重复。</li>
<li>rank()：连续重复</li>
<li>dense_rank()：重复不连续</li>
<li>percent_rank()：对dense_rank归一化</li>
<li>ntile()：n等份</li>
</ul>
</li>
<li>解析函数 <code>analytic functions</code>包括:<ul>
<li>cume_dist()：对rank归一化</li>
<li>lag()</li>
<li>lead()</li>
</ul>
</li>
<li>聚合函数 <code>aggregate functions</code>包括:<ul>
<li>sum()</li>
<li>first()</li>
<li>last()</li>
<li>max()</li>
<li>min()</li>
<li>mean()</li>
<li>stddev()</li>
</ul>
</li>
</ul>
<p>例子：找到每年当中最冷那一天的温度 /  最冷那一天的日期。</p>
<ul>
<li>窗口函数的过程</li>
</ul>
<ol>
<li>根据某个条件对数据进行分组，PartitionBy</li>
<li>根据需求计算聚合函数</li>
<li>将计算结果Join回一个大dataframe</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line">each_year = Window.partitionBy(<span class="string">"year"</span>)</span><br><span class="line">gsod.withColumn(<span class="string">'min_temp'</span>,F.min(<span class="string">"temp"</span>).over(each_year))\</span><br><span class="line">.where(<span class="string">"temp=min_temp"</span>)\</span><br><span class="line">.select(<span class="string">"year"</span>, <span class="string">"month"</span>, <span class="string">"day"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-三种函数的例子"><a href="#2-三种函数的例子" class="headerlink" title="2.三种函数的例子"></a>2.三种函数的例子</h2><h3 id="（1）创建一个-PySpark-DataFrame"><a href="#（1）创建一个-PySpark-DataFrame" class="headerlink" title="（1）创建一个 PySpark DataFrame"></a>（1）创建一个 PySpark DataFrame</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line">employee_salary = [</span><br><span class="line">    (<span class="string">"Ali"</span>, <span class="string">"Sales"</span>, <span class="number">8000</span>),</span><br><span class="line">    (<span class="string">"Bob"</span>, <span class="string">"Sales"</span>, <span class="number">7000</span>),</span><br><span class="line">    (<span class="string">"Cindy"</span>, <span class="string">"Sales"</span>, <span class="number">7500</span>),</span><br><span class="line">    (<span class="string">"Davd"</span>, <span class="string">"Finance"</span>, <span class="number">10000</span>),</span><br><span class="line">    (<span class="string">"Elena"</span>, <span class="string">"Sales"</span>, <span class="number">8000</span>),</span><br><span class="line">    (<span class="string">"Fancy"</span>, <span class="string">"Finance"</span>, <span class="number">12000</span>),</span><br><span class="line">    (<span class="string">"George"</span>, <span class="string">"Finance"</span>, <span class="number">11000</span>),</span><br><span class="line">    (<span class="string">"Haffman"</span>, <span class="string">"Marketing"</span>, <span class="number">7000</span>),</span><br><span class="line">    (<span class="string">"Ilaja"</span>, <span class="string">"Marketing"</span>, <span class="number">8000</span>),</span><br><span class="line">    (<span class="string">"Joey"</span>, <span class="string">"Sales"</span>, <span class="number">9000</span>)]</span><br><span class="line"> </span><br><span class="line">columns= [<span class="string">"name"</span>, <span class="string">"department"</span>, <span class="string">"salary"</span>]</span><br><span class="line">df = spark.createDataFrame(data = employee_salary, schema = columns)</span><br><span class="line">df.show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+</span><br><span class="line">|name   |department|salary|</span><br><span class="line">+-------+----------+------+</span><br><span class="line">|Ali    |Sales     |<span class="number">8000</span>  |</span><br><span class="line">|Bob    |Sales     |<span class="number">7000</span>  |</span><br><span class="line">|Cindy  |Sales     |<span class="number">7500</span>  |</span><br><span class="line">|Davd   |Finance   |<span class="number">10000</span> |</span><br><span class="line">|Elena  |Sales     |<span class="number">8000</span>  |</span><br><span class="line">|Fancy  |Finance   |<span class="number">12000</span> |</span><br><span class="line">|George |Finance   |<span class="number">11000</span> |</span><br><span class="line">|Haffman|Marketing |<span class="number">7000</span>  |</span><br><span class="line">|Ilaja  |Marketing |<span class="number">8000</span>  |</span><br><span class="line">|Joey   |Sales     |<span class="number">9000</span>  |</span><br><span class="line">+-------+----------+------+</span><br></pre></td></tr></table></figure>
<h3 id="（2）排名函数-ranking-functions"><a href="#（2）排名函数-ranking-functions" class="headerlink" title="（2）排名函数 ranking functions"></a>（2）排名函数 <code>ranking functions</code></h3><h4 id="2-1-row-number"><a href="#2-1-row-number" class="headerlink" title="2.1 row_number()"></a>2.1 <code>row_number()</code></h4><p><code>row_number()</code> 窗口函数用于给出从1开始到每个窗口分区的结果的连续行号。 与 <code>groupBy</code> 不同 <code>Window</code> 以 <code>partitionBy</code> 作为分组条件，<code>orderBy</code> 对 <code>Window</code> 分组内的数据进行排序。</p>
<p><strong>同分数不并列。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以 department 字段进行分组，以 salary 倒序排序</span></span><br><span class="line"><span class="comment"># 按照部门对薪水排名，薪水最低的为第一名</span></span><br><span class="line">windowSpec = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.asc(<span class="string">"salary"</span>))</span><br><span class="line"><span class="comment"># 分组内增加 row_number</span></span><br><span class="line">df_part = df.withColumn(</span><br><span class="line">    <span class="string">"row_number"</span>, </span><br><span class="line">    F.row_number().over(windowSpec)</span><br><span class="line">)</span><br><span class="line">print(df_part.toPandas().to_markdown())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">|    | name    | department   |   salary |   row_number |</span><br><span class="line">|---:|:--------|:-------------|---------:|-------------:|</span><br><span class="line">|  <span class="number">0</span> | Bob     | Sales        |     <span class="number">7000</span> |            <span class="number">1</span> |</span><br><span class="line">|  <span class="number">1</span> | Cindy   | Sales        |     <span class="number">7500</span> |            <span class="number">2</span> |</span><br><span class="line">|  <span class="number">2</span> | Ali     | Sales        |     <span class="number">8000</span> |            <span class="number">3</span> |</span><br><span class="line">|  <span class="number">3</span> | Elena   | Sales        |     <span class="number">8000</span> |            <span class="number">4</span> |</span><br><span class="line">|  <span class="number">4</span> | Joey    | Sales        |     <span class="number">9000</span> |            <span class="number">5</span> |</span><br><span class="line">|  <span class="number">5</span> | Davd    | Finance      |    <span class="number">10000</span> |            <span class="number">1</span> |</span><br><span class="line">|  <span class="number">6</span> | George  | Finance      |    <span class="number">11000</span> |            <span class="number">2</span> |</span><br><span class="line">|  <span class="number">7</span> | Fancy   | Finance      |    <span class="number">12000</span> |            <span class="number">3</span> |</span><br><span class="line">|  <span class="number">8</span> | Haffman | Marketing    |     <span class="number">7000</span> |            <span class="number">1</span> |</span><br><span class="line">|  <span class="number">9</span> | Ilaja   | Marketing    |     <span class="number">8000</span> |            <span class="number">2</span> |</span><br></pre></td></tr></table></figure>
<p>观察上面的数据，发现同样的薪水会有不同的排名（都是8000的薪水，有的第二有的第三），这是因为<code>row_number()</code>是按照行来给定序号，其不关注实际数值的大小。由此我们可以引申出另一个用于给出排序数的函数rank。</p>
<h5 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h5><ul>
<li>选取本部门工资收入第N高的记录</li>
<li>（思考）选取某日第N笔交易记录</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(df_part.where(F.col(<span class="string">'row_number'</span>) == <span class="number">2</span>).toPandas().to_markdown())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">|    | name   | department   |   salary |   row_number |</span><br><span class="line">|---:|:-------|:-------------|---------:|-------------:|</span><br><span class="line">|  <span class="number">0</span> | Cindy  | Sales        |     <span class="number">7500</span> |            <span class="number">2</span> |</span><br><span class="line">|  <span class="number">1</span> | George | Finance      |    <span class="number">11000</span> |            <span class="number">2</span> |</span><br><span class="line">|  <span class="number">2</span> | Ilaja  | Marketing    |     <span class="number">8000</span> |            <span class="number">2</span> |</span><br></pre></td></tr></table></figure>
<h4 id="2-2-rank"><a href="#2-2-rank" class="headerlink" title="2.2 rank()"></a>2.2 <code>rank()</code></h4><p><strong>同分数并列。下一个顺延。</strong></p>
<p><code>rank()</code>用来给按照指定列排序的分组窗增加一个排序的序号，</p>
<p>如果有相同数值，则排序数相同，下一个序数顺延一位。来看如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 rank 排序，都是8000的薪水，就同列第二</span></span><br><span class="line">windowSpec = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">df_rank = df.withColumn(<span class="string">"rank"</span>, F.rank().over(windowSpec))</span><br><span class="line">print(df_rank.toPandas().to_markdown())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">|    | name    | department   |   salary |   rank |</span><br><span class="line">|---:|:--------|:-------------|---------:|-------:|</span><br><span class="line">|  <span class="number">0</span> | Joey    | Sales        |     <span class="number">9000</span> |      <span class="number">1</span> |</span><br><span class="line">|  <span class="number">1</span> | Ali     | Sales        |     <span class="number">8000</span> |      <span class="number">2</span> |</span><br><span class="line">|  <span class="number">2</span> | Elena   | Sales        |     <span class="number">8000</span> |      <span class="number">2</span> |</span><br><span class="line">|  <span class="number">3</span> | Cindy   | Sales        |     <span class="number">7500</span> |      <span class="number">4</span> |</span><br><span class="line">|  <span class="number">4</span> | Bob     | Sales        |     <span class="number">7000</span> |      <span class="number">5</span> |</span><br><span class="line">|  <span class="number">5</span> | Fancy   | Finance      |    <span class="number">12000</span> |      <span class="number">1</span> |</span><br><span class="line">|  <span class="number">6</span> | George  | Finance      |    <span class="number">11000</span> |      <span class="number">2</span> |</span><br><span class="line">|  <span class="number">7</span> | Davd    | Finance      |    <span class="number">10000</span> |      <span class="number">3</span> |</span><br><span class="line">|  <span class="number">8</span> | Ilaja   | Marketing    |     <span class="number">8000</span> |      <span class="number">1</span> |</span><br><span class="line">|  <span class="number">9</span> | Haffman | Marketing    |     <span class="number">7000</span> |      <span class="number">2</span> |</span><br><span class="line">print(df_rank.where(F.col(<span class="string">"rank"</span>)==<span class="string">"2"</span>).toPandas().to_markdown())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">|    | name    | department   |   salary |   rank |</span><br><span class="line">|---:|:--------|:-------------|---------:|-------:|</span><br><span class="line">|  <span class="number">0</span> | Ali     | Sales        |     <span class="number">8000</span> |      <span class="number">2</span> |</span><br><span class="line">|  <span class="number">1</span> | Elena   | Sales        |     <span class="number">8000</span> |      <span class="number">2</span> |</span><br><span class="line">|  <span class="number">2</span> | George  | Finance      |    <span class="number">11000</span> |      <span class="number">2</span> |</span><br><span class="line">|  <span class="number">3</span> | Haffman | Marketing    |     <span class="number">7000</span> |      <span class="number">2</span> |</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">window = Window.partitionBy([<span class="string">'a'</span>]).orderBy([<span class="string">'a'</span>])</span><br><span class="line">df.withColumn(<span class="string">'rank'</span>,F.rank().over(window)).filter(<span class="string">"rank = '1'"</span>).drop(<span class="string">'rank'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-3-dense-rank"><a href="#2-3-dense-rank" class="headerlink" title="2.3 dense_rank"></a>2.3 <code>dense_rank</code></h4><p>观察 <code>dense_rank</code> 与 <code>rank</code> 的区别。</p>
<p><strong>同分数并列且下一个不顺延。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意 rank 排序，8000虽然为同列第二，但7500属于第4名</span></span><br><span class="line"><span class="comment"># dense_rank()中， 8000同列第二后，7500属于第3名</span></span><br><span class="line">windowSpec  = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">df.withColumn(<span class="string">"dense_rank"</span>, F.dense_rank().over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+----------+</span><br><span class="line">|   name|department|salary|dense_rank|</span><br><span class="line">+-------+----------+------+----------+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>|         <span class="number">1</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>|         <span class="number">2</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>|         <span class="number">2</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>|         <span class="number">3</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>|         <span class="number">4</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|         <span class="number">1</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|         <span class="number">2</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|         <span class="number">3</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>|         <span class="number">1</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>|         <span class="number">2</span>|</span><br><span class="line">+-------+----------+------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-4-percent-rank-：百分比排序。（将-dense-rank-的结果进行归一化）"><a href="#2-4-percent-rank-：百分比排序。（将-dense-rank-的结果进行归一化）" class="headerlink" title="2.4 percent_rank()：百分比排序。（将 dense_rank() 的结果进行归一化）"></a>2.4 percent_rank()：百分比排序。（将 <code>dense_rank()</code> 的结果进行归一化）</h4><p>一些业务场景下，我们需要计算不同数值的百分比排序数据，先来看一个例子吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">windowSpec  = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">df.withColumn(<span class="string">"percent_rank"</span>,F.percent_rank().over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+------------+</span><br><span class="line">|   name|department|salary|percent_rank|</span><br><span class="line">+-------+----------+------+------------+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>|         <span class="number">0.0</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>|        <span class="number">0.25</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>|        <span class="number">0.25</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>|        <span class="number">0.75</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>|         <span class="number">1.0</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|         <span class="number">0.0</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|         <span class="number">0.5</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|         <span class="number">1.0</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>|         <span class="number">0.0</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>|         <span class="number">1.0</span>|</span><br><span class="line">+-------+----------+------+------------+</span><br></pre></td></tr></table></figure>
<p>上述结果可以理解为将 <code>dense_rank()</code> 的结果进行归一化， 即可得到<code>0-1</code>以内的百分数。<code>percent_rank()</code> 与 <code>SQL</code> 中的 <code>PERCENT_RANK</code> 函数效果一致。</p>
<h4 id="2-5-ntile-：分组切分n均等数据。"><a href="#2-5-ntile-：分组切分n均等数据。" class="headerlink" title="2.5 ntile()：分组切分n均等数据。"></a>2.5 <code>ntile()</code>：分组切分n均等数据。</h4><p><code>ntile()</code>可将分组的数据按照指定数值<code>n</code>切分为<code>n</code>个部分， 每一部分按照行的先后给定相同的序数。例如n指定为2，则将组内数据分为两个部分， 第一部分序号为1，第二部分序号为2。理论上两部分数据行数是均等的， 但当数据为奇数行时，中间的那一行归到前一部分。</p>
<p>按照部门对数据进行分组，然后在组内按照薪水高低进行排序， 再使用 <code>ntile()</code> 将组内数据切分为两个部分。结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按照部门对数据进行分组，然后在组内按照薪水高低进行排序 </span></span><br><span class="line">windowSpec = Window.partitionBy(</span><br><span class="line">    <span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line"><span class="comment"># 使用ntile() 将组内数据切分为两个部分</span></span><br><span class="line">df.withColumn(<span class="string">"ntile"</span>, F.ntile(<span class="number">2</span>).over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+-----+</span><br><span class="line">|   name|department|salary|ntile|</span><br><span class="line">+-------+----------+------+-----+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>|    <span class="number">1</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>|    <span class="number">1</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>|    <span class="number">1</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>|    <span class="number">2</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>|    <span class="number">2</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|    <span class="number">1</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|    <span class="number">1</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|    <span class="number">2</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>|    <span class="number">1</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>|    <span class="number">2</span>|</span><br><span class="line">+-------+----------+------+-----+</span><br></pre></td></tr></table></figure>
<h3 id="（3）-Analytic-functions"><a href="#（3）-Analytic-functions" class="headerlink" title="（3）. Analytic functions"></a>（3）. Analytic functions</h3><h4 id="3-1-cume-dist-：（将-rank-的结果进行归一化），和percent-rank很像。"><a href="#3-1-cume-dist-：（将-rank-的结果进行归一化），和percent-rank很像。" class="headerlink" title="3.1 cume_dist()：（将 rank() 的结果进行归一化），和percent_rank很像。"></a>3.1 <code>cume_dist()</code>：（将 <code>rank()</code> 的结果进行归一化），和percent_rank很像。</h4><p><code>cume_dist()</code>函数用来获取数值的累进分布值，看如下例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">windowSpec = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">df.withColumn(</span><br><span class="line">    <span class="string">"cume_dist"</span>, F.cume_dist().over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+------------------+</span><br><span class="line">|   name|department|salary|         cume_dist|</span><br><span class="line">+-------+----------+------+------------------+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>|               <span class="number">0.2</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>|               <span class="number">0.6</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>|               <span class="number">0.6</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>|               <span class="number">0.8</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>|               <span class="number">1.0</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|<span class="number">0.3333333333333333</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|<span class="number">0.6666666666666666</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|               <span class="number">1.0</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>|               <span class="number">0.5</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>|               <span class="number">1.0</span>|</span><br><span class="line">+-------+----------+------+------------------+</span><br><span class="line"><span class="comment"># 和 percent_rank 对比一下</span></span><br><span class="line">df.withColumn(</span><br><span class="line">    <span class="string">'percent_rank'</span>,</span><br><span class="line">    F.percent_rank().over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+------------+</span><br><span class="line">|   name|department|salary|percent_rank|</span><br><span class="line">+-------+----------+------+------------+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>|         <span class="number">0.0</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>|        <span class="number">0.25</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>|        <span class="number">0.25</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>|        <span class="number">0.75</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>|         <span class="number">1.0</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|         <span class="number">0.0</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|         <span class="number">0.5</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|         <span class="number">1.0</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>|         <span class="number">0.0</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>|         <span class="number">1.0</span>|</span><br><span class="line">+-------+----------+------+------------+</span><br></pre></td></tr></table></figure>
<p>结果好像和前面的<code>percent_rank()</code>很类似对不对，于是我们联想到这个其实也是一种归一化结果， 其按照 <code>rank()</code> 的结果进行归一化处理。回想一下前面讲过的 <code>rank()</code> 函数，并列排序会影响后续排序， 于是序号中间可能存在隔断。这样Sales组的排序数就是1、2、2、4、5， 归一化以后就得到了0.2、0.6、0.6、0.8、1。这个统计结果按照实际业务来理解就是：</p>
<ul>
<li>9000及以上的人占了20%，</li>
<li>8000及以上的人占了60%，</li>
<li>7500以上的人数占了80%，</li>
<li>7000以上的人数占了100%，</li>
</ul>
<h4 id="3-2-lag-：排序后找上一个数值。"><a href="#3-2-lag-：排序后找上一个数值。" class="headerlink" title="3.2 lag()：排序后找上一个数值。"></a>3.2 <code>lag()</code>：排序后找上一个数值。</h4><p><code>lag()</code> 函数用于寻找按照指定列排好序的分组内每个数值的上一个数值，</p>
<p>通俗的说，就是数值排好序以后，寻找排在每个数值的上一个数值。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相当于滞后项</span></span><br><span class="line">windowSpec  = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">df.withColumn(<span class="string">"lag"</span>, F.lag(<span class="string">"salary"</span>,<span class="number">1</span>).over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+-----+</span><br><span class="line">|   name|department|salary|  lag|</span><br><span class="line">+-------+----------+------+-----+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>| null|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>| <span class="number">9000</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>| <span class="number">8000</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>| <span class="number">8000</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>| <span class="number">7500</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>| null|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|<span class="number">12000</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|<span class="number">11000</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>| null|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>| <span class="number">8000</span>|</span><br><span class="line">+-------+----------+------+-----+</span><br></pre></td></tr></table></figure>
<h4 id="3-3-lead-：排序后找下一个数值。"><a href="#3-3-lead-：排序后找下一个数值。" class="headerlink" title="3.3 lead()：排序后找下一个数值。"></a>3.3 <code>lead()</code>：排序后找下一个数值。</h4><p><code>lead()</code> 用于获取排序后的数值的下一个，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 和滞后项相反，提前一位</span></span><br><span class="line">windowSpec  = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">df.withColumn(<span class="string">"lead"</span>,F.lead(<span class="string">"salary"</span>,<span class="number">1</span>).over(windowSpec)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+-----+</span><br><span class="line">|   name|department|salary| lead|</span><br><span class="line">+-------+----------+------+-----+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>| <span class="number">8000</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>| <span class="number">8000</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>| <span class="number">7500</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>| <span class="number">7000</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>| null|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|<span class="number">11000</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|<span class="number">10000</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>| null|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>| <span class="number">7000</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>| null|</span><br><span class="line">+-------+----------+------+-----+</span><br></pre></td></tr></table></figure>
<ol>
<li>实际业务场景中，假设我们获取了每个月的销售数据， 我们可能想要知道，某月份与上一个月或下一个月数据相比怎么样， 于是就可以使用<code>lag</code>和<code>lead</code>来进行数据分析了。</li>
<li>思考差分如何做？增长率如何做（同比、环比）？</li>
</ol>
<h3 id="（4）-Aggregate-Functions"><a href="#（4）-Aggregate-Functions" class="headerlink" title="（4）. Aggregate Functions"></a>（4）. Aggregate Functions</h3><p>常见的聚合函数有<code>avg, sum, min, max, count, approx_count_distinct()</code>等，我们用如下代码来同时使用这些函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分组，并对组内数据排序</span></span><br><span class="line">windowSpec  = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line"><span class="comment"># 仅分组</span></span><br><span class="line">windowSpecAgg  = Window.partitionBy(<span class="string">"department"</span>)</span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">"row"</span>, F.row_number().over(windowSpec)) \</span><br><span class="line">  .withColumn(<span class="string">"avg"</span>, F.avg(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"sum"</span>, F.sum(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"min"</span>, F.min(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"max"</span>, F.max(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"count"</span>, F.count(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"distinct_count"</span>, F.approxCountDistinct(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+</span><br><span class="line">|   name|department|salary|row|    avg|  sum|  min|  max|count|distinct_count|</span><br><span class="line">+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+</span><br><span class="line">|   Joey|     Sales|  <span class="number">9000</span>|  <span class="number">1</span>| <span class="number">7900.0</span>|<span class="number">39500</span>| <span class="number">7000</span>| <span class="number">9000</span>|    <span class="number">5</span>|             <span class="number">4</span>|</span><br><span class="line">|    Ali|     Sales|  <span class="number">8000</span>|  <span class="number">2</span>| <span class="number">7900.0</span>|<span class="number">39500</span>| <span class="number">7000</span>| <span class="number">9000</span>|    <span class="number">5</span>|             <span class="number">4</span>|</span><br><span class="line">|  Elena|     Sales|  <span class="number">8000</span>|  <span class="number">3</span>| <span class="number">7900.0</span>|<span class="number">39500</span>| <span class="number">7000</span>| <span class="number">9000</span>|    <span class="number">5</span>|             <span class="number">4</span>|</span><br><span class="line">|  Cindy|     Sales|  <span class="number">7500</span>|  <span class="number">4</span>| <span class="number">7900.0</span>|<span class="number">39500</span>| <span class="number">7000</span>| <span class="number">9000</span>|    <span class="number">5</span>|             <span class="number">4</span>|</span><br><span class="line">|    Bob|     Sales|  <span class="number">7000</span>|  <span class="number">5</span>| <span class="number">7900.0</span>|<span class="number">39500</span>| <span class="number">7000</span>| <span class="number">9000</span>|    <span class="number">5</span>|             <span class="number">4</span>|</span><br><span class="line">|  Fancy|   Finance| <span class="number">12000</span>|  <span class="number">1</span>|<span class="number">11000.0</span>|<span class="number">33000</span>|<span class="number">10000</span>|<span class="number">12000</span>|    <span class="number">3</span>|             <span class="number">3</span>|</span><br><span class="line">| George|   Finance| <span class="number">11000</span>|  <span class="number">2</span>|<span class="number">11000.0</span>|<span class="number">33000</span>|<span class="number">10000</span>|<span class="number">12000</span>|    <span class="number">3</span>|             <span class="number">3</span>|</span><br><span class="line">|   Davd|   Finance| <span class="number">10000</span>|  <span class="number">3</span>|<span class="number">11000.0</span>|<span class="number">33000</span>|<span class="number">10000</span>|<span class="number">12000</span>|    <span class="number">3</span>|             <span class="number">3</span>|</span><br><span class="line">|  Ilaja| Marketing|  <span class="number">8000</span>|  <span class="number">1</span>| <span class="number">7500.0</span>|<span class="number">15000</span>| <span class="number">7000</span>| <span class="number">8000</span>|    <span class="number">2</span>|             <span class="number">2</span>|</span><br><span class="line">|Haffman| Marketing|  <span class="number">7000</span>|  <span class="number">2</span>| <span class="number">7500.0</span>|<span class="number">15000</span>| <span class="number">7000</span>| <span class="number">8000</span>|    <span class="number">2</span>|             <span class="number">2</span>|</span><br><span class="line">+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+</span><br></pre></td></tr></table></figure>
<p>需要注意的是 <code>approx_count_distinct()</code> 函数适用与窗函数的统计， 而在<code>groupby</code>中通常用<code>countDistinct()</code>来代替该函数，用来求组内不重复的数值的条数。</p>
<p>从结果来看，统计值基本上是按照部门分组，统计组内的salary情况。 如果我们只想要保留部门的统计结果，而将每个人的实际情况去掉，可以采用如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">windowSpec  = Window.partitionBy(<span class="string">"department"</span>).orderBy(F.desc(<span class="string">"salary"</span>))</span><br><span class="line">windowSpecAgg  = Window.partitionBy(<span class="string">"department"</span>)</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">"row"</span>, F.row_number().over(windowSpec)) \</span><br><span class="line">  .withColumn(<span class="string">"avg"</span>, F.avg(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"sum"</span>, F.sum(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"min"</span>, F.min(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"max"</span>, F.max(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"count"</span>, F.count(<span class="string">"salary"</span>).over(windowSpecAgg)) \</span><br><span class="line">  .withColumn(<span class="string">"distinct_count"</span>, F.approx_count_distinct(<span class="string">"salary"</span>).over(windowSpecAgg))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅选取分组第一行数据</span></span><br><span class="line"><span class="comment"># 用F.col 去选row 行，怪怪的</span></span><br><span class="line">df_part  = df.where(F.col(<span class="string">"row"</span>)==<span class="number">1</span>)</span><br><span class="line">df_part.select(<span class="string">"department"</span>,<span class="string">"avg"</span>,<span class="string">"sum"</span>,<span class="string">"min"</span>,<span class="string">"max"</span>,<span class="string">"count"</span>,<span class="string">"distinct_count"</span>).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-------+-----+-----+-----+-----+--------------+</span><br><span class="line">|department|    avg|  sum|  min|  max|count|distinct_count|</span><br><span class="line">+----------+-------+-----+-----+-----+-----+--------------+</span><br><span class="line">|     Sales| <span class="number">7900.0</span>|<span class="number">39500</span>| <span class="number">7000</span>| <span class="number">9000</span>|    <span class="number">5</span>|             <span class="number">4</span>|</span><br><span class="line">|   Finance|<span class="number">11000.0</span>|<span class="number">33000</span>|<span class="number">10000</span>|<span class="number">12000</span>|    <span class="number">3</span>|             <span class="number">3</span>|</span><br><span class="line">| Marketing| <span class="number">7500.0</span>|<span class="number">15000</span>| <span class="number">7000</span>| <span class="number">8000</span>|    <span class="number">2</span>|             <span class="number">2</span>|</span><br><span class="line">+----------+-------+-----+-----+-----+-----+--------------+</span><br></pre></td></tr></table></figure>
<h2 id="3-frame函数"><a href="#3-frame函数" class="headerlink" title="3. frame函数"></a>3. frame函数</h2><h3 id="（1）Window-rangeBetween-start-end"><a href="#（1）Window-rangeBetween-start-end" class="headerlink" title="（1）Window.rangeBetween(start, end)"></a>（1）Window.rangeBetween(start, end)</h3><blockquote>
<p>创建一个<code>WindowSpec</code>，定义了从<code>start</code>(含)到<code>end</code>(含)的帧边界。start<code>和</code>end` 都是相对于当前行的。例如“0”表示“current row”，“-1”表示当前行前一关，“5”表示当前行后五关。</p>
</blockquote>
<p>基准为当前行</p>
<p>行数选择</p>
<ul>
<li>rowsBetween(x, y)</li>
<li>Window.unboundedPreceding 表示当前行之前的无限行</li>
<li>Window.currentRow 表示当前行</li>
<li>Window.unboundedFollowing 表示当前行之后的无限行</li>
</ul>
<p>rowsBetween(-1,1)：函数作用范围为当前行的上一行至下一行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Window</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc = SparkContext.getOrCreate()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sqlContext = SQLContext(sc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup = [(<span class="number">1</span>, <span class="string">"a"</span>), (<span class="number">1</span>, <span class="string">"a"</span>), (<span class="number">2</span>, <span class="string">"a"</span>), (<span class="number">1</span>, <span class="string">"b"</span>), (<span class="number">2</span>, <span class="string">"b"</span>), (<span class="number">3</span>, <span class="string">"b"</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = sqlContext.createDataFrame(tup, [<span class="string">"id"</span>, <span class="string">"category"</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>window = Window.partitionBy(<span class="string">"category"</span>).orderBy(<span class="string">"id"</span>).rangeBetween(Window.currentRow, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.withColumn(<span class="string">"sum"</span>, func.sum(<span class="string">"id"</span>).over(window)).sort(<span class="string">"id"</span>, <span class="string">"category"</span>).show()</span><br><span class="line">+---+--------+---+</span><br><span class="line">| id|category|sum|</span><br><span class="line">+---+--------+---+</span><br><span class="line">|  <span class="number">1</span>|       a|  <span class="number">4</span>|</span><br><span class="line">|  <span class="number">1</span>|       a|  <span class="number">4</span>|</span><br><span class="line">|  <span class="number">1</span>|       b|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|       a|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">2</span>|       b|  <span class="number">5</span>|</span><br><span class="line">|  <span class="number">3</span>|       b|  <span class="number">3</span>|</span><br><span class="line">+---+--------+---+</span><br></pre></td></tr></table></figure>
<h3 id="（2）rowsBetween"><a href="#（2）rowsBetween" class="headerlink" title="（2）rowsBetween"></a>（2）rowsBetween</h3><ul>
<li><code>ROWS BETWEEN</code>不关心确切的值.它只关心行的顺序,并在计算帧时采用固定数量的前后行. （比较行的顺序）</li>
<li><code>RANGE BETWEEN</code> 在计算帧时考虑值   .(比较值的大小)</li>
</ul>
<p>行范围设置 rangeBetween(x,y)<br>基准为当前行的值</p>
<ul>
<li>rangeBetween(20,50)<br>例如当前值为18<br>则选取的值范围为[-2,68]</li>
</ul>
<h2 id="4-例子"><a href="#4-例子" class="headerlink" title="4. 例子"></a>4. 例子</h2><h3 id="（1）小例子"><a href="#（1）小例子" class="headerlink" title="（1）小例子"></a>（1）小例子</h3><p>需求：组内按分数排序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select($<span class="string">"uid"</span>, $<span class="string">"date"</span>, $<span class="string">"score"</span>, row_number().over(Window.partitionBy(<span class="string">"uid"</span>).orderBy($<span class="string">"score"</span>.desc)).<span class="keyword">as</span>(<span class="string">"rank"</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://ask.qcloudimg.com/http-save/yehe-7131597/qi0imvjt5y.png?imageView2/2/w/1620" alt="img"></p>
<h3 id="（2）udf-窗口函数"><a href="#（2）udf-窗口函数" class="headerlink" title="（2）udf + 窗口函数"></a>（2）udf + 窗口函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Window</span><br><span class="line"></span><br><span class="line"><span class="meta">@pandas_udf("double")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_udf</span><span class="params">(v: pd.Series)</span> -&gt; float:</span></span><br><span class="line">    <span class="keyword">return</span> v.mean()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">1</span>, <span class="number">1.0</span>), (<span class="number">1</span>, <span class="number">2.0</span>), (<span class="number">2</span>, <span class="number">3.0</span>), (<span class="number">2</span>, <span class="number">5.0</span>), (<span class="number">2</span>, <span class="number">10.0</span>)], (<span class="string">"id"</span>, <span class="string">"v"</span>))</span><br><span class="line">w = Window.partitionBy(<span class="string">'id'</span>).orderBy(<span class="string">'v'</span>).rowsBetween(<span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line">df.withColumn(<span class="string">'mean_v'</span>, mean_udf(<span class="string">"v"</span>).over(w)).show()</span><br><span class="line"></span><br><span class="line">+---+----+------+</span><br><span class="line">| id|   v|mean_v|</span><br><span class="line">+---+----+------+</span><br><span class="line">|  <span class="number">1</span>| <span class="number">1.0</span>|   <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">1</span>| <span class="number">2.0</span>|   <span class="number">1.5</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">3.0</span>|   <span class="number">3.0</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">5.0</span>|   <span class="number">4.0</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="number">10.0</span>|   <span class="number">7.5</span>|</span><br><span class="line">+---+----+------+</span><br></pre></td></tr></table></figure>
<h3 id="（3）同时groupby-两个key"><a href="#（3）同时groupby-两个key" class="headerlink" title="（3）同时groupby 两个key"></a>（3）同时groupby 两个key</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成首次付费时间(FirstPayDate), 首次付费当天的付费金额(FirstPayPrice), 总付费金额(AllPayPrice). 首次单天付费频次(FirstPayFreq), 总付费频次(AllPayFreq)</span></span><br><span class="line">w1 = Window.partitionBy(<span class="string">"OrderId"</span>).orderBy(col(<span class="string">"PayDate"</span>))</span><br><span class="line">w2 = Window.partitionBy(<span class="string">"OrderId"</span>)</span><br><span class="line">df_pay_data = df_order_all.filter((F.col(<span class="string">"OrderType"</span>) == <span class="number">0</span>) &amp; ((F.col(<span class="string">"IsPay"</span>) == <span class="number">1</span>)))\</span><br><span class="line">    .withColumnRenamed(<span class="string">"OrderTime"</span>, <span class="string">"PayTime"</span>) \</span><br><span class="line">    .withColumnRenamed(<span class="string">"OrderPrice"</span>, <span class="string">"PayPrice"</span>) \</span><br><span class="line">    .withColumn(<span class="string">"PayDate"</span>, date_trunc(<span class="string">'day'</span>, to_timestamp(F.col(<span class="string">"PayTime"</span>)/<span class="number">1000</span>)))\</span><br><span class="line">    .withColumn(<span class="string">"row"</span>,row_number().over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"AllPayPrice"</span>, sum(col(<span class="string">"PayPrice"</span>)).over(w2)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayDate"</span>, min(col(<span class="string">"PayDate"</span>)).over(w2)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayPrice"</span>, sum(col(<span class="string">"PayPrice"</span>)).over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayFreq"</span>, count(col(<span class="string">"IsPay"</span>)).over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"AllPayFreq"</span>, count(col(<span class="string">"IsPay"</span>)).over(w2)) \</span><br><span class="line">    .where(col(<span class="string">"row"</span>) == <span class="number">1</span>).select(<span class="string">"AllPayPrice"</span>, <span class="string">"FirstPayPrice"</span>, <span class="string">"FirstPayDate"</span>,<span class="string">"OrderId"</span>, <span class="string">"FirstPayFreq"</span>, <span class="string">"AllPayFreq"</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/" target="_blank" rel="noopener">https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/</a></p>
<p><a href="https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe" target="_blank" rel="noopener">https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe</a> (相加)</p>
<h3 id="（4）groupby-sort-list"><a href="#（4）groupby-sort-list" class="headerlink" title="（4）groupby  + sort + list"></a>（4）groupby  + sort + list</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group_df = qds_com.groupby([<span class="string">'company'</span>]) \</span><br><span class="line">    .agg(F.sort_array(F.collect_list(F.struct(<span class="string">"features"</span>, <span class="string">"label"</span>, <span class="string">"samples_count"</span>))) \</span><br><span class="line">         .alias(<span class="string">"pair"</span>))</span><br></pre></td></tr></table></figure>
<h3 id="（5）求众数"><a href="#（5）求众数" class="headerlink" title="（5）求众数"></a>（5）求众数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_com_cookie.groupBy([<span class="string">'company'</span>,<span class="string">'price'</span>]).agg(F.count(<span class="string">'price'</span>).alias(<span class="string">'count_price'</span>)).orderBy([<span class="string">'company'</span>,<span class="string">'count_price'</span>], ascending=<span class="literal">False</span>).drop_duplicates(subset=[<span class="string">'company'</span>]).show()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Spark持久化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark持久化/" itemprop="url">10. Spark持久化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-19T00:00:00+08:00">
                2023-03-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>本文仅做学习总结，如有侵权立删</p>
<h1 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h1><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p><img src="E:\GitHub_learn\blog\source\imgs\RDD持久化.jpg" alt></p>
<blockquote>
<p>查看上面那段伪代码，两个count触发算子会产生两个job, 那么,这两个job会往回去找errors，lines这两个rdd，最后到磁盘上拿数据。也就是每个job都会去读一遍磁盘，这里可以做优化， 将errors这个rdd保存到内存中， 然后第一个count会去磁盘度数， 但第二个count直接可以从内存中读数据了。</p>
</blockquote>
<p><strong>控制算子有三种，cache,persist,checkpoint，以上算子都可以将RDD持久化，持久化的单位是partition。</strong></p>
<p><strong>cache和persist都是懒执行的。</strong></p>
<p><strong>必须有一个action类算子触发执行。checkpoint算子不仅能将RDD持久化到磁盘，还能切断RDD之间的依赖关系</strong></p>
<p>RDD 可以使用 persist() 方法或 cache() 方法进行持久化。数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。</p>
<p>在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据。这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法。</p>
<h2 id="存储级别"><a href="#存储级别" class="headerlink" title="存储级别"></a>存储级别</h2><blockquote>
<p>可以看到StorageLevel类的主构造器包含了5个参数：</p>
<ul>
<li>useDisk：使用硬盘（外存）</li>
<li>useMemory：使用内存</li>
<li>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li>
<li>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li>
<li>replication：备份数（在多个节点上备份）</li>
</ul>
</blockquote>
<p>每个持久化的 RDD 可以使用不同的存储级别进行缓存，例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。这些存储级别通过传递一个 StorageLevel 对象给 persist() 方法进行设置。<br>详细的存储级别介绍如下：</p>
<ul>
<li>MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。</li>
<li>MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。</li>
<li>MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。</li>
<li>MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。</li>
<li>DISK_ONLY : 只在磁盘上缓存 RDD。</li>
<li>MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。</li>
<li>OFF_HEAP）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。（Off-heap是指在堆外内存）</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="..\imgs\Spark持久化1.jpg" alt></p>
<h2 id="如何选择存储级别"><a href="#如何选择存储级别" class="headerlink" title="如何选择存储级别"></a>如何选择存储级别</h2><p>Spark 的存储级别的选择，核心问题是在<strong>内存使用率和 CPU 效率</strong>之间进行权衡。建议按下面的过程进行存储级别的选择 :</p>
<ul>
<li>如果使用默认的存储级别<strong>（MEMORY_ONLY）</strong>，存储在内存中的 RDD 没有发生溢出，那么就选择默认的存储级别。默认存储级别可以最大程度的提高 CPU 的效率,可以使在 RDD 上的操作以最快的速度运行。</li>
<li>如果内存不能全部存储 RDD，那么使用 <strong>MEMORY_ONLY_SER</strong>，并挑选一个快速序列化库将对象序列化，以节省内存空间。使用这种存储级别，计算速度仍然很快。</li>
<li>除了在计算该数据集的代价特别高，或者在需要过滤大量数据的情况下，尽量不要将溢出的数据存储到磁盘。因为，<strong>重新计算这个数据分区的耗时与从磁盘读取这些数据的耗时差不多。</strong></li>
<li>如果想快速还原故障，建议使用<strong>多副本存储级别</strong>（例如，使用 Spark 作为 web 应用的后台服务，在服务出故障时需要快速恢复的场景下）。所有的存储级别都通过重新计算丢失的数据的方式，提供了完全容错机制。但是多副本级别在发生数据丢失时，不需要重新计算对应的数据库，可以让任务继续运行。</li>
</ul>
<h1 id="2-persist"><a href="#2-persist" class="headerlink" title="2. persist"></a>2. persist</h1><p>persist使用场景：</p>
<ul>
<li>某个步骤计算非常耗时，需要进行persist持久化</li>
<li>计算链条非常长，重新恢复要算很多步骤</li>
<li>需要checkpoint的RDD最好进行persist，checkpoint机制会在job执行完成之后根据DAG向前回溯，找到需要进行checkpoint的RDD，另起一个job来计算该RDD，将计算结果存储到HDFS，如果在job执行的过程中对该RDD进行了persist，那么进行checkpoint会非常快</li>
<li>shuffle之前进行persist，Spark默认将数据持久化到磁盘，自动完成，无需干预</li>
<li>shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</li>
</ul>
<h1 id="3-cache"><a href="#3-cache" class="headerlink" title="3. cache"></a>3. cache</h1><h2 id="RDD的cache和persist的区别"><a href="#RDD的cache和persist的区别" class="headerlink" title="RDD的cache和persist的区别"></a>RDD的cache和persist的区别</h2><p>cache()调用的persist()，是使用默认存储级别的快捷设置方法（MEMORY_ONLY）</p>
<p>通过源码可以看出cache()是persist()的简化方式，调用persist的无参版本，也就是调用persist(StorageLevel.MEMORY_ONLY)，cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中，而persist可以通过传递一个 StorageLevel 对象来设置其它的缓存级别。</p>
<h2 id="4-unpersist-释放缓存："><a href="#4-unpersist-释放缓存：" class="headerlink" title="4. unpersist 释放缓存："></a>4. unpersist 释放缓存：</h2><p>Spark 自动监控各个节点上的缓存使用率，并以最近最少使用的方式（LRU）将旧数据块移除内存。如果想手动移除一个 RDD，而不是等待该 RDD 被 Spark 自动移除，可以使用 RDD.unpersist() 方法</p>
<p>注意：如果缓存的RDD之间有依赖关系，比如</p>
<p>val rdd_a = df.persist</p>
<p>val rdd_<em>b = rdd_a.filter.persist</em></p>
<p><em>val rdd_c = rdd_b.map.persist</em></p>
<p>在用unpersist清理缓存时，当首先清理rdd_a时，会重建rdd_b和rdd_c的缓存，如果数据量巨大，这个过程可能花费很长时间，即使rdd<em>b</em>和rdd_c后面也即将被清理，但是重建过程也会进行，可能会出现一个现象，所有job都以完成，但是任务长时间处于RUNNING状态，这可能就是因为最后再清理缓存时又会把依赖于它的RDD再重算一遍。这时可以只用使用<em>spark</em>.<em>sharedState</em>.<em>cacheManager</em>.uncacheQuery(df, cascade = true, blocking = false)来全部释放，参数cascade 表示是否清理所有引用此RDD的其他RDD，以下是unpersist的源码，可以一目了然</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(blocking: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.uncacheQuery(<span class="keyword">this</span>, cascade = <span class="literal">false</span>, blocking)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-checkpoint"><a href="#5-checkpoint" class="headerlink" title="5. checkpoint"></a>5. checkpoint</h2><ul>
<li><p>checkpoint将RDD持久化到磁盘，还可以切断RDD之间的依赖关系</p>
</li>
<li><p>checkpoint 的执行原理</p>
<ul>
<li>当RDD的job执行完毕后，会从finalRDD从后往前回溯。</li>
<li>当回溯到某一个RDD调用了checkpoint方法，会对当前的RDD做一个标记。</li>
<li>Spark框架会自动启动一个新的job，重新计算这个RDD的数据，将数据持久化到HDFS上。</li>
</ul>
</li>
<li><p>优化：<strong>对RDD执行checkpoint之前，最好对这个RDD先执行cache，这样新启动的job只需要将内存中的数据拷贝到HDFS上就可以，省去了重新计算这一步</strong></p>
</li>
<li><p>解释：cache 机制是每计算出一个要 cache 的 partition 就直接将其 cache 到内存了。但 checkpoint 没有使用这种第一次计算得到就存储的方法，而是等到 job 结束后另外启动专门的 job 去完成 checkpoint 。也就是说需要 checkpoint 的 RDD 会被计算两次。因此，在使用 rdd.checkpoint() 的时候，建议加上 rdd.cache()，这样第二次运行的 job 就不用再去计算该 rdd 了，直接读取 cache 写磁盘。</p>
</li>
</ul>
<h4 id="（1）cache-和-checkpoint-之间有一个重大的区别，"><a href="#（1）cache-和-checkpoint-之间有一个重大的区别，" class="headerlink" title="（1）cache 和 checkpoint 之间有一个重大的区别，"></a><strong>（1）cache 和 checkpoint 之间有一个重大的区别，</strong></h4><p><strong>cache 将 RDD 以及 RDD 的血统(记录了这个RDD如何产生)缓存到内存中</strong>，当缓存的 RDD 失效的时候(如内存损坏)，它们可以通过血统重新计算来进行恢复。但是 <strong>checkpoint 将 RDD 缓存到了 HDFS 中，同时忽略了它的血统(也就是RDD之前的那些依赖)</strong>。为什么要丢掉依赖？因为可以利用 HDFS 多副本特性保证容错！</p>
<h4 id="（2）persist与checkpoint的区别"><a href="#（2）persist与checkpoint的区别" class="headerlink" title="（2）persist与checkpoint的区别"></a>（2）persist与checkpoint的区别</h4><p>rdd.persist(StorageLevel.DISK_ONLY) 与 checkpoint 也有区别。前者虽然可以将 RDD 的 partition 持久化到磁盘，但该 partition 由 blockManager 管理。一旦 driver program 执行结束，也就是 executor 所在进程 CoarseGrainedExecutorBackend stop，blockManager 也会 stop，<strong>被 cache 到磁盘上的 RDD 也会被清空</strong>（整个 blockManager 使用的 local 文件夹被删除）。</p>
<p>而 <strong>checkpoint 将 RDD 持久化到 HDFS 或本地文件夹</strong>，如果不被手动 remove 掉，是一直存在的，也就是说可以被下一个 driver program 使用，而 cached RDD 不能被其他 dirver program 使用。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark相比Hadoop的优势在于尽量不去持久化，所以使用 pipeline，cache 等机制。用户如果感觉 job 可能会出错可以手动去 checkpoint 一些 critical 的 RDD，job 如果出错，下次运行时直接从 checkpoint 中读取数据。唯一不足的是，checkpoint 需要两次运行 job。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/9.-数据源/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/9.-数据源/" itemprop="url">9. 数据源</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-19T00:00:00+08:00">
                2023-03-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="一、核心数据源"><a href="#一、核心数据源" class="headerlink" title="一、核心数据源"></a>一、核心数据源</h1><ul>
<li>CSV</li>
<li>JSON</li>
<li>Parquet</li>
<li>ORC</li>
<li>JDBC</li>
<li><p>纯文本</p>
</li>
<li><p>Hbase</p>
</li>
<li>MongoDB</li>
<li>AWS Redshift</li>
<li>XML</li>
</ul>
<p>。。。</p>
<h1 id="二、Read-API结构"><a href="#二、Read-API结构" class="headerlink" title="二、Read API结构"></a>二、Read API结构</h1><p>读取数据的核心结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrameReader.format(...).option(<span class="string">"key"</span>, <span class="string">"value"</span>).schema(...).load()</span><br></pre></td></tr></table></figure>
<p>使用以上格式来读取所有数据源，format是可选的，默认是parquet格式。</p>
<p>Spark数据读取使用DataFrameReader，通过SparkSession的read属性得到：</p>
<p>​    Spark.read</p>
<p>需指定：</p>
<ul>
<li>format</li>
<li>schema</li>
<li>read模式</li>
<li>一系列option选项</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)</span><br><span class="line">.option(<span class="string">"path"</span>, <span class="string">"path/to/file(s)"</span>)</span><br><span class="line">.schema(someSchema)</span><br><span class="line">.load()</span><br></pre></td></tr></table></figure>
<h2 id="1-读取模式mode"><a href="#1-读取模式mode" class="headerlink" title="(1) 读取模式mode"></a>(1) 读取模式mode</h2><p>从外部源读取数据很容易会遇到错误格式的数据，指定读取模式可以当Spark遇到错误格式的记录时应采取什么操作。</p>
<ul>
<li>permissive：当遇到错误格式的记录时，将所有字段设置为null并将所有错误格式的记录放在名为_corrupt_record字符串列中</li>
<li>dropMalformed：删除包含错误格式记录的行</li>
<li>failFast：遇到错误格式的记录后立即返回失败</li>
</ul>
<p>默认是permissive.</p>
<h1 id="三、Write-API结构"><a href="#三、Write-API结构" class="headerlink" title="三、Write API结构"></a>三、Write API结构</h1><p>写数据的核心结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()</span><br></pre></td></tr></table></figure>
<h2 id="1-保存模式mode"><a href="#1-保存模式mode" class="headerlink" title="(1) 保存模式mode"></a>(1) 保存模式mode</h2><p>.option(“mode”, “OVERWRITE”)</p>
<ul>
<li>append：将输出内容追加到目标文件中</li>
<li>overwrite：将输出内容重写到目标文件中</li>
<li>errorIfExists：如果目标路径已存在数据或文件，则抛出错误并返回写入操作失败</li>
<li>ignore：如果目标路径已存在数据或文件，则不执行任何操作</li>
</ul>
<p>默认值为errorIfExists.</p>
<h1 id="四、CSV-json-parquet等文件"><a href="#四、CSV-json-parquet等文件" class="headerlink" title="四、CSV/json/parquet等文件"></a>四、CSV/json/parquet等文件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>) <span class="comment"># json/ parquet/orc/</span></span><br><span class="line">.option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">.load(<span class="string">"file.csv"</span>)</span><br><span class="line"></span><br><span class="line">df.write.format(<span class="string">"csv"</span>).mode(<span class="string">"overwrite"</span>).option(<span class="string">"sep"</span>, <span class="string">"\t"</span>).save(<span class="string">"file.csv"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="五、文本文件"><a href="#五、文本文件" class="headerlink" title="五、文本文件"></a>五、文本文件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读文件</span></span><br><span class="line">spark.read.textFile(<span class="string">"file.csv"</span>).selectExpr(<span class="string">"split(value, ',') as rows"</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 写文件</span></span><br><span class="line">df.limit(<span class="number">10</span>).select(<span class="string">"a"</span>, <span class="string">"b"</span>).write..partitionBy(<span class="string">'b'</span>).text(<span class="string">'file.csv'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="六、高级I-O概念"><a href="#六、高级I-O概念" class="headerlink" title="六、高级I/O概念"></a>六、高级I/O概念</h1><ul>
<li><p>可分割的文件类型和压缩(gzip压缩格式的Parquet等)</p>
</li>
<li><p>并行读写数据（分区概念)</p>
<ul>
<li><p>repartitions：减少分区数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.repartitions(<span class="number">5</span>).write.format(<span class="string">"csv"</span>).save(<span class="string">"file.csv"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>partitionBy：按什么字段来分区</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.mode(<span class="string">"overwrite"</span>).partitionBy(<span class="string">"a"</span>).save(<span class="string">"file.csv"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>数据分桶：bucketBy</p>
</li>
</ul>
<p>具有相同桶ID（哈希分桶的ID）的数据将放置到一个物理分区中，这样就可以避免在稍后读取数据时进行shuffle。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataFrameWriter.bucketBy(numBuckets, col, *cols)</span><br><span class="line"><span class="comment"># 按给定列存储输出。如果指定，则输出布局在文件系统上，类似于 Hive 的分桶方案，但具有不同的桶哈希函数，并且与 Hive 的分桶不兼容。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>(df.write.format(<span class="string">'parquet'</span>)  </span><br><span class="line">     .bucketBy(<span class="number">100</span>, <span class="string">'year'</span>, <span class="string">'month'</span>)</span><br><span class="line">     .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">     .saveAsTable(<span class="string">'bucketed_table'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>管理文件</li>
</ul>
<p>！！！写数据对文件大小不那么重要，但读取很重要！管理大量小文件产生很大的元数据开销，Spark特别不适合处理小文件。</p>
<p>​    maxRecordsPerFile选项来指定每个文件的最大记录数，这使得你可以通过控制写入每个文件的记录数来控制文件大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.option(<span class="string">'maxRecordsPerFile'</span>, <span class="number">5000</span>) <span class="comment"># Spark确保每个文件最多包含5000条记录</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/7.Spark处理不同的数据类型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/7.Spark处理不同的数据类型/" itemprop="url">7. 处理不同的数据类型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-18T00:00:00+08:00">
                2023-03-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p>[TOC]</p>
<h1 id="处理不同的数据类型"><a href="#处理不同的数据类型" class="headerlink" title="处理不同的数据类型"></a>处理不同的数据类型</h1><h2 id="1-数据类型"><a href="#1-数据类型" class="headerlink" title="1. 数据类型"></a>1. 数据类型</h2><ul>
<li>布尔型</li>
<li>数值型</li>
<li>字符串型</li>
<li>日期和时间戳类型</li>
<li>空值处理</li>
<li>复杂类型</li>
<li>自定义函数</li>
</ul>
<h2 id="2-处理布尔类型"><a href="#2-处理布尔类型" class="headerlink" title="2. 处理布尔类型"></a>2. 处理布尔类型</h2><p>（1）and、or、true 和false</p>
<p>（2）&amp;、|、1、0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.functions <span class="keyword">as</span> F</span><br><span class="line">filter1 = F.col(<span class="string">'aa'</span>) &gt; <span class="number">600</span></span><br><span class="line">filter2 = F.col(<span class="string">'bb'</span>) &gt; <span class="number">2</span></span><br><span class="line">df.where(filter1 &amp; filter2).where(F.col(<span class="string">'cc'</span>).isin(<span class="string">"DOT"</span>)).show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：创建布尔表达式时注意空值处理！！！</p>
<p>将df.where(F.col(‘a’) != ‘hello’)改写成df.where (F.col(‘a’).eqNullSafe(“hello”))可以保证空值安全。</p>
</blockquote>
<h2 id="3-处理数值类型"><a href="#3-处理数值类型" class="headerlink" title="3. 处理数值类型"></a>3. 处理数值类型</h2><ul>
<li>pow：平方</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例子</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pow</span><br><span class="line">f = pow(F.col(<span class="string">'a'</span>) * F.col(<span class="string">'b'</span>), <span class="number">2</span>) + <span class="number">5</span>  <span class="comment"># (a * b)^2 + 5</span></span><br><span class="line">df.select(f.alias(<span class="string">'c'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># selectExpr</span></span><br><span class="line">df.selectExpr(<span class="string">'a'</span>, <span class="string">'pow(F.col('</span>a<span class="string">') * F.col('</span><span class="string">b'), 2) + 5'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>round：向上取整，bround：向下取整</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from pyspark.sql.functions import lit, round, bround</span></span><br><span class="line">df.select(round(lit(<span class="string">"2.5"</span>)), bround(lit(<span class="string">"2.5"</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li>corr：计算两列的相关性</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> corr</span><br><span class="line">df.stat.corr(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line">df.select(corr(<span class="string">'a'</span>, <span class="string">'b'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>describe：计算一列或一组列的汇总统计信息，可以用describe来实现。</li>
<li>count：计数</li>
<li>mean：平均值</li>
<li>stddev_pop：标准差</li>
<li>min：最小值</li>
<li><p>max：最大值</p>
</li>
<li><p>StatFunctions包中封装了许多可使用的统计函数</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">quantileProbs = [<span class="number">0.5</span>]</span><br><span class="line">relError = <span class="number">0.05</span></span><br><span class="line">df.stat.approxQuantile(<span class="string">'a'</span>, quantileProbs, relError)</span><br></pre></td></tr></table></figure>
<ul>
<li>monotonically_increasing_id函数：从0开始，为每行添加一个唯一的ID。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> monotonically_increasing_id</span><br><span class="line">df.select(monotonically_increasing_id())</span><br></pre></td></tr></table></figure>
<h2 id="4-处理字符串类型"><a href="#4-处理字符串类型" class="headerlink" title="4. 处理字符串类型"></a>4. 处理字符串类型</h2><ul>
<li>initcap：将给定字符串中空格分隔的每个单词首字母大写。</li>
<li>lower：全部小写</li>
<li>upper：全部大写</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> initcap</span><br><span class="line">df.select(initcap(F.col(<span class="string">'aaa'</span>))， lower(F.col(<span class="string">'bbb'</span>)), upper(F.col(<span class="string">'ccc'</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li>lpad：从左边对字符串使用指定的字符进行填充。</li>
<li>ltrim：从左边对字符串使用指定的字符进行删除空格。</li>
<li>rpad: 从右边对字符串使用指定的字符进行填充。</li>
<li>trim：从右边对字符串使用指定的字符进行删除空格。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, ltrim, rtim, rpad, lpad, trim</span><br><span class="line">df.select(ltrim(lit(<span class="string">"    HELLO    "</span>)).alias(<span class="string">"ltrim"</span>),  </span><br><span class="line">		  rtrim(lit(<span class="string">"    HELLO    "</span>)).alias(<span class="string">"rtrim"</span>),</span><br><span class="line">		  trim(lit(<span class="string">"    HELLO    "</span>)).alias(<span class="string">"trim"</span>),</span><br><span class="line">		  lpad(lit(<span class="string">"HELLO"</span>, <span class="number">3</span>, <span class="string">" "</span>)).alias(<span class="string">"lpad"</span>),</span><br><span class="line">		  rpad(lit(<span class="string">"HELLO"</span>, <span class="number">10</span>, <span class="string">" "</span>)).alias(<span class="string">"rpad"</span>)</span><br><span class="line">)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">ltrim: "HELLO     "</span></span><br><span class="line"><span class="string">rtrim："     HELLO"</span></span><br><span class="line"><span class="string">trim:"HELLO"</span></span><br><span class="line"><span class="string">lpad："HEL"</span></span><br><span class="line"><span class="string">rpad: "HELLO     "</span></span><br><span class="line"><span class="string">注意 lpad或rpad输入的数值小于字符串长度，它将从字符串的右侧删除字符</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h2 id="5、正则表达式"><a href="#5、正则表达式" class="headerlink" title="5、正则表达式"></a>5、正则表达式</h2><ul>
<li>regexp_extract（列名，正则表达式，第几个）：提取值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_extract</span><br><span class="line">regex_string = <span class="string">"BLACK|WHITE|RED|GREEN|BLUE"</span></span><br><span class="line">df.select(regexp_extract(F.col(<span class="string">'a'</span>), regex_string, <span class="number">1</span>)) <span class="comment"># 将列名为a的字段中出现包含在正则表达式的第一个单词取出来。</span></span><br><span class="line"><span class="comment"># 如 df['a'] = "WHITE HANGING HEA", 结果为WHITE</span></span><br></pre></td></tr></table></figure>
<ul>
<li>regexp_replace：替换值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_replace</span><br><span class="line">regex_string = <span class="string">"BLACK|WHITE|RED|GREEN|BLUE"</span>  <span class="comment"># |在正则中表示或的意思</span></span><br><span class="line">df.select(</span><br><span class="line">	regexp_replace(F.col(<span class="string">'a'</span>), regex_string, <span class="string">'color'</span>).alias(<span class="string">'color_clean'</span>),</span><br><span class="line">) <span class="comment"># 将字段a中包含regex_string这些字段换成color。</span></span><br></pre></td></tr></table></figure>
<ul>
<li>translate：替换，将给定字符串替换掉所有出现的某字符串。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> translate</span><br><span class="line">df.select(translate(F.col(<span class="string">'a'</span>), <span class="string">'LEET'</span>, <span class="string">'1337'</span>)) <span class="comment"># L替换成1， E替换成3， T替换成7.</span></span><br><span class="line"><span class="comment"># 所以WHITE 会被替换成WHI73.</span></span><br></pre></td></tr></table></figure>
<ul>
<li>instr：是否存在（类似contains)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> instr</span><br><span class="line">containsBlack = instr(F.col(<span class="string">'a'</span>), <span class="string">'BLACK'</span>) &gt;= <span class="number">1</span></span><br><span class="line">df.withColumn(<span class="string">'b'</span>, containsBlack)</span><br></pre></td></tr></table></figure>
<ul>
<li>locate(substr, str, pos=1)：在位置 pos 之后定位字符串列中第一次出现 substr 的位置。<ul>
<li>该位置不是基于零的，而是基于 1 的索引。如果在 str 中找不到 substr，则返回 0。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame([(<span class="string">'abcd'</span>,)], [<span class="string">'s'</span>,])</span><br><span class="line">df.select(locate(<span class="string">'b'</span>, df.s, <span class="number">1</span>).alias(<span class="string">'s'</span>)).collect()</span><br><span class="line">[Row(s=<span class="number">2</span>)]</span><br></pre></td></tr></table></figure>
<h2 id="6、处理日期和时间戳类型"><a href="#6、处理日期和时间戳类型" class="headerlink" title="6、处理日期和时间戳类型"></a>6、处理日期和时间戳类型</h2><p><a href="https://zhuanlan.zhihu.com/p/450636026" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/450636026</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<h3 id="（1）-示例数据"><a href="#（1）-示例数据" class="headerlink" title="（1） 示例数据"></a>（1） 示例数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data=[[<span class="string">"1"</span>,<span class="string">"2020-02-01"</span>],[<span class="string">"2"</span>,<span class="string">"2019-03-01"</span>],[<span class="string">"3"</span>,<span class="string">"2021-03-01"</span>]]</span><br><span class="line">df=spark.createDataFrame(data, [<span class="string">"id"</span>,<span class="string">"time"</span>])</span><br><span class="line">df.show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+---+----------+</span><br><span class="line">| id|      time|</span><br><span class="line">+---+----------+</span><br><span class="line">|  <span class="number">1</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">|  <span class="number">3</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">+---+----------+</span><br></pre></td></tr></table></figure>
<h3 id="（2）-日期"><a href="#（2）-日期" class="headerlink" title="（2）. 日期"></a>（2）. 日期</h3><h4 id="2-1-当前日期-current-date"><a href="#2-1-当前日期-current-date" class="headerlink" title="2.1 当前日期 current_date()"></a>2.1 当前日期 <code>current_date()</code></h4><ul>
<li>获取当前系统日期。默认情况下，数据将以<code>yyyy-dd-mm</code>格式返回。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.current_date().alias(<span class="string">"current_date"</span>)).show(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+------------+</span><br><span class="line">|current_date|</span><br><span class="line">+------------+</span><br><span class="line">|  <span class="number">2021</span><span class="number">-12</span><span class="number">-28</span>|</span><br><span class="line">+------------+</span><br><span class="line">only showing top <span class="number">1</span> row</span><br></pre></td></tr></table></figure>
<h4 id="2-2-日期格式-date-format"><a href="#2-2-日期格式-date-format" class="headerlink" title="2.2 日期格式 date_format()"></a>2.2 日期格式 <code>date_format()</code></h4><ul>
<li>解析日期并转换<code>yyyy-dd-mm</code>为<code>MM-dd-yyyy</code>格式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.date_format(F.col(<span class="string">"time"</span>), <span class="string">"MM-dd-yyyy"</span>).alias(<span class="string">"date_format"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-----------+</span><br><span class="line">|      time|date_format|</span><br><span class="line">+----------+-----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>| <span class="number">02</span><span class="number">-01</span><span class="number">-2020</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">03</span><span class="number">-01</span><span class="number">-2019</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">03</span><span class="number">-01</span><span class="number">-2021</span>|</span><br><span class="line">+----------+-----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-3-使用to-date-将日期格式字符串yyyy-MM-dd转换为DateType-yyyy-MM-dd"><a href="#2-3-使用to-date-将日期格式字符串yyyy-MM-dd转换为DateType-yyyy-MM-dd" class="headerlink" title="2.3 使用to_date()将日期格式字符串yyyy-MM-dd转换为DateType yyyy-MM-dd"></a>2.3 使用<code>to_date()</code>将日期格式字符串<code>yyyy-MM-dd</code>转换为<code>DateType yyyy-MM-dd</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.to_date(F.col(<span class="string">"time"</span>), <span class="string">"yyy-MM-dd"</span>).alias(<span class="string">"to_date"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----------+</span><br><span class="line">|      time|   to_date|</span><br><span class="line">+----------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">+----------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-4-两个日期之间的日差datediff"><a href="#2-4-两个日期之间的日差datediff" class="headerlink" title="2.4 两个日期之间的日差datediff()"></a>2.4 两个日期之间的日差<code>datediff()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.datediff(F.current_date(), F.col(<span class="string">"time"</span>)).alias(<span class="string">"datediff"</span>)  </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+--------+</span><br><span class="line">|      time|datediff|</span><br><span class="line">+----------+--------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|     <span class="number">696</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|    <span class="number">1033</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|     <span class="number">302</span>|</span><br><span class="line">+----------+--------+</span><br></pre></td></tr></table></figure>
<h4 id="2-5-两个日期之间的月份months-between"><a href="#2-5-两个日期之间的月份months-between" class="headerlink" title="2.5 两个日期之间的月份months_between()"></a>2.5 两个日期之间的月份<code>months_between()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.months_between(F.current_date(),F.col(<span class="string">"time"</span>)).alias(<span class="string">"months_between"</span>)  </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+--------------+</span><br><span class="line">|      time|months_between|</span><br><span class="line">+----------+--------------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|   <span class="number">22.87096774</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|   <span class="number">33.87096774</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|    <span class="number">9.87096774</span>|</span><br><span class="line">+----------+--------------+</span><br></pre></td></tr></table></figure>
<h4 id="2-6-截断指定单位的日期trunc"><a href="#2-6-截断指定单位的日期trunc" class="headerlink" title="2.6 截断指定单位的日期trunc()"></a>2.6 截断指定单位的日期<code>trunc()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.trunc(F.col(<span class="string">"time"</span>),<span class="string">"Month"</span>).alias(<span class="string">"Month_Trunc"</span>), </span><br><span class="line">    F.trunc(F.col(<span class="string">"time"</span>),<span class="string">"Year"</span>).alias(<span class="string">"Month_Year"</span>), </span><br><span class="line">    F.trunc(F.col(<span class="string">"time"</span>),<span class="string">"Month"</span>).alias(<span class="string">"Month_Trunc"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+-----------+----------+-----------+</span><br><span class="line">|      time|Month_Trunc|Month_Year|Month_Trunc|</span><br><span class="line">+----------+-----------+----------+-----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>| <span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-01</span><span class="number">-01</span>| <span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-01</span><span class="number">-01</span>| <span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>| <span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-01</span><span class="number">-01</span>| <span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|</span><br><span class="line">+----------+-----------+----------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># pyspark.sql.functions.date_trunc(format, timestamp)</span></span><br><span class="line"><span class="comment"># 返回截断为格式指定单位的时间戳。</span></span><br><span class="line"><span class="comment"># 2.3.0 版中的新函数。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = spark.createDataFrame([(<span class="string">'1997-02-28 05:02:11'</span>,)], [<span class="string">'t'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(date_trunc(<span class="string">'year'</span>, df.t).alias(<span class="string">'year'</span>)).collect()</span><br><span class="line">[Row(year=datetime.datetime(<span class="number">1997</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(date_trunc(<span class="string">'mon'</span>, df.t).alias(<span class="string">'month'</span>)).collect()</span><br><span class="line">[Row(month=datetime.datetime(<span class="number">1997</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))]</span><br></pre></td></tr></table></figure>
<h4 id="2-7-月、日加减法"><a href="#2-7-月、日加减法" class="headerlink" title="2.7 月、日加减法"></a>2.7 月、日加减法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.add_months(F.col(<span class="string">"time"</span>),<span class="number">3</span>).alias(<span class="string">"add_months"</span>), </span><br><span class="line">    F.add_months(F.col(<span class="string">"time"</span>),<span class="number">-3</span>).alias(<span class="string">"sub_months"</span>), </span><br><span class="line">    F.date_add(F.col(<span class="string">"time"</span>),<span class="number">4</span>).alias(<span class="string">"date_add"</span>), </span><br><span class="line">    F.date_sub(F.col(<span class="string">"time"</span>),<span class="number">4</span>).alias(<span class="string">"date_sub"</span>) </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----------+----------+----------+----------+</span><br><span class="line">|      time|add_months|sub_months|  date_add|  date_sub|</span><br><span class="line">+----------+----------+----------+----------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-05</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-11</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-05</span>|<span class="number">2020</span><span class="number">-01</span><span class="number">-28</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-06</span><span class="number">-01</span>|<span class="number">2018</span><span class="number">-12</span><span class="number">-01</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-05</span>|<span class="number">2019</span><span class="number">-02</span><span class="number">-25</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-06</span><span class="number">-01</span>|<span class="number">2020</span><span class="number">-12</span><span class="number">-01</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-05</span>|<span class="number">2021</span><span class="number">-02</span><span class="number">-25</span>|</span><br><span class="line">+----------+----------+----------+----------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-8-年、月、下一天、一年中第几个星期"><a href="#2-8-年、月、下一天、一年中第几个星期" class="headerlink" title="2.8 年、月、下一天、一年中第几个星期"></a>2.8 年、月、下一天、一年中第几个星期</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">     F.year(F.col(<span class="string">"time"</span>)).alias(<span class="string">"year"</span>), </span><br><span class="line">     F.month(F.col(<span class="string">"time"</span>)).alias(<span class="string">"month"</span>), </span><br><span class="line">     F.next_day(F.col(<span class="string">"time"</span>),<span class="string">"Sunday"</span>).alias(<span class="string">"next_day"</span>), </span><br><span class="line">     F.weekofyear(F.col(<span class="string">"time"</span>)).alias(<span class="string">"weekofyear"</span>) </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+----+-----+----------+----------+</span><br><span class="line">|      time|year|month|  next_day|weekofyear|</span><br><span class="line">+----------+----+-----+----------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|<span class="number">2020</span>|    <span class="number">2</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>|         <span class="number">5</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2019</span>|    <span class="number">3</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-03</span>|         <span class="number">9</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|<span class="number">2021</span>|    <span class="number">3</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-07</span>|         <span class="number">9</span>|</span><br><span class="line">+----------+----+-----+----------+----------+</span><br></pre></td></tr></table></figure>
<h4 id="2-9-星期几、月日、年日"><a href="#2-9-星期几、月日、年日" class="headerlink" title="2.9 星期几、月日、年日"></a>2.9 星期几、月日、年日</h4><ul>
<li>查询星期几</li>
<li>一个月中的第几天</li>
<li>一年中的第几天</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df.select(F.col(<span class="string">"time"</span>),  </span><br><span class="line">     F.dayofweek(F.col(<span class="string">"time"</span>)).alias(<span class="string">"dayofweek"</span>), </span><br><span class="line">     F.dayofmonth(F.col(<span class="string">"time"</span>)).alias(<span class="string">"dayofmonth"</span>), </span><br><span class="line">     F.dayofyear(F.col(<span class="string">"time"</span>)).alias(<span class="string">"dayofyear"</span>), </span><br><span class="line">).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+----------+---------+----------+---------+</span><br><span class="line">|      time|dayofweek|dayofmonth|dayofyear|</span><br><span class="line">+----------+---------+----------+---------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>|        <span class="number">7</span>|         <span class="number">1</span>|       <span class="number">32</span>|</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span>|        <span class="number">6</span>|         <span class="number">1</span>|       <span class="number">60</span>|</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span>|        <span class="number">2</span>|         <span class="number">1</span>|       <span class="number">60</span>|</span><br><span class="line">+----------+---------+----------+---------+</span><br></pre></td></tr></table></figure>
<h3 id="（3）-时间"><a href="#（3）-时间" class="headerlink" title="（3）. 时间"></a>（3）. 时间</h3><h4 id="3-1-创建一个测试数据"><a href="#3-1-创建一个测试数据" class="headerlink" title="3.1 创建一个测试数据"></a>3.1 创建一个测试数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data=[</span><br><span class="line">    [<span class="string">"1"</span>,<span class="string">"02-01-2020 11 01 19 06"</span>],</span><br><span class="line">    [<span class="string">"2"</span>,<span class="string">"03-01-2019 12 01 19 406"</span>],</span><br><span class="line">    [<span class="string">"3"</span>,<span class="string">"03-01-2021 12 01 19 406"</span>]]</span><br><span class="line">df2=spark.createDataFrame(data,[<span class="string">"id"</span>,<span class="string">"time"</span>])</span><br><span class="line">df2.show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+---+-----------------------+</span><br><span class="line">|id |time                   |</span><br><span class="line">+---+-----------------------+</span><br><span class="line">|<span class="number">1</span>  |<span class="number">02</span><span class="number">-01</span><span class="number">-2020</span> <span class="number">11</span> <span class="number">01</span> <span class="number">19</span> <span class="number">06</span> |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">03</span><span class="number">-01</span><span class="number">-2019</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|</span><br><span class="line">|<span class="number">3</span>  |<span class="number">03</span><span class="number">-01</span><span class="number">-2021</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|</span><br><span class="line">+---+-----------------------+</span><br></pre></td></tr></table></figure>
<h4 id="3-2-以-spark-默认格式yyyy-MM-dd-HH-mm-ss返回当前时间戳"><a href="#3-2-以-spark-默认格式yyyy-MM-dd-HH-mm-ss返回当前时间戳" class="headerlink" title="3.2 以 spark 默认格式yyyy-MM-dd HH:mm:ss返回当前时间戳"></a>3.2 以 spark 默认格式<code>yyyy-MM-dd HH:mm:ss</code>返回当前时间戳</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">df2.select(F.current_timestamp().alias(<span class="string">"current_timestamp"</span>)).show()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+--------------------+</span><br><span class="line">|   current_timestamp|</span><br><span class="line">+--------------------+</span><br><span class="line">|<span class="number">2021</span><span class="number">-12</span><span class="number">-28</span> <span class="number">09</span>:<span class="number">31</span>:...|</span><br><span class="line">|<span class="number">2021</span><span class="number">-12</span><span class="number">-28</span> <span class="number">09</span>:<span class="number">31</span>:...|</span><br><span class="line">|<span class="number">2021</span><span class="number">-12</span><span class="number">-28</span> <span class="number">09</span>:<span class="number">31</span>:...|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
<h4 id="3-3-将字符串时间戳转换为时间戳类型格式-to-timestamp"><a href="#3-3-将字符串时间戳转换为时间戳类型格式-to-timestamp" class="headerlink" title="3.3 将字符串时间戳转换为时间戳类型格式 to_timestamp()"></a>3.3 将字符串时间戳转换为时间戳类型格式 <code>to_timestamp()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df2.select(F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.to_timestamp(F.col(<span class="string">"time"</span>), <span class="string">"MM-dd-yyyy HH mm ss SSS"</span>).alias(<span class="string">"to_timestamp"</span>) </span><br><span class="line">    ).show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-----------------------+-----------------------+</span><br><span class="line">|time                   |to_timestamp           |</span><br><span class="line">+-----------------------+-----------------------+</span><br><span class="line">|<span class="number">02</span><span class="number">-01</span><span class="number">-2020</span> <span class="number">11</span> <span class="number">01</span> <span class="number">19</span> <span class="number">06</span> |<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span> <span class="number">11</span>:<span class="number">01</span>:<span class="number">19.06</span> |</span><br><span class="line">|<span class="number">03</span><span class="number">-01</span><span class="number">-2019</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|</span><br><span class="line">|<span class="number">03</span><span class="number">-01</span><span class="number">-2021</span> <span class="number">12</span> <span class="number">01</span> <span class="number">19</span> <span class="number">406</span>|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|</span><br><span class="line">+-----------------------+-----------------------+</span><br></pre></td></tr></table></figure>
<h4 id="3-4-获取小时、分钟、秒"><a href="#3-4-获取小时、分钟、秒" class="headerlink" title="3.4 获取小时、分钟、秒"></a>3.4 获取<code>小时</code>、<code>分钟</code>、<code>秒</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">data=[</span><br><span class="line">    [<span class="string">"1"</span>,<span class="string">"2020-02-01 11:01:19.06"</span>],</span><br><span class="line">    [<span class="string">"2"</span>,<span class="string">"2019-03-01 12:01:19.406"</span>],</span><br><span class="line">    [<span class="string">"3"</span>,<span class="string">"2021-03-01 12:01:19.406"</span>]]</span><br><span class="line">df3=spark.createDataFrame(data,[<span class="string">"id"</span>,<span class="string">"time"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取小时、分钟、秒</span></span><br><span class="line">df3.select(</span><br><span class="line">    F.col(<span class="string">"time"</span>), </span><br><span class="line">    F.hour(F.col(<span class="string">"time"</span>)).alias(<span class="string">"hour"</span>), </span><br><span class="line">    F.minute(F.col(<span class="string">"time"</span>)).alias(<span class="string">"minute"</span>),</span><br><span class="line">    F.second(F.col(<span class="string">"time"</span>)).alias(<span class="string">"second"</span>) </span><br><span class="line">    ).show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-----------------------+----+------+------+</span><br><span class="line">|time                   |hour|minute|second|</span><br><span class="line">+-----------------------+----+------+------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span> <span class="number">11</span>:<span class="number">01</span>:<span class="number">19.06</span> |<span class="number">11</span>  |<span class="number">1</span>     |<span class="number">19</span>    |</span><br><span class="line">|<span class="number">2019</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|<span class="number">12</span>  |<span class="number">1</span>     |<span class="number">19</span>    |</span><br><span class="line">|<span class="number">2021</span><span class="number">-03</span><span class="number">-01</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">19.406</span>|<span class="number">12</span>  |<span class="number">1</span>     |<span class="number">19</span>    |</span><br><span class="line">+-----------------------+----+------+------+</span><br></pre></td></tr></table></figure>
<h2 id="7-处理数据中的空值"><a href="#7-处理数据中的空值" class="headerlink" title="7. 处理数据中的空值"></a>7. 处理数据中的空值</h2><h2 id="8-复杂类型"><a href="#8-复杂类型" class="headerlink" title="8. 复杂类型"></a>8. 复杂类型</h2><ul>
<li>结构体: struct</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> struct</span><br><span class="line">df1 = df.select(struct(<span class="string">'a'</span>, <span class="string">'b'</span>).alias(<span class="string">'c'</span>))</span><br><span class="line"><span class="comment"># 可以通过"."来访问或列方法getField来实现：</span></span><br><span class="line">df1.select(<span class="string">"c.a"</span>)</span><br><span class="line">df1.select(F.col(<span class="string">"c"</span>).getField(<span class="string">"a"</span>))</span><br><span class="line"><span class="comment"># 可以通过 ".*"来查询结构体中所有值</span></span><br><span class="line">df1.select(<span class="string">"c.*"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>数组<ul>
<li>split：指定分隔符</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split</span><br><span class="line">df.select(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>)) </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">列名：split(a,)</span></span><br><span class="line"><span class="string">结果：[WHITE, HANGING, ...]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df.select(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>).alias(<span class="string">"array_col"</span>)) </span><br><span class="line">.selectExpr(<span class="string">"array_col[0]"</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">结果</span></span><br><span class="line"><span class="string">列名：array_col[0]</span></span><br><span class="line"><span class="string">结果：WHITE</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>​    数组长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> size</span><br><span class="line">df.select(size(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>array_contains：数组是否包含某个值
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> array_contains</span><br><span class="line">df.select(array_contains(split(F.col(<span class="string">"a"</span>), <span class="string">"\t"</span>), <span class="string">"WHITE"</span>))</span><br><span class="line"><span class="comment"># 结果为true</span></span><br></pre></td></tr></table></figure>
<p>​    explode：一行拆分成多行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">"sub_str"</span>, explode(split(df[<span class="string">"str_col"</span>], <span class="string">"_"</span>))) </span><br><span class="line"><span class="comment"># 将str_col按-拆分成list，list中的每一个元素成为sub_str,与原行中的其他列一起组成新的行</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">eg:</span></span><br><span class="line"><span class="string">"hello world","other column"</span></span><br><span class="line"><span class="string">split ===&gt; ["hello", "world"], "other column"</span></span><br><span class="line"><span class="string">explode ===&gt; </span></span><br><span class="line"><span class="string">		"hello", "other column"; </span></span><br><span class="line"><span class="string">		"world", "other column"</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>将嵌套数组 <code>DataFrame</code> 列分解为行</p>
<p>创建一个带有嵌套数组列的 <code>DataFrame</code>。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">arrayArrayData = [</span><br><span class="line">  (<span class="string">"James"</span>,[[<span class="string">"Java"</span>,<span class="string">"Scala"</span>,<span class="string">"C++"</span>],[<span class="string">"Spark"</span>,<span class="string">"Java"</span>]]),</span><br><span class="line">  (<span class="string">"Michael"</span>,[[<span class="string">"Spark"</span>,<span class="string">"Java"</span>,<span class="string">"C++"</span>],[<span class="string">"Spark"</span>,<span class="string">"Java"</span>]]),</span><br><span class="line">  (<span class="string">"Robert"</span>,[[<span class="string">"CSharp"</span>,<span class="string">"VB"</span>],[<span class="string">"Spark"</span>,<span class="string">"Python"</span>]])</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(data=arrayArrayData, schema = [<span class="string">'name'</span>,<span class="string">'subjects'</span>])</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- subjects: array (nullable = true)</span><br><span class="line"> |    |-- element: array (containsNull = true)</span><br><span class="line"> |    |    |-- element: string (containsNull = true)</span><br><span class="line"></span><br><span class="line">+-------+-----------------------------------+</span><br><span class="line">|name   |subjects                           |</span><br><span class="line">+-------+-----------------------------------+</span><br><span class="line">|James  |[[Java, Scala, C++], [Spark, Java]]|</span><br><span class="line">|Michael|[[Spark, Java, C++], [Spark, Java]]|</span><br><span class="line">|Robert |[[CSharp, VB], [Spark, Python]]    |</span><br><span class="line">+-------+-----------------------------------+</span><br></pre></td></tr></table></figure>
<p>​    展平数组，请使用 <code>flatten</code> 函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> flatten</span><br><span class="line">df.select(df.name, flatten(df.subjects)).show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+-------------------------------+</span><br><span class="line">|name   |flatten(subjects)              |</span><br><span class="line">+-------+-------------------------------+</span><br><span class="line">|James  |[Java, Scala, C++, Spark, Java]|</span><br><span class="line">|Michael|[Spark, Java, C++, Spark, Java]|</span><br><span class="line">|Robert |[CSharp, VB, Spark, Python]    |</span><br><span class="line">+-------+-------------------------------+</span><br></pre></td></tr></table></figure>
<pre><code>展平再分解
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">df.select(df.name, explode(flatten(df.subjects))).show(truncate=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output Data:</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+-------+------+</span><br><span class="line">|name   |col   |</span><br><span class="line">+-------+------+</span><br><span class="line">|James  |Java  |</span><br><span class="line">|James  |Scala |</span><br><span class="line">|James  |C++   |</span><br><span class="line">|James  |Spark |</span><br><span class="line">|James  |Java  |</span><br><span class="line">|Michael|Spark |</span><br><span class="line">|Michael|Java  |</span><br><span class="line">|Michael|C++   |</span><br><span class="line">|Michael|Spark |</span><br><span class="line">|Michael|Java  |</span><br><span class="line">|Robert |CSharp|</span><br><span class="line">|Robert |VB    |</span><br><span class="line">|Robert |Spark |</span><br><span class="line">|Robert |Python|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Map</p>
<ul>
<li>create_map：键值对</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split</span><br><span class="line">df.select(create_map(F.col(<span class="string">"a"</span>), F.col(<span class="string">"b"</span>)).alias(<span class="string">"c_map"</span>))</span><br></pre></td></tr></table></figure>
<p>​    根据key值取value</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split</span><br><span class="line">df.select(create_map(F.col(<span class="string">"a"</span>), F.col(<span class="string">"b"</span>)).alias(<span class="string">"c_map"</span>))\</span><br><span class="line">.selectExpt(<span class="string">"c_map['WHILE METAL LANTERN']"</span>)</span><br></pre></td></tr></table></figure>
<p>​    展开map类型，将其转换成列:explode</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.select(create_map(F.col(<span class="string">"a"</span>), F.col(<span class="string">"b"</span>)).alias(<span class="string">"c_map"</span>))\</span><br><span class="line">.selectExpt(<span class="string">"explode('c_map')"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="9-处理Json类型"><a href="#9-处理Json类型" class="headerlink" title="9. 处理Json类型"></a>9. 处理Json类型</h2><h3 id="（1）创建一个Json类型的列："><a href="#（1）创建一个Json类型的列：" class="headerlink" title="（1）创建一个Json类型的列："></a>（1）创建一个Json类型的列：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jsonDF = spark.range(<span class="number">1</span>).selectExpr(<span class="string">"""</span></span><br><span class="line"><span class="string">	'&#123;</span></span><br><span class="line"><span class="string">        "a": &#123;</span></span><br><span class="line"><span class="string">            "aa": [1,2,3]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">	&#125;' as jsonString</span></span><br><span class="line"><span class="string">"""</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-get-json-object：查询JSON对象"><a href="#2-get-json-object：查询JSON对象" class="headerlink" title="(2) get_json_object：查询JSON对象"></a>(2) get_json_object：查询JSON对象</h3><p>pyspark.sql.functions.get_json_object(col, path) : 根据指定的 json 路径从 json 字符串中提取 json 对象，并返回提取的 json 对象的 json 字符串。如果输入的 json 字符串无效，它将返回 null。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [(<span class="string">"1"</span>, <span class="string">'''&#123;"f1": "value1", "f2": "value2"&#125;'''</span>), (<span class="string">"2"</span>, <span class="string">'''&#123;"f1": "value12"&#125;'''</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = spark.createDataFrame(data, (<span class="string">"key"</span>, <span class="string">"jstring"</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(df.key, get_json_object(df.jstring, <span class="string">'$.f1'</span>).alias(<span class="string">"c0"</span>), \</span><br><span class="line"><span class="meta">... </span>                  get_json_object(df.jstring, <span class="string">'$.f2'</span>).alias(<span class="string">"c1"</span>) ).collect()</span><br><span class="line">[Row(key=<span class="string">'1'</span>, c0=<span class="string">'value1'</span>, c1=<span class="string">'value2'</span>), Row(key=<span class="string">'2'</span>, c0=<span class="string">'value12'</span>, c1=<span class="literal">None</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="3-若此查询的JSON对象仅有一层嵌套，则可使用json-tuple"><a href="#3-若此查询的JSON对象仅有一层嵌套，则可使用json-tuple" class="headerlink" title="(3) 若此查询的JSON对象仅有一层嵌套，则可使用json_tuple"></a>(3) 若此查询的JSON对象仅有一层嵌套，则可使用json_tuple</h3><p>pyspark.sql.functions.json_tuple(col, *fields): 根据给定的字段名称为 json 列创建一个新行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [(<span class="string">"1"</span>, <span class="string">'''&#123;"f1": "value1", "f2": "value2"&#125;'''</span>), (<span class="string">"2"</span>, <span class="string">'''&#123;"f1": "value12"&#125;'''</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = spark.createDataFrame(data, (<span class="string">"key"</span>, <span class="string">"jstring"</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(df.key, json_tuple(df.jstring, <span class="string">'f1'</span>, <span class="string">'f2'</span>)).collect()</span><br><span class="line">[Row(key=<span class="string">'1'</span>, c0=<span class="string">'value1'</span>, c1=<span class="string">'value2'</span>), Row(key=<span class="string">'2'</span>, c0=<span class="string">'value12'</span>, c1=<span class="literal">None</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="4）to-json：将StructType转换成JSON字符串"><a href="#4）to-json：将StructType转换成JSON字符串" class="headerlink" title="(4）to_json：将StructType转换成JSON字符串"></a>(4）to_json：将StructType转换成JSON字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = ps.DataFrame([[<span class="string">'a'</span>, <span class="string">'b'</span>], [<span class="string">'c'</span>, <span class="string">'d'</span>]],</span><br><span class="line"><span class="meta">... </span>                  columns=[<span class="string">'col 1'</span>, <span class="string">'col 2'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.to_json()</span><br><span class="line"><span class="string">'[&#123;"col 1":"a","col 2":"b"&#125;,&#123;"col 1":"c","col 2":"d"&#125;]'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df[<span class="string">'col 1'</span>].to_json()</span><br><span class="line"><span class="string">'[&#123;"col 1":"a"&#125;,&#123;"col 1":"c"&#125;]'</span></span><br></pre></td></tr></table></figure>
<h3 id="5-from-json：解析JSON数据，需指定模式"><a href="#5-from-json：解析JSON数据，需指定模式" class="headerlink" title="(5) from_json：解析JSON数据，需指定模式"></a>(5) from_json：解析JSON数据，需指定模式</h3><p>  可以使用<code>pyspark.sql.functions.from_json</code>函数将DataFrame中的字典列拆分为多列</p>
<h2 id="10-UDF-自定义函数"><a href="#10-UDF-自定义函数" class="headerlink" title="10. UDF (自定义函数)"></a>10. UDF (自定义函数)</h2><p>UDF允许使用多种不同的变成语言编写，这些UDF函数被注册为SparkSession或者Context的临时函数。</p>
<p>Spark将在Driver进程上序列化UDF函数，并将它通过网络传递到所有的executor进程。</p>
<blockquote>
<p>如果用<strong>Scala或Java编写的，可以在JVM</strong>中使用它。除了不能使用Spark为内置函数提供的代码生成功能之外，会导致性能的下降。</p>
<p>如果函数是<strong>用Python编写的，Spark在worker上启动一个Python进程</strong>，<strong>将所有程序列化为Python可解释的格式</strong>（在此之前程序位于JVM中），在Python进程中对该程序逐行执行函数，最终将对每行的操作结果返回给JVM和Spark。</p>
</blockquote>
<p>将程序序列化为Python可解释的格式这个过程代价很高！！！！</p>
<ol>
<li>计算昂贵</li>
<li>程序进入Python后Spark无法管理worker内存。若某个worker因资源受限而失败（JVM和Python在同一台机器争夺内存），可能会导致该worker出现故障。———–建议用java/scala编写UDF。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power1</span><span class="params">(v)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> v**<span class="number">2</span></span><br><span class="line">powerudf = udf(power1)</span><br><span class="line">df.select(powerudf(F.col(<span class="string">'a'</span>)))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Spark的Join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark的Join/" itemprop="url">Join</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-16T00:00:00+08:00">
                2023-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="PySpark的Join"><a href="#PySpark的Join" class="headerlink" title="PySpark的Join"></a>PySpark的Join</h1><p><a href="https://zhuanlan.zhihu.com/p/344080090" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/344080090</a></p>
<h2 id="一、Join方式："><a href="#一、Join方式：" class="headerlink" title="一、Join方式："></a>一、Join方式：</h2><p>pyspark主要分为以下几种join方式：</p>
<ul>
<li><strong>Inner joins</strong> （内连接)： 两边都有的保持</li>
<li><strong>Outer joins</strong> (外连接)：两边任意一边有的保持</li>
<li><strong>Left outer joins</strong> (左外连接)：只保留左边有的records</li>
<li><strong>Right outer joins</strong> (右外连接)：只保留右边有的records</li>
<li><strong>Left semi joins</strong> (左半连接)：只保留在右边记录里出现的左边的records</li>
<li><strong>Left anti joins</strong> (左反连接)：只保留没出现在右边记录里的左边records（可以用来做过滤）</li>
<li><strong>natural join</strong>（自然连接）：通过隐式匹配两个数据集之间具有相同名称的列来执行连接）</li>
<li><strong>cross join</strong>（笛卡尔连接）：将左侧数据集的每一行与右侧数据集中的每一行匹配，结果行数很多。</li>
</ul>
<p>##二、使用方式和例子</p>
<p>下面造个数据集来看看这些join的例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">person = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Bill Chambers"</span>, <span class="number">0</span>, [<span class="number">100</span>]),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"Matei Zaharia"</span>, <span class="number">1</span>, [<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Michael Armbrust"</span>, <span class="number">1</span>, [<span class="number">250</span>, <span class="number">100</span>])])\</span><br><span class="line">  .toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"graduate_program"</span>, <span class="string">"spark_status"</span>)</span><br><span class="line">graduateProgram = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Masters"</span>, <span class="string">"School of Information"</span>, <span class="string">"UC Berkeley"</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Masters"</span>, <span class="string">"EECS"</span>, <span class="string">"UC Berkeley"</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"Ph.D."</span>, <span class="string">"EECS"</span>, <span class="string">"UC Berkeley"</span>)])\</span><br><span class="line">  .toDF(<span class="string">"id"</span>, <span class="string">"degree"</span>, <span class="string">"department"</span>, <span class="string">"school"</span>)</span><br><span class="line">sparkStatus = spark.createDataFrame([</span><br><span class="line">    (<span class="number">500</span>, <span class="string">"Vice President"</span>),</span><br><span class="line">    (<span class="number">250</span>, <span class="string">"PMC Member"</span>),</span><br><span class="line">    (<span class="number">100</span>, <span class="string">"Contributor"</span>)])\</span><br><span class="line">  .toDF(<span class="string">"id"</span>, <span class="string">"status"</span>)</span><br></pre></td></tr></table></figure>
<p>Inner Joins</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">joinExpression = person[<span class="string">"graduate_program"</span>] == graduateProgram[<span class="string">'id'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># in Python</span></span><br><span class="line">wrongJoinExpression = person[<span class="string">"name"</span>] == graduateProgram[<span class="string">"school"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># default so no need to specify</span></span><br><span class="line">person.join(graduateProgram, joinExpression).show()</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"></span><br><span class="line">+---+----------------+----------------+---------------+---+-------+----------+---</span><br><span class="line">| id|            name|graduate_program|   spark_status| id| degree|department|...</span><br><span class="line">+---+----------------+----------------+---------------+---+-------+----------+---</span><br><span class="line">|  <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|          [<span class="number">100</span>]|  <span class="number">0</span>|Masters| School...|...</span><br><span class="line">|  <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|[<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|      EECS|...</span><br><span class="line">|  <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|     [<span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|      EECS|...</span><br><span class="line">+---+----------------+----------------+---------------+---+-------+----------+---</span><br></pre></td></tr></table></figure>
<p><strong>Outer Joins</strong></p>
<blockquote>
<p>Outerjoins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. If there is no equivalent row in either the left or right DataFrame, Spark will insert<code>null</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"outer"</span></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+----+----------------+----------------+---------------+---+-------+-------------</span><br><span class="line">|  id|            name|graduate_program|   spark_status| id| degree| departmen...</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+-------------</span><br><span class="line">|   <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|[<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|       EEC...</span><br><span class="line">|   <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|     [<span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|       EEC...</span><br><span class="line">|null|            null|            null|           null|  <span class="number">2</span>|Masters|       EEC...</span><br><span class="line">|   <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|          [<span class="number">100</span>]|  <span class="number">0</span>|Masters|    School...</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+-------------</span><br></pre></td></tr></table></figure>
<p><strong>Left Outer Joins</strong></p>
<blockquote>
<p>Leftouter joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert<code>null</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"left_outer"</span></span><br><span class="line"></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">+---+-------+----------+-----------+----+----------------+----------------+---</span><br><span class="line">| id| degree|department|     school|  id|            name|graduate_program|...</span><br><span class="line">+---+-------+----------+-----------+----+----------------+----------------+---</span><br><span class="line">|  <span class="number">0</span>|Masters| School...|UC Berkeley|   <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|...</span><br><span class="line">|  <span class="number">2</span>|Masters|      EECS|UC Berkeley|null|            null|            null|...</span><br><span class="line">|  <span class="number">1</span>|  Ph.D.|      EECS|UC Berkeley|   <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|...</span><br><span class="line">|  <span class="number">1</span>|  Ph.D.|      EECS|UC Berkeley|   <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|...</span><br><span class="line">+---+-------+----------+-----------+----+----------------+----------------+---</span><br></pre></td></tr></table></figure>
<p><strong>Right Outer Joins</strong></p>
<blockquote>
<p>Rightouter joins evaluate the keys in both of the DataFrames or tables and includes all rows from the right DataFrame as well as any rows in the left DataFrame that have a match in the right DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert<code>null</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"right_outer"</span></span><br><span class="line"></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+----+----------------+----------------+---------------+---+-------+------------+</span><br><span class="line">|  id|            name|graduate_program|   spark_status| id| degree|  department|</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+------------+</span><br><span class="line">|   <span class="number">0</span>|   Bill Chambers|               <span class="number">0</span>|          [<span class="number">100</span>]|  <span class="number">0</span>|Masters|School of...|</span><br><span class="line">|null|            null|            null|           null|  <span class="number">2</span>|Masters|        EECS|</span><br><span class="line">|   <span class="number">2</span>|Michael Armbrust|               <span class="number">1</span>|     [<span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|        EECS|</span><br><span class="line">|   <span class="number">1</span>|   Matei Zaharia|               <span class="number">1</span>|[<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>]|  <span class="number">1</span>|  Ph.D.|        EECS|</span><br><span class="line">+----+----------------+----------------+---------------+---+-------+------------+</span><br></pre></td></tr></table></figure>
<p><strong>Left Semi Joins 用作数据筛选（include 方式）</strong></p>
<blockquote>
<p>Semijoins are a bit of a departure from the other joins. They do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame. Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"left_semi"</span></span><br><span class="line"></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+---+-------+--------------------+-----------+</span><br><span class="line">| id| degree|          department|     school|</span><br><span class="line">+---+-------+--------------------+-----------+</span><br><span class="line">|  <span class="number">0</span>|Masters|School of Informa...|UC Berkeley|</span><br><span class="line">|  <span class="number">1</span>|  Ph.D.|                EECS|UC Berkeley|</span><br><span class="line">+---+-------+--------------------+-----------+</span><br></pre></td></tr></table></figure>
<p><strong>Left Anti Joins 用作数据筛选（exclude的方式）</strong></p>
<blockquote>
<p>Leftanti joins are the opposite of left semi joins. Like left semi joins, they do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that<em>do not</em>have a corresponding key in the second DataFrame. Think of anti joins as a<code>NOT IN</code>SQL-style filter</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">joinType = <span class="string">"left_anti"</span></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"></span><br><span class="line">+---+-------+----------+-----------+</span><br><span class="line">| id| degree|department|     school|</span><br><span class="line">+---+-------+----------+-----------+</span><br><span class="line">|  <span class="number">2</span>|Masters|      EECS|UC Berkeley|</span><br><span class="line">+---+-------+----------+-----------+</span><br></pre></td></tr></table></figure>
<h2 id="三、常见问题和解决方案"><a href="#三、常见问题和解决方案" class="headerlink" title="三、常见问题和解决方案"></a>三、常见问题和解决方案</h2><h3 id="1-处理重复列名：两张表如果存在相同列名？？"><a href="#1-处理重复列名：两张表如果存在相同列名？？" class="headerlink" title="1. 处理重复列名：两张表如果存在相同列名？？"></a>1. 处理重复列名：两张表如果存在相同列名？？</h3><p>方法1：采用不同的连接表达式：</p>
<p>​    当有两个同名的键时，将连接表达式从布尔表达式更改为字符串或序列，这会在连接过程中自动删除其中一个列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.join(b, <span class="string">'id'</span>) <span class="comment"># 不写成a.join(b , a['id']==b['id'])</span></span><br></pre></td></tr></table></figure>
<p>方法2：连接后删除列，采用drop</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.join(b, a[<span class="string">'id'</span>]==b[<span class="string">'id'</span>]).drop(a.id)</span><br></pre></td></tr></table></figure>
<p>方法3：在连接前重命名列,采用withColumnRenamed</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = a.withColumnRenamed(<span class="string">'id1'</span>, F.col(<span class="string">'id'</span>))</span><br><span class="line">a.join(b, a.id1=b.id)</span><br></pre></td></tr></table></figure>
<h1 id="四、Spark如何执行连接"><a href="#四、Spark如何执行连接" class="headerlink" title="四、Spark如何执行连接"></a>四、Spark如何执行连接</h1><p><strong>两个核心模块</strong>：点对点通信模式和逐点计算模式</p>
<ul>
<li><p>大表和大表的连接</p>
<ul>
<li>shuffle join ：每个节点都与所有其他节点进行通信，并根据哪个节点具有某些键来共享数据。由于网络会因通信量而阻塞，所以这种方式很耗时，特殊是如果数据没有合理分区的情况下。</li>
</ul>
</li>
<li><p>大表与小表连接</p>
<ul>
<li><p>broadcast join：当表的大小足够小以便能够放入单个节点内存中且还有空闲空间的时候，可优化join。</p>
<p>把数据量较小的DataFrame复制到集群中的所有工作节点上，只需在开始时执行一次，然后让每个工作节点独立执行作业，而无需等待其他工作节点，也无需与其他工作节点通信。</p>
<p><img src="..\imgs\Spark_broadcast.jpg" alt></p>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/广播变量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/广播变量/" itemprop="url">广播变量</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-14T00:00:00+08:00">
                2023-03-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p>[TOC]</p>
<p>如果想在节点之间共享一份变量，spark提供了两种特定的共享变量，来完成节点之间的变量共享。</p>
<p>（1）广播变量（2）累加器</p>
<h1 id="一、广播变量"><a href="#一、广播变量" class="headerlink" title="一、广播变量"></a>一、广播变量</h1><h2 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h2><p>广播变量允许程序员缓存一个<strong>只读的变量</strong>在每台机器上(worker)，而不是每个任务(task)保存一个拷贝。例如，利用广播变量，我们能够以一种更有效率的方式将一个大数据量输入集合的副本分配给每个节点。</p>
<p>广播变量用于跨所有节点保存数据副本。 此变量缓存在所有计算机上，<strong>而不是在具有任务的计算机上发送。</strong></p>
<p>一个广播变量可以通过调用<strong>SparkContext.broadcast(v</strong>)方法从一个初始变量v中创建。广播变量是v的一个包装变量，它的值可以通过value方法访问。</p>
<p>用途：比如一个配置文件，可以共享给所有节点。比如一个Node的计算结果需要共享给其他节点。</p>
<ul>
<li>可以通过广播变量, 通知当前worker上所有的task, 来共享这个数据,避免数据的多次复制,可以大大降低内存的开销</li>
<li>sparkContext.broadcast(要共享的数据)</li>
</ul>
<h2 id="声明：broadcast"><a href="#声明：broadcast" class="headerlink" title="声明：broadcast"></a>声明：broadcast</h2><p>调用broadcast，Scala中一切可序列化的对象都可以进行广播。</p>
<p>sc.broadcast(xxx)</p>
<h2 id="引用广播变量数据：value"><a href="#引用广播变量数据：value" class="headerlink" title="引用广播变量数据：value"></a>引用广播变量数据：value</h2><p>可在各个计算节点中通过 bc.value来引用广播的数据。</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012131609444-530646160.png" alt="img"></p>
<h2 id="更新广播变量：unpersist"><a href="#更新广播变量：unpersist" class="headerlink" title="更新广播变量：unpersist"></a>更新广播变量：unpersist</h2><p>由于广播变量是只读的，即广播出去的变量没法再修改，</p>
<p>利用unpersist函数将老的广播变量删除，然后重新广播一遍新的广播变量。</p>
<p>bc.unpersist()</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012131623395-1393879813.png" alt="img"></p>
<h2 id="销毁广播变量：destroy"><a href="#销毁广播变量：destroy" class="headerlink" title="销毁广播变量：destroy"></a>销毁广播变量：destroy</h2><p>bc.destroy()可将广播变量的数据和元数据一同销毁，销毁之后就不能再使用了。</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012131656566-939265172.png" alt="img"></p>
<h1 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h1><h2 id="概念：-1"><a href="#概念：-1" class="headerlink" title="概念："></a>概念：</h2><p>累加器是一种只能利用关联操作做“加”操作的变数，因此他能够快速执行并行操作。而且其能够操作counters和sums。Spark原本支援数值类型的累加器，程序员可以自行增加可被支援的类型。如果建立一个具体的累加器，其可在spark UI上显示。</p>
<p><img src="..\imgs\Spark累加器.jpg" alt></p>
<h2 id="用途："><a href="#用途：" class="headerlink" title="用途："></a>用途：</h2><p>对信息进行聚合，累加器的一个常见的用途是在调试时对作业的执行过程中事件进行计数。</p>
<h2 id="创建累加器：accumulator"><a href="#创建累加器：accumulator" class="headerlink" title="创建累加器：accumulator"></a>创建累加器：accumulator</h2><p>调用SparkContext.accumulator(v)方法从一个初始变量v中创建。</p>
<p>运行在集群上的任务可以通过add方法或者使用+=操作来给它加值。然而，它们无法读取这个值。和广播变量相反，累加器是一种add only的变项。</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012134901872-1936239419.png" alt="img"></p>
<h2 id="累加器的陷阱"><a href="#累加器的陷阱" class="headerlink" title="累加器的陷阱"></a>累加器的陷阱</h2><p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012134924591-641324349.png" alt="img"></p>
<h2 id="打破累加器陷阱：persist函数"><a href="#打破累加器陷阱：persist函数" class="headerlink" title="打破累加器陷阱：persist函数"></a>打破累加器陷阱：persist函数</h2><p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012135059496-1976934299.png" alt="img"></p>
<p>存累加器初始值：</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012135113704-339134018.png" alt="img"></p>
<p><img src="..\imgs\Spark累加器1.jpg" alt></p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012135002352-2093815302.png" alt="img"></p>
<p> 累加器实现一些基本的功能：</p>
<p><img src="https://img2018.cnblogs.com/blog/1338991/201810/1338991-20181012141358473-580698126.png" alt="img"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/窗口函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/窗口函数/" itemprop="url">窗口函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-13T00:00:00+08:00">
                2023-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p>[TOC]</p>
<h1 id="一、窗口函数的概念"><a href="#一、窗口函数的概念" class="headerlink" title="一、窗口函数的概念"></a>一、窗口函数的概念</h1><blockquote>
<p> 能返回整个dataframe，也能进行聚合运算。Spark支持三种窗口函数：排名函数、解析函数和聚合函数。</p>
</blockquote>
<p>例子：找到每年当中最冷那一天的温度 /  最冷那一天的日期。</p>
<ul>
<li>groupBy实现 </li>
</ul>
<p>【每年当中最冷的那一天的温度】：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = gsod.groupby(<span class="string">'year'</span>).agg(F.min(<span class="string">'temp'</span>).alias(<span class="string">'temp'</span>))</span><br><span class="line">a.orderBy(<span class="string">'temp'</span>)</span><br></pre></td></tr></table></figure>
<p>将上面的结果join回原来的dataframe得到【最冷那一天的日期】：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gsod.join(a, how = <span class="string">'left'</span>, on = [<span class="string">"year"</span>, <span class="string">"temp"</span>]).select(<span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'day'</span>, <span class="string">'temp'</span>)</span><br></pre></td></tr></table></figure>
<p>但是上面做法影响效率，left join！</p>
<ul>
<li>窗口函数的过程</li>
</ul>
<ol>
<li>根据某个条件对数据进行分组，PartitionBy</li>
<li>根据需求计算聚合函数</li>
<li>将计算结果Join回一个大dataframe</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line">each_year = Window.partitionBy(<span class="string">"year"</span>)</span><br><span class="line">gsod.withColumn(<span class="string">'min_temp'</span>,F.min(<span class="string">"temp"</span>).over(each_year))\</span><br><span class="line">.where(<span class="string">"temp=min_temp"</span>)\</span><br><span class="line">.select(<span class="string">"year"</span>, <span class="string">"month"</span>, <span class="string">"day"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="二、窗口函数"><a href="#二、窗口函数" class="headerlink" title="二、窗口函数"></a>二、窗口函数</h1><p>对于一个数据集，<code>map</code> 是对每行进行操作，为每行得到一个结果；<code>reduce</code> 则是对多行进行操作，得到一个结果；而 <code>window</code> 函数则是对多行进行操作，得到多个结果（每行一个）。</p>
<p>窗口函数是什么？来源于数据库，窗口函数是用与当前行<strong>有关</strong>的数据行参与计算。Mysql中：</p>
<ul>
<li>partition by：用于对全量数据表进行切分（与SQL中的groupby功能类似，但功能完全不同），直接体现的是前面窗口函数定义中的“有关”，即切分到同一组的即为有关，否则就是无关；</li>
<li>order by：用于指定对partition后各组内的数据进行排序；</li>
<li>rows between：用于对切分后的数据进一步限定“有关”行的数量，此种情景下即使partition后分到一组，也可能是跟当前行的计算无关。</li>
</ul>
<p><img src="https://ask.qcloudimg.com/http-save/yehe-7131597/hixjnsj96z.png?imageView2/2/w/1620" alt="img"></p>
<p>需求：组内按分数排序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select($<span class="string">"uid"</span>, $<span class="string">"date"</span>, $<span class="string">"score"</span>, row_number().over(Window.partitionBy(<span class="string">"uid"</span>).orderBy($<span class="string">"score"</span>.desc)).<span class="keyword">as</span>(<span class="string">"rank"</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://ask.qcloudimg.com/http-save/yehe-7131597/qi0imvjt5y.png?imageView2/2/w/1620" alt="img"></p>
<h2 id="udf-窗口函数"><a href="#udf-窗口函数" class="headerlink" title="udf + 窗口函数"></a>udf + 窗口函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Window</span><br><span class="line"></span><br><span class="line"><span class="meta">@pandas_udf("double")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_udf</span><span class="params">(v: pd.Series)</span> -&gt; float:</span></span><br><span class="line">    <span class="keyword">return</span> v.mean()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">1</span>, <span class="number">1.0</span>), (<span class="number">1</span>, <span class="number">2.0</span>), (<span class="number">2</span>, <span class="number">3.0</span>), (<span class="number">2</span>, <span class="number">5.0</span>), (<span class="number">2</span>, <span class="number">10.0</span>)], (<span class="string">"id"</span>, <span class="string">"v"</span>))</span><br><span class="line">w = Window.partitionBy(<span class="string">'id'</span>).orderBy(<span class="string">'v'</span>).rowsBetween(<span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line">df.withColumn(<span class="string">'mean_v'</span>, mean_udf(<span class="string">"v"</span>).over(w)).show()</span><br><span class="line"></span><br><span class="line">+---+----+------+</span><br><span class="line">| id|   v|mean_v|</span><br><span class="line">+---+----+------+</span><br><span class="line">|  <span class="number">1</span>| <span class="number">1.0</span>|   <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">1</span>| <span class="number">2.0</span>|   <span class="number">1.5</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">3.0</span>|   <span class="number">3.0</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">5.0</span>|   <span class="number">4.0</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="number">10.0</span>|   <span class="number">7.5</span>|</span><br><span class="line">+---+----+------+</span><br></pre></td></tr></table></figure>
<h1 id="三、-同时groupby-两个key"><a href="#三、-同时groupby-两个key" class="headerlink" title="三、 同时groupby 两个key"></a>三、 同时groupby 两个key</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成首次付费时间(FirstPayDate), 首次付费当天的付费金额(FirstPayPrice), 总付费金额(AllPayPrice). 首次单天付费频次(FirstPayFreq), 总付费频次(AllPayFreq)</span></span><br><span class="line">w1 = Window.partitionBy(<span class="string">"OrderId"</span>).orderBy(col(<span class="string">"PayDate"</span>))</span><br><span class="line">w2 = Window.partitionBy(<span class="string">"OrderId"</span>)</span><br><span class="line">df_pay_data = df_order_all.filter((F.col(<span class="string">"OrderType"</span>) == <span class="number">0</span>) &amp; ((F.col(<span class="string">"IsPay"</span>) == <span class="number">1</span>)))\</span><br><span class="line">    .withColumnRenamed(<span class="string">"OrderTime"</span>, <span class="string">"PayTime"</span>) \</span><br><span class="line">    .withColumnRenamed(<span class="string">"OrderPrice"</span>, <span class="string">"PayPrice"</span>) \</span><br><span class="line">    .withColumn(<span class="string">"PayDate"</span>, date_trunc(<span class="string">'day'</span>, to_timestamp(F.col(<span class="string">"PayTime"</span>)/<span class="number">1000</span>)))\</span><br><span class="line">    .withColumn(<span class="string">"row"</span>,row_number().over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"AllPayPrice"</span>, sum(col(<span class="string">"PayPrice"</span>)).over(w2)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayDate"</span>, min(col(<span class="string">"PayDate"</span>)).over(w2)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayPrice"</span>, sum(col(<span class="string">"PayPrice"</span>)).over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"FirstPayFreq"</span>, count(col(<span class="string">"IsPay"</span>)).over(w1)) \</span><br><span class="line">    .withColumn(<span class="string">"AllPayFreq"</span>, count(col(<span class="string">"IsPay"</span>)).over(w2)) \</span><br><span class="line">    .where(col(<span class="string">"row"</span>) == <span class="number">1</span>).select(<span class="string">"AllPayPrice"</span>, <span class="string">"FirstPayPrice"</span>, <span class="string">"FirstPayDate"</span>,<span class="string">"OrderId"</span>, <span class="string">"FirstPayFreq"</span>, <span class="string">"AllPayFreq"</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/" target="_blank" rel="noopener">https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/</a></p>
<p><a href="https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe" target="_blank" rel="noopener">https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe</a> (相加)</p>
<h1 id="四-groupby-sort-list"><a href="#四-groupby-sort-list" class="headerlink" title="四. groupby  + sort + list"></a>四. groupby  + sort + list</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group_df = qds_com.groupby([<span class="string">'company'</span>]) \</span><br><span class="line">    .agg(F.sort_array(F.collect_list(F.struct(<span class="string">"features"</span>, <span class="string">"label"</span>, <span class="string">"samples_count"</span>))) \</span><br><span class="line">         .alias(<span class="string">"pair"</span>))</span><br></pre></td></tr></table></figure>
<h1 id="五、求众数"><a href="#五、求众数" class="headerlink" title="五、求众数"></a>五、求众数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_com_cookie.groupBy([<span class="string">'company'</span>,<span class="string">'price'</span>]).agg(F.count(<span class="string">'price'</span>).alias(<span class="string">'count_price'</span>)).orderBy([<span class="string">'company'</span>,<span class="string">'count_price'</span>], ascending=<span class="literal">False</span>).drop_duplicates(subset=[<span class="string">'company'</span>]).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">window = Window.partitionBy([<span class="string">'a'</span>]).orderBy([<span class="string">'a'</span>])</span><br><span class="line">df.withColumn(<span class="string">'rank'</span>,F.rank().over(window)).filter(<span class="string">"rank = '1'"</span>).drop(<span class="string">'rank'</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/DataFrame/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/DataFrame/" itemprop="url">DataFrame</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-12T00:00:00+08:00">
                2023-03-12
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><h2 id="一、Spark-SQL"><a href="#一、Spark-SQL" class="headerlink" title="一、Spark SQL"></a>一、Spark SQL</h2><blockquote>
<p><strong>Spark SQL用于对结构化数据进行处理，它提供了DataFrame的抽象</strong>，作为分布式平台数据查询引擎，可以在此组件上构建大数据仓库。</p>
<p><strong>DataFrame是一个分布式数据集，在概念上类似于传统数据库的表结构</strong>，数据被组织成命名的列，DataFrame的数据源可以是结构化的数据文件，也可以是Hive中的表或外部数据库，也还可以是现有的RDD。</p>
<p>DataFrame的一个主要优点是，Spark引擎一开始就构建了一个逻辑执行计划，而且执行生成的代码是基于成本优化程序确定的物理计划。与Java或者Scala相比，<strong>Python中的RDD是非常慢的，而DataFrame的引入则使性能在各种语言中都保持稳定。</strong></p>
</blockquote>
<h2 id="二、初始化"><a href="#二、初始化" class="headerlink" title="二、初始化"></a>二、初始化</h2><blockquote>
<p>在过去，你可能会使用SparkConf、SparkContext、SQLContext和HiveContext来分别执行配置、Spark环境、SQL环境和Hive环境的各种Spark查询。</p>
<p><strong>SparkSession现在是读取数据、处理元数据、配置会话和管理集群资源的入口。SparkSession本质上是这些环境的组合</strong>，包括StreamingContext。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark=SparkSession \</span><br><span class="line">   .builder \</span><br><span class="line">   .appName(<span class="string">'test'</span>) \</span><br><span class="line">   .config(<span class="string">'master'</span>,<span class="string">'yarn'</span>) \</span><br><span class="line">   .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>Spark 交互式环境下，默认已经创建了名为 spark 的 SparkSession 对象，不需要自行创建。</p>
<p><strong>从RDD创建DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推断schema</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">lines = sc.textFile(<span class="string">"users.txt"</span>)</span><br><span class="line">parts = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line">data = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>],age=p[<span class="number">1</span>],city=p[<span class="number">2</span>]))</span><br><span class="line">df=createDataFrame(data)</span><br><span class="line"><span class="comment"># 指定schema</span></span><br><span class="line">data = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>],age=int(p[<span class="number">1</span>]),city=p[<span class="number">2</span>]))</span><br><span class="line">df=createDataFrame(data)</span><br><span class="line"><span class="comment"># StructType指定schema</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">schema = StructType([</span><br><span class="line">    StructField(<span class="string">'name'</span>,StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">'age'</span>,LongType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">'city'</span>,StringType(),<span class="literal">True</span>)</span><br><span class="line">    ])</span><br><span class="line">df=createDataFrame(parts, schema)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>StructField包括以下方面的内容：<br>name：字段名<br>dataType：数据类型<br>nullable：此字段的值是否为空</p>
</blockquote>
<p><strong>从文件系统创建DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.json(<span class="string">"customer.json"</span>)</span><br><span class="line">df = spark.read.load(<span class="string">"customer.json"</span>, format=<span class="string">"json"</span>)</span><br><span class="line">df = spark.read.load(<span class="string">"users.parquet"</span>)</span><br><span class="line">df = spark.read.text(<span class="string">"users.txt"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输出和保存</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.rdd <span class="comment"># df转化为RDD</span></span><br><span class="line">df.toJSON() <span class="comment"># df转化为RDD字符串</span></span><br><span class="line">df.toPandas() <span class="comment"># df转化为pandas</span></span><br><span class="line">df.write.save(<span class="string">"customer.json"</span>, format=<span class="string">"json"</span>)</span><br><span class="line">df.write.save(<span class="string">"users.parquet"</span>)</span><br><span class="line">df.write.json(<span class="string">"users.json"</span>)</span><br><span class="line">df.write.text(<span class="string">"users.txt"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>数据库读写</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.sql(<span class="string">'select name,age,city from users'</span>) </span><br><span class="line">df.createOrReplaceTempView(name) <span class="comment"># 创建临时视图</span></span><br><span class="line">df.write.saveAsTable(name,mode=<span class="string">'overwrite'</span>,partitionBy=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p><strong>操作hive表</strong><br><code>df.write</code> 有两种方法操作hive表</p>
<ul>
<li><code>saveAsTable()</code><br>如果hive中不存在该表，则spark会自动创建此表匹。<br>如果表已存在，则匹配插入数据和原表 schema(数据格式，分区等)，只要有区别就会报错<br>若是分区表可以调用<code>partitionBy</code>指定分区，使用<code>mode</code>方法调整数据插入方式：</li>
</ul>
<blockquote>
<p>Specifies the behavior when data or table already exists. Options include:</p>
<ul>
<li><code>overwrite</code>: 覆盖原始数据(包括原表的格式，注释等)</li>
<li><code>append</code>: 追加数据(需要严格匹配)</li>
<li><code>ignore</code>: ignore the operation (i.e. no-op).</li>
<li><code>error</code> or <code>errorifexists</code>: default option, throw an exception at runtime.</li>
</ul>
</blockquote>
<ul>
<li><code>df.write.partitionBy(&#39;dt&#39;).mode(&#39;append&#39;).saveAsTable(&#39;tb2&#39;)</code></li>
<li><code>insertInto()</code></li>
</ul>
<p>无关schema，只按数据的顺序插入，如果原表不存在则会报错<br>对于分区表，先开启Hive动态分区，则不需要指定分区字段，如果有一个分区，那么默认为数据中最后一列为分区字段，有两个分区则为最后两列为分区字段，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition"</span>, <span class="string">"true"</span>)</span><br><span class="line">sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition.mode"</span>, <span class="string">"nonstrict"</span>)</span><br><span class="line">df.write.insertInto(<span class="string">'tb2'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>同样也可以先开启Hive动态分区，用SQL语句直接运行<br><code>sql(&quot;insert into tb2 select * from tb1&quot;)</code></li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">DataFrame信息</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.show(n)</code></td>
<td style="text-align:left">预览前 n 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.collect()</code></td>
<td style="text-align:left">列表形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>df.dtypes</code></td>
<td style="text-align:left">列名与数据类型</td>
</tr>
<tr>
<td style="text-align:left"><code>df.head(n)</code></td>
<td style="text-align:left">返回前 n 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.first()</code></td>
<td style="text-align:left">返回第 1 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.take(n)</code></td>
<td style="text-align:left">返回前 n 行数据</td>
</tr>
<tr>
<td style="text-align:left"><code>df.printSchema()</code></td>
<td style="text-align:left">打印模式信息</td>
</tr>
<tr>
<td style="text-align:left"><code>df.columns</code></td>
<td style="text-align:left">列名</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">查询语句</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.select(*cols)</code></td>
<td style="text-align:left"><code>SELECT</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>df.union(other)</code></td>
<td style="text-align:left"><code>UNION ALL</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>df.when(condition,value)</code></td>
<td style="text-align:left"><code>CASE WHEN</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>df.alias(*alias,**kwargs)</code></td>
<td style="text-align:left"><code>as</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>F.cast(dataType)</code></td>
<td style="text-align:left">数据类型转换（函数）</td>
</tr>
<tr>
<td style="text-align:left"><code>F.lit(col)</code></td>
<td style="text-align:left">常数列（函数）</td>
</tr>
<tr>
<td style="text-align:left">selectExpr</td>
<td style="text-align:left">表查询</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line">df.select(<span class="string">'*'</span>)</span><br><span class="line">df.select(<span class="string">'name'</span>,<span class="string">'age'</span>) <span class="comment"># 字段名查询</span></span><br><span class="line">df.select([<span class="string">'name'</span>,<span class="string">'age'</span>]) <span class="comment"># 字段列表查询</span></span><br><span class="line">df.select(df[<span class="string">'name'</span>],df[<span class="string">'age'</span>]+<span class="number">1</span>) <span class="comment"># 表达式查询</span></span><br><span class="line">df.select(<span class="string">'name'</span>,df.mobile.alias(<span class="string">'phone'</span>)) <span class="comment"># 重命名列</span></span><br><span class="line">df.select(<span class="string">'name'</span>,<span class="string">'age'</span>,F.lit(<span class="string">'2020'</span>).alias(<span class="string">'update'</span>))  <span class="comment"># 常数</span></span><br><span class="line">df.select(<span class="string">'name'</span>,</span><br><span class="line">          F.when(df.age &gt; <span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">           .when(df.age &lt; <span class="number">0</span>,<span class="number">-1</span>)</span><br><span class="line">           .otherwise(df.age)</span><br><span class="line">          ).show()</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">df.select(<span class="string">'name'</span>,df.age.cast(<span class="string">'float'</span>))</span><br><span class="line">df.select(<span class="string">'name'</span>,df.age.cast(FloatType()))</span><br><span class="line"><span class="comment"># selectExpr接口支持并行计算</span></span><br><span class="line">expr=[<span class="string">'count(&#123;&#125;)'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> df.columns]</span><br><span class="line">df.selectExpr(*expr).collect()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#表查询selectExpr,可以使用UDF函数，指定别名等</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">spark.udf.register(<span class="string">"getBirthYear"</span>,<span class="keyword">lambda</span> age:datetime.datetime.now().year-age)</span><br><span class="line">dftest = df.selectExpr(<span class="string">"name"</span>, <span class="string">"getBirthYear(age) as birth_year"</span> , <span class="string">"UPPER(gender) as gender"</span> )</span><br><span class="line">dftest.show()</span><br><span class="line"></span><br><span class="line">---------------------------</span><br><span class="line"><span class="comment">#窗口函数</span></span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([(<span class="string">"LiLei"</span>,<span class="number">78</span>,<span class="string">"class1"</span>),(<span class="string">"HanMeiMei"</span>,<span class="number">87</span>,<span class="string">"class1"</span>),</span><br><span class="line">                           (<span class="string">"DaChui"</span>,<span class="number">65</span>,<span class="string">"class2"</span>),(<span class="string">"RuHua"</span>,<span class="number">55</span>,<span class="string">"class2"</span>)]) \</span><br><span class="line">    .toDF(<span class="string">"name"</span>,<span class="string">"score"</span>,<span class="string">"class"</span>)</span><br><span class="line"></span><br><span class="line">df.show()</span><br><span class="line">dforder = df.selectExpr(<span class="string">"name"</span>,<span class="string">"score"</span>,<span class="string">"class"</span>,</span><br><span class="line">         <span class="string">"row_number() over (partition by class order by score desc) as order"</span>)</span><br><span class="line"></span><br><span class="line">dforder.show()</span><br><span class="line">+---------+-----+------+</span><br><span class="line">|     name|score| <span class="class"><span class="keyword">class</span>|</span></span><br><span class="line"><span class="class">+---------+-----+------+</span></span><br><span class="line"><span class="class">|    <span class="title">LiLei</span>|   78|<span class="title">class1</span>|</span></span><br><span class="line"><span class="class">|<span class="title">HanMeiMei</span>|   87|<span class="title">class1</span>|</span></span><br><span class="line"><span class="class">|   <span class="title">DaChui</span>|   65|<span class="title">class2</span>|</span></span><br><span class="line"><span class="class">|    <span class="title">RuHua</span>|   55|<span class="title">class2</span>|</span></span><br><span class="line"><span class="class">+---------+-----+------+</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">+---------+-----+------+-----+</span></span><br><span class="line"><span class="class">|     <span class="title">name</span>|<span class="title">score</span>| <span class="title">class</span>|<span class="title">order</span>|</span></span><br><span class="line"><span class="class">+---------+-----+------+-----+</span></span><br><span class="line"><span class="class">|   <span class="title">DaChui</span>|   65|<span class="title">class2</span>|    1|</span></span><br><span class="line"><span class="class">|    <span class="title">RuHua</span>|   55|<span class="title">class2</span>|    2|</span></span><br><span class="line"><span class="class">|<span class="title">HanMeiMei</span>|   87|<span class="title">class1</span>|    1|</span></span><br><span class="line"><span class="class">|    <span class="title">LiLei</span>|   78|<span class="title">class1</span>|    2|</span></span><br><span class="line"><span class="class">+---------+-----+------+-----+</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">排序</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.sort(*col,**kwargs)</code></td>
<td style="text-align:left">排序</td>
</tr>
<tr>
<td style="text-align:left"><code>df.orderBy(*col,**kwargs)</code></td>
<td style="text-align:left">排序(用法同sort)</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.sort(df.age.desc()).show()</span><br><span class="line">df.sort(<span class="string">'age'</span>,ascending=<span class="literal">True</span>).show()</span><br><span class="line">df.sort(desc(<span class="string">'age'</span>),<span class="string">'name'</span>).show()</span><br><span class="line">df.sort([<span class="string">'age'</span>,<span class="string">'name'</span>],ascending=[<span class="number">0</span>,<span class="number">1</span>]).show()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">筛选方法</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.filter(condition)</code></td>
<td style="text-align:left">筛选</td>
</tr>
<tr>
<td style="text-align:left"><code>column.isin(*cols)</code></td>
<td style="text-align:left"><code>in (...)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>column.like(pattern)</code></td>
<td style="text-align:left">SQL通配符匹配</td>
</tr>
<tr>
<td style="text-align:left"><code>column.rlike(pattern)</code></td>
<td style="text-align:left">正则表达式匹配</td>
</tr>
<tr>
<td style="text-align:left"><code>column.startswith(pattern)</code></td>
<td style="text-align:left">匹配开始</td>
</tr>
<tr>
<td style="text-align:left"><code>column.endswith(pattern)</code></td>
<td style="text-align:left">匹配结尾</td>
</tr>
<tr>
<td style="text-align:left"><code>column.substr(start,length)</code></td>
<td style="text-align:left">截取字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>column.between(lower,upper)</code></td>
<td style="text-align:left"><code>between ... and ...</code></td>
</tr>
<tr>
<td style="text-align:left">column.where</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.filter(<span class="string">"age = 22"</span>).show()</span><br><span class="line">df.filter(df.age == <span class="number">22</span>).show()</span><br><span class="line">df.select(df[<span class="string">'age'</span>] == <span class="number">22</span>).show()</span><br><span class="line">df.select(df.name.isin(<span class="string">'Bill'</span>,<span class="string">'Elon'</span>)).show()</span><br><span class="line">df.filter(<span class="string">"name like Elon%"</span>).show()</span><br><span class="line">df.filter(df.name.rlike(<span class="string">"Musk$"</span>).show()</span><br><span class="line">          </span><br><span class="line">df.where(<span class="string">"gender='male' and age &gt; 15"</span>).show()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">统计信息</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.describe()</code></td>
<td style="text-align:left">描述性统计</td>
</tr>
<tr>
<td style="text-align:left"><code>df.count()</code></td>
<td style="text-align:left">行数</td>
</tr>
<tr>
<td style="text-align:left"><code>df.approxQuantile(col,prob,relativeError)</code></td>
<td style="text-align:left">百分位数</td>
</tr>
<tr>
<td style="text-align:left"><code>df.corr(col1,col2,method=None)</code></td>
<td style="text-align:left">相关系数</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异常值处理</span></span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    quantiles = df.approxQuantile(col,[<span class="number">0.25</span>,<span class="number">0.75</span>],<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># 第三个参数relativeError代表可接受的错误程度，越小精度越高</span></span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [quantiles[<span class="number">0</span>]<span class="number">-1.5</span>*IQR, quantiles[<span class="number">1</span>]+<span class="number">1.5</span>*IQR]</span><br><span class="line">    <span class="comment"># bounds保存了每个特征的上下限</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">分组和聚合</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.groupBy(*cols)</code></td>
<td style="text-align:left">分组，返回GroupedData</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.count()</code></td>
<td style="text-align:left">计数</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.sum(*cols)</code></td>
<td style="text-align:left">求和</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.avg(*cols)</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.mean(*cols)</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.max(*cols)</code></td>
<td style="text-align:left">最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.min(*cols)</code></td>
<td style="text-align:left">最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>groupedData.agg(*exprs)</code></td>
<td style="text-align:left">应用表达式</td>
</tr>
</tbody>
</table>
<blockquote>
<p> 聚合函数还包括 countDistinct, kurtosis, skewness, stddev, sumDistinct, variance 等</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">'city'</span>).count().collect()</span><br><span class="line">df.groupBy(df.city).avg(<span class="string">'age'</span>).collect()</span><br><span class="line">df.groupBy(<span class="string">'city'</span>,df.age).count().collect()</span><br><span class="line">df.groupBy(<span class="string">'city'</span>).agg(&#123;<span class="string">'age'</span>:<span class="string">'mean'</span>&#125;).collect() <span class="comment"># 字典形式给出</span></span><br><span class="line">df.groupBy(<span class="string">'city'</span>).agg(&#123;<span class="string">'*'</span>:<span class="string">'count'</span>&#125;).collect() </span><br><span class="line">df.groupBy(<span class="string">'city'</span>).agg(F.mean(df.age)).collect() </span><br><span class="line"><span class="comment"># groupBy + collect_list</span></span><br><span class="line">df.groupBy(<span class="string">"gender"</span>).agg(F.expr(<span class="string">"avg(age)"</span>),F.expr(<span class="string">"collect_list(name)"</span>)).show()</span><br><span class="line">+------+--------+------------------+</span><br><span class="line">|gender|avg(age)|collect_list(name)|</span><br><span class="line">+------+--------+------------------+</span><br><span class="line">|  null|    <span class="number">16.0</span>|           [RuHua]|</span><br><span class="line">|female|    <span class="number">16.0</span>|       [HanMeiMei]|</span><br><span class="line">|  male|    <span class="number">16.0</span>|   [LiLei, DaChui]|</span><br><span class="line">+------+--------+------------------+</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">去重</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.distinct()</code></td>
<td style="text-align:left">唯一值（整行去重）</td>
</tr>
<tr>
<td style="text-align:left"><code>df.dropDuplicates(subset=None)</code></td>
<td style="text-align:left">删除重复项（可以指定字段）</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">添加、修改、删除列</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.withColumnRenamed(existing,new)</code></td>
<td style="text-align:left">重命名</td>
</tr>
<tr>
<td style="text-align:left"><code>df.withColumn(colname,new)</code></td>
<td style="text-align:left">修改列</td>
</tr>
<tr>
<td style="text-align:left"><code>df.drop(*cols)</code></td>
<td style="text-align:left">删除列</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df=df.withColumn(<span class="string">'age'</span>,df.age+<span class="number">1</span>)</span><br><span class="line">df=df.drop(<span class="string">'age'</span>)</span><br><span class="line">df=df.drop(df.age)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">缺失值处理</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.na.fill(value,subset=None)</code></td>
<td style="text-align:left">缺失值填充</td>
</tr>
<tr>
<td style="text-align:left"><code>df.na.drop(how=&#39;any&#39;,thresh=None,subset=None)</code></td>
<td style="text-align:left">缺失值删除</td>
</tr>
<tr>
<td style="text-align:left"><code>df.na.replace(to_teplace,value,subset=None)</code></td>
<td style="text-align:left">替换</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df=df.na.fill(<span class="number">0</span>)</span><br><span class="line">df=df.na.fill(&#123;<span class="string">'age'</span>:<span class="number">50</span>,<span class="string">'name'</span>:<span class="string">'unknow'</span>&#125;)</span><br><span class="line">df=df.na.drop()</span><br><span class="line">df = df.dropna() <span class="comment"># 跟上面那种方式是一样的</span></span><br><span class="line">df=df.na.replace([<span class="string">'Alice'</span>,<span class="string">'Bob'</span>],[<span class="string">'A'</span>,<span class="string">'B'</span>],<span class="string">'name'</span>)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">分区和缓存</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>df.repartition(n)</code></td>
<td style="text-align:left">将df拆分为10个分区</td>
</tr>
<tr>
<td style="text-align:left"><code>df.coalesce(n)</code></td>
<td style="text-align:left">将df合并为n个分区</td>
</tr>
<tr>
<td style="text-align:left"><code>df.cache()</code></td>
<td style="text-align:left">缓存</td>
</tr>
</tbody>
</table>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/理解RDD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/理解RDD/" itemprop="url">RDD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-11T00:00:00+08:00">
                2023-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<h1 id="一、理解RDD"><a href="#一、理解RDD" class="headerlink" title="一、理解RDD"></a>一、理解RDD</h1><blockquote>
<p> RDD可以被抽象地理解为一个大的数组，但是这个数组是分布在集群上的。</p>
<p><img src="..\imgs\RDD的理解.jpg" alt></p>
</blockquote>
<h1 id="二、RDD的生命周期"><a href="#二、RDD的生命周期" class="headerlink" title="二、RDD的生命周期"></a>二、RDD的生命周期</h1><p>创建—变换—动作（结束）</p>
<h1 id="三、RDD依赖"><a href="#三、RDD依赖" class="headerlink" title="三、RDD依赖"></a>三、RDD依赖</h1><p>1、窄依赖（RDD）—–原地变换，不需要shuffle【即各个RDD之间不需要统计】</p>
<p><img src="..\imgs\RDD1.jpg" alt></p>
<p>2、宽依赖（RDD）—–需要shuffle，与其他RDD交换资料，时间消耗长。</p>
<p><img src="..\imgs\RDD2.jpg" alt></p>
<p>3、任务优化，如</p>
<p><img src="..\imgs\RDD3.jpg" alt></p>
<h1 id="四、RDD的基本操作"><a href="#四、RDD的基本操作" class="headerlink" title="四、RDD的基本操作"></a>四、RDD的基本操作</h1><p>RDD可以有两种计算操作算子：Transformation（变换）与Action（行动）。</p>
<p><img src="..\imgs\RDD4.jpg" alt></p>
<h2 id="1、基本的RDD"><a href="#1、基本的RDD" class="headerlink" title="1、基本的RDD"></a>1、基本的RDD</h2><p>（1）建立RDD</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wordsList =  [<span class="string">'cat'</span>,<span class="string">'ele'</span>,<span class="string">'rat'</span>,<span class="string">'rat'</span>,<span class="string">'cat'</span>]</span><br><span class="line">wordsRDD =  sc.parallelize(wordsList , <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>   #建立RDD   <code>wordsList =   [&#39;cat&#39;,&#39;ele&#39;,&#39;rat&#39;,&#39;rat&#39;,&#39;cat&#39;]</code>   <code>wordsRDD =   sc.parallelize(wordsList , 4)</code>   </p>
<p>   #计算法每个单词的长度   <code>wordsRDD.map(len).collect()</code>   </p>
<ul>
<li>转换操作</li>
</ul>
<p><img src="..\imgs\RDD5.jpg" alt></p>
<ul>
<li>动作操作</li>
</ul>
<p><img src="..\imgs\RDD6.jpg" alt></p>
<h2 id="键值对PairRDD："><a href="#键值对PairRDD：" class="headerlink" title="键值对PairRDD："></a>键值对PairRDD：</h2><p>是一种以（key,value)方式存储的RDD。</p>
<p>   <code>pairRDD =   wordsRDD.map(lambda x : (x , 1))</code>   </p>
<ul>
<li><p>转换操作</p>
<p><img src="..\imgs\RDD7.jpg" alt></p>
</li>
<li><p>动作操作</p>
<p><img src="..\imgs\RDD8.jpg" alt></p>
</li>
</ul>
<p>一些小问题：</p>
<p>1、spark中的RDD是什么</p>
<p>概念：<strong>分布式数据集，**</strong>spark<strong>**中基本的数据抽象，代表一个不可变、可分区、元素可并行计算的集合。</strong></p>
<p>2、RDD的五大特性：</p>
<p>①有一个<strong>分区</strong>列表，即能被切分，可并行。</p>
<p>②由一个<strong>函数</strong>计算每一个分片</p>
<p>③<strong>容错机制</strong>，对其他RDD的<strong>依赖</strong>（宽依赖和窄依赖），但并非所有RDD都要依赖。</p>
<p>RDD每次transformations（转换）都会生成一个新的RDD，两者之间会形成依赖关系。在部分分区数据丢失时，可通过依赖关系重新计算丢失的数据。</p>
<p>④key-value型的RDD是根据<strong>哈希</strong>来分区的，控制Key分到哪个reduce。</p>
<p>⑤每一分片<strong>计算优先位置</strong>，比如HDFS的block的所在位置应该是优先计算的位置。</p>
<p>3、概述一下spark中的常用算子区别（map、mapPartitions、foreach、foreachPartition）</p>
<table>
<thead>
<tr>
<th>map</th>
<th>遍历RDD，将函数f应用于每一个元素，返回新的RDD</th>
<th>transformation算子</th>
</tr>
</thead>
<tbody>
<tr>
<td>mapPartitions</td>
<td>用于遍历操作RDD中的每一个分区，返回生成一个新的RDD</td>
<td>transformation</td>
</tr>
<tr>
<td>collect</td>
<td>将RDD元素送回Master并返回List类型</td>
<td>Action</td>
</tr>
<tr>
<td>foreach</td>
<td>用于遍历RDD,将函数f应用于每一个元素，无返回值</td>
<td>action算子</td>
</tr>
<tr>
<td>foreachPartition</td>
<td>用于遍历操作RDD中的每一个分区。无返回值</td>
<td>action算子</td>
</tr>
<tr>
<td>总结</td>
<td>一般使用mapPartitions或者foreachPartition算子比map和foreach更加高效，推荐使用。</td>
</tr>
</tbody>
</table>
<p>4、谈谈spark中的宽窄依赖</p>
<ul>
<li>RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖和宽依赖。</li>
<li>宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition分区【需要shuffle，与其他RDD交换资料】 例如 groupByKey、 reduceByKey、 sortByKey等操作会产生宽依赖，会产生shuffle      </li>
<li>窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个Partition分区使用。【原地变换，不需要shuffle】      例如map、filter、union等操作会产生窄依赖 </li>
</ul>
<p>5、spark中如何划分stage</p>
<p>Stage划分的依据就是宽依赖，何时产生宽依赖，例如reduceByKey,groupByKey的算子，会导致宽依赖的产生。</p>
<table>
<thead>
<tr>
<th>先介绍什么是RDD中的宽窄依赖，</th>
</tr>
</thead>
<tbody>
<tr>
<td>然后在根据DAG有向无环图进行划分，从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage,</td>
</tr>
<tr>
<td>然后继续按照这种方式在继续往前推，如在遇到宽依赖，又划分成一个stage,一直到最前面的一个算子。</td>
</tr>
<tr>
<td>最后整个job会被划分成多个stage,而stage之间又存在依赖关系，后面的stage依赖于前面的stage。</td>
</tr>
</tbody>
</table>
<h1 id="五、代码学习"><a href="#五、代码学习" class="headerlink" title="五、代码学习"></a>五、代码学习</h1><h3 id="1-建立RDD"><a href="#1-建立RDD" class="headerlink" title="1. 建立RDD"></a>1. 建立RDD</h3><p><strong>创建RDD的两种方法：</strong></p>
<p>1 读取一个数据集(SparkContext.textFile()) : lines = sc.textFile(“README.md”)<br>2 读取一个集合(SparkContext.parallelize()) : lines = sc.paralelize(List(“pandas”,”i like pandas”))</p>
<p><img src="E:\GitHub_learn\blog\source\imgs\rdd_1.jpg" alt></p>
<p>#take操作将前若干个数据汇集到Driver，相比collect安全</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建RDD</span></span><br><span class="line"><span class="comment"># 从并行集合创建</span></span><br><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#collect操作将数据汇集到Driver,数据过大时有超内存风险</span></span><br><span class="line">all_data = rdd.collect()</span><br><span class="line">all_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#take操作将前若干个数据汇集到Driver，相比collect安全</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>) </span><br><span class="line">part_data = rdd.take(<span class="number">4</span>)</span><br><span class="line">part_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#takeSample可以随机取若干个到Driver,第一个参数设置是否放回抽样</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>) </span><br><span class="line">sample_data = rdd.takeSample(<span class="literal">False</span>,<span class="number">10</span>,<span class="number">0</span>)</span><br><span class="line">sample_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#first取第一个数据</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>) </span><br><span class="line">first_data = rdd.first()</span><br><span class="line">print(first_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#count查看RDD元素数量</span></span><br><span class="line">rdd = sc.parallelize(range(<span class="number">10</span>),<span class="number">5</span>)</span><br><span class="line">data_count = rdd.count()</span><br><span class="line">print(data_count)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>从text中读取，read.text</strong></li>
</ul>
<p><img src="..\imgs\rdd_2.jpg" alt></p>
<ul>
<li><strong>从csv中读取:read.csv</strong></li>
</ul>
<p><img src="..\imgs\rdd_3.jpg" alt></p>
<ul>
<li><strong>从json中读取：read.json</strong></li>
</ul>
<p><img src="..\imgs\rdd_4.jpg" alt></p>
<h3 id="2-RDD与Dataframe的转换"><a href="#2-RDD与Dataframe的转换" class="headerlink" title="2. RDD与Dataframe的转换"></a>2. RDD与Dataframe的转换</h3><p><strong>（1）dataframe转换成rdd：</strong></p>
<p><strong>法一：datardd = dataDataframe.rdd</strong></p>
<p><strong>法二：datardd = sc.parallelize(_)</strong></p>
<p><strong>（2）rdd转换成dataframe：</strong></p>
<p><strong>dataDataFrame = spark.createDataFrame(datardd)</strong></p>
<p><img src="..\imgs\rdd_5.jpg" alt></p>
<p><img src="..\imgs\rdd_6.jpg" alt></p>
<p><img src="..\imgs\rdd_7.jpg" alt></p>
<h3 id="3-rdd函数"><a href="#3-rdd函数" class="headerlink" title="3. rdd函数"></a>3. rdd函数</h3><table>
<thead>
<tr>
<th style="text-align:left">mapReduce</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.map(func)</code></td>
<td style="text-align:left">将<a href="http://www.aisouwen.com/tags_38.html" target="_blank" rel="noopener">函数</a>应用于RDD中的每个元素并返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.mapValues(func)</code></td>
<td style="text-align:left">不改变key，只对value执行map</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.flatMap(func)</code></td>
<td style="text-align:left">先map后扁平化返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.flatMapValues(func)</code></td>
<td style="text-align:left">不改变key，只对value执行flatMap</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.reduce(func)</code></td>
<td style="text-align:left">合并RDD的元素返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.reduceByKey(func)</code></td>
<td style="text-align:left">合并每个key的value</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.foreach(func)</code></td>
<td style="text-align:left">用迭代的方法将函数应用于每个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.keyBy(func)</code></td>
<td style="text-align:left">执行函数于每个元素创建key-value对RDD</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.map(<span class="keyword">lambda</span> x:x+<span class="number">1</span>).collect()</span><br><span class="line">[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.reduce(<span class="keyword">lambda</span> x,y : x+y)</span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.keyBy(<span class="keyword">lambda</span> x:x%<span class="number">2</span>).collect()</span><br><span class="line">[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">0</span>,<span class="number">4</span>),(<span class="number">1</span>,<span class="number">5</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.mapValues(<span class="keyword">lambda</span> x:x+<span class="number">1</span>).collect()</span><br><span class="line">[(<span class="string">'a'</span>,<span class="number">8</span>),(<span class="string">'a'</span>,<span class="number">3</span>),(<span class="string">'b'</span>,<span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.reduceByKey(<span class="keyword">lambda</span> x,y : x+y).collect()</span><br><span class="line">[(<span class="string">'a'</span>,<span class="number">9</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names=sc.parallelize([<span class="string">'Elon Musk'</span>,<span class="string">'Bill Gates'</span>,<span class="string">'Jim Green'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names.map(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>)).collect()</span><br><span class="line">[(<span class="string">'Elon'</span>,<span class="string">'Musk'</span>),(<span class="string">'Bill'</span>,<span class="string">'Gates'</span>),(<span class="string">'Jim'</span>,<span class="string">'Green'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names.flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>)).collect()</span><br><span class="line">[<span class="string">'Elon'</span>,<span class="string">'Musk'</span>,<span class="string">'Bill'</span>,<span class="string">'Gates'</span>,<span class="string">'Jim'</span>,<span class="string">'Green'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>（1）map操作</strong></p>
<p><img src="..\imgs\rdd_8.jpg" alt></p>
<p><strong>（2）collect操作</strong></p>
<p><img src="..\imgs\rdd_10.jpg" alt></p>
<table>
<thead>
<tr>
<th style="text-align:left">提取</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.collect()</code></td>
<td style="text-align:left">将RDD以列表形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.collectAsMap()</code></td>
<td style="text-align:left">将RDD以字典形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.take(n)</code></td>
<td style="text-align:left">提取前n个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.takeSample(replace,n,seed)</code></td>
<td style="text-align:left">随机提取n个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.first()</code></td>
<td style="text-align:left">提取第1名</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.top(n)</code></td>
<td style="text-align:left">提取前n名</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.keys()</code></td>
<td style="text-align:left">返回RDD的keys</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.values()</code></td>
<td style="text-align:left">返回RDD的values</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.isEmpty()</code></td>
<td style="text-align:left">检查RDD是否为空</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.collectAsMap()</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">2</span>,<span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.keys().collect()</span><br><span class="line">[<span class="string">'a'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.values().collect()</span><br><span class="line">[<span class="number">7</span>,<span class="number">2</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">分组和聚合</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.groupBy(func)</code></td>
<td style="text-align:left">将RDD元素通过函数变换分组为key-iterable集</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.groupByKey()</code></td>
<td style="text-align:left">将key-value元素集分组为key-iterable集</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.aggregate(zeroValue,seqOp,combOp)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.aggregateByKey(zeroValue,seqOp,combOp)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.fold(zeroValue,func)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.foldByKey(zeroValue,func)</code></td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.groupBy(<span class="keyword">lambda</span> x: x % <span class="number">2</span>).mapValues(list).collect()</span><br><span class="line">[(<span class="number">0</span>,[<span class="number">2</span>,<span class="number">4</span>]),(<span class="number">1</span>,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>])]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.groupByKey().mapValues(list).collect()</span><br><span class="line">[(<span class="string">'a'</span>,[<span class="number">7</span>,<span class="number">2</span>]),(<span class="string">'b'</span>,[<span class="number">2</span>])]</span><br></pre></td></tr></table></figure>
<blockquote>
<h3 id="更好的解决方案：reduceByKey-非groupBykey"><a href="#更好的解决方案：reduceByKey-非groupBykey" class="headerlink" title="更好的解决方案：reduceByKey,非groupBykey"></a>更好的解决方案：reduceByKey,非groupBykey</h3><p>reduceByKey能够直接将资料根据key值聚合，减少多余的交换（shuffle）动作。</p>
<p>避免使用groupbykey，如果数据量过大，会造成内存溢出。</p>
</blockquote>
<ol>
<li><strong>spark中groupByKey 、aggregateByKey、reduceByKey 有什么区别？使用上需要注意什么？</strong></li>
</ol>
<p>（1）groupByKey()是对RDD中的所有数据做shuffle,根据不同的Key映射到不同的partition中再进行aggregate。</p>
<p>（3） distinct()也是对RDD中的所有数据做shuffle进行aggregate后再去重。</p>
<p>（2）aggregateByKey()是先对每个partition中的数据根据不同的Key进行aggregate，然后将结果进行shuffle，完成各个partition之间的aggregate。因此，和groupByKey()相比，运算量小了很多。</p>
<p>（4）reduceByKey()也是先在单台机器中计算，再将结果进行shuffle，减小运算量 </p>
<p><img src="..\imgs\rdd_11.jpg" alt></p>
<table>
<thead>
<tr>
<th style="text-align:left">选择数据</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.sample(replace,frac,seed)</code></td>
<td style="text-align:left">抽样</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.filter(func)</code></td>
<td style="text-align:left">筛选满足函数的元素(变换)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.distinct()</code></td>
<td style="text-align:left">去重</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2=sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.sample(<span class="literal">False</span>,<span class="number">0.8</span>,seed=<span class="number">42</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>).collect()</span><br><span class="line">[<span class="number">2</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">排序</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.sortBy(func,ascending=True)</code></td>
<td style="text-align:left">按RDD元素变换后的值排序</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.sortByKey(ascending=True)</code></td>
<td style="text-align:left">按key排序</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">统计</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.count()</code></td>
<td style="text-align:left">返回RDD中的元素数</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.countByKey()</code></td>
<td style="text-align:left">按key计算RDD元素数量</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.countByValue()</code></td>
<td style="text-align:left">按RDD元素计算数量</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.sum()</code></td>
<td style="text-align:left">求和</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.mean()</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.max()</code></td>
<td style="text-align:left">最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.min()</code></td>
<td style="text-align:left">最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.stdev()</code></td>
<td style="text-align:left">标准差</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.variance()</code></td>
<td style="text-align:left">方差</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.histograme()</code></td>
<td style="text-align:left">分箱（Bin）生成直方图</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.stats()</code></td>
<td style="text-align:left">综合统计（计数、平均值、标准差、最大值和最小值）</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pairRDD=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]) <span class="comment"># key-value对RDD</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.count()</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.countByKey()</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,&#123;<span class="string">'a'</span>:<span class="number">2</span>,<span class="string">'b'</span>:<span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.countByValue()</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,&#123;(<span class="string">'b'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">7</span>):<span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.histogram(<span class="number">3</span>)</span><br><span class="line">([<span class="number">0</span>,<span class="number">33</span>,<span class="number">66</span>,<span class="number">99</span>],[<span class="number">33</span>,<span class="number">33</span>,<span class="number">34</span>])</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">连接运算</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.union(other)</code></td>
<td style="text-align:left">并集(不去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.intersection(other)</code></td>
<td style="text-align:left">交集(去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.subtract(other)</code></td>
<td style="text-align:left">差集(不去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.cartesian(other)</code></td>
<td style="text-align:left">笛卡尔积</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.subtractByKey(other)</code></td>
<td style="text-align:left">按key差集</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.join(other)</code></td>
<td style="text-align:left">内连接</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.leftOuterJoin(other)</code></td>
<td style="text-align:left">左连接</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.rightOuterJoin(other)</code></td>
<td style="text-align:left">右连接</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1=sc.parallelize([<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2=sc.parallelize([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.union(rdd1).collect()</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.intersection(rdd2).collect()</span><br><span class="line">[<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.subtract(rdd2).collect()</span><br><span class="line">[<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.cartesian(rdd2).collect()</span><br><span class="line">[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">1</span>),(<span class="number">3</span>,<span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1=sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2=sc.parallelize([(<span class="string">'b'</span>,<span class="string">'B'</span>),(<span class="string">'c'</span>,<span class="string">'C'</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.subtractByKey(rdd2).collect()</span><br><span class="line">[(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.join(rdd2).collect()</span><br><span class="line">[(<span class="string">'b'</span>,(<span class="number">2</span>,<span class="string">'B'</span>))]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">持久化</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.persist()</code></td>
<td style="text-align:left">标记为持久化</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.cache()</code></td>
<td style="text-align:left">等价于<code>rdd.persist(MEMORY_ONLY)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.unpersist()</code></td>
<td style="text-align:left">释放缓存</td>
</tr>
</tbody>
</table>
<ol>
<li><strong>cache后面能不能接其他算子,它是不是action操作？</strong></li>
</ol>
<p>Cache后可以接其他算子，但是接了算子之后，起不到缓存的作用，因为会重复出发cache。</p>
<p>Cache不是action操作。</p>
<ol>
<li><strong>cache和pesist有什么区别？</strong> </li>
</ol>
<p>（1）cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间； </p>
<p>（2）cache只有一个默认的缓存级别MEMORY_ONLY ，cache调用了persist，而persist可以根据情况设置其它的缓存级别； </p>
<p>（3）executor执行的时候，默认60%做cache，40%做task操作，persist最根本的函数，最底层的函数 </p>
<ol>
<li><strong>Spark为什么要持久化，一般什么场景下要进行persist操作？</strong></li>
</ol>
<p>为什么要进行持久化？</p>
<p>spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤</p>
<p>只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。</p>
<p>以下场景会使用persist</p>
<p>1）某个步骤计算非常耗时，需要进行persist持久化</p>
<p>2）计算链条非常长，重新恢复要算很多步骤，很好使，persist</p>
<p>3）checkpoint所在的rdd要持久化persist，</p>
<p>lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前</p>
<p>要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。</p>
<p>4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</p>
<p>5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</p>
<table>
<thead>
<tr>
<th style="text-align:left">分区</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>rdd.getNumPartitions()</code></td>
<td style="text-align:left">获取RDD分区数</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.repartition(n)</code></td>
<td style="text-align:left">新建一个含n个分区的RDD</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.coalesce(n)</code></td>
<td style="text-align:left">将RDD中的分区减至n个</td>
</tr>
<tr>
<td style="text-align:left"><code>rdd.partitionBy(key,func)</code></td>
<td style="text-align:left">自定义分区</td>
</tr>
</tbody>
</table>
<p><strong>文件系统读写</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取</span></span><br><span class="line">rdd=sc.textFile(<span class="string">'hdfs://file_path'</span>)  <span class="comment"># 从hdfs集群读取</span></span><br><span class="line">rdd=sc.textFile(<span class="string">'file_path'</span>) </span><br><span class="line">rdd=sc.textFile(<span class="string">'file:///local_file_path'</span>) <span class="comment"># 从本地文件读取</span></span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">'hdfs://file_path'</span>)</span><br><span class="line">rdd.saveAsTextFile(<span class="string">'file_path'</span>) <span class="comment"># hdfs路径</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">'file:///local_file_path'</span>)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Spark简介/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lee_yl">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lee_yl's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark简介/" itemprop="url">1.Spark简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-03-09T00:00:00+08:00">
                2023-03-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文仅做学习总结，如有侵权立删</p>
<p><a href="https://blog.csdn.net/weixin_42331985/article/details/124126019" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42331985/article/details/124126019</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/396809439" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/396809439</a></p>
<p><a href="https://blog.csdn.net/czz1141979570/article/details/105877261/" target="_blank" rel="noopener">https://blog.csdn.net/czz1141979570/article/details/105877261/</a></p>
<p>[TOC]</p>
<h1 id="Spark生态架构图"><a href="#Spark生态架构图" class="headerlink" title="Spark生态架构图"></a>Spark生态架构图</h1><h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h2><p><img src="..\imgs\spark组件介绍.jpg" alt></p>
<blockquote>
<p><strong>Spark：</strong>基于内存的迭代式计算引擎。</p>
<p><strong>RDD：</strong>Resillient Distributed Dataset（弹性分布式数据集），是分布式内存的一个抽象概念。</p>
<p><strong>DAG：</strong>Directed Acyclic Graph（有向无环图），反映RDD之间的依赖关系。</p>
<p><img src="https://img-blog.csdnimg.cn/99c95c8e6e724185b86fa7e0ba42f7fc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAUmVsaWFu5ZOI5ZOI,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img"></p>
<p><strong>Executor</strong>：是运行在工作节点（WorkerNode）的一个进程，负责运行Task</p>
<p><strong>应用（Application）</strong>：用户编写的Spark应用程序</p>
<p><strong>任务（ Task ）</strong>：运行在Executor上的工作单元(线程)</p>
<p><strong>作业（ Job ）</strong>：一个作业包含多个RDD及作用于相应RDD上的各种操作</p>
<p><strong>阶段（ Stage ）</strong>：是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集, 下图为DAG划分Stage过程：</p>
<p><img src="https://img-blog.csdnimg.cn/70e593dbb2c54f53a01f239947f7e451.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAUmVsaWFu5ZOI5ZOI,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img"></p>
</blockquote>
<h2 id="2-组件关系"><a href="#2-组件关系" class="headerlink" title="2. 组件关系"></a>2. 组件关系</h2><p>当执行一个Application时，Driver会向Yarn申请资源，启动Executor（Worker），并向Executor发送代码和文件，执行任务，任务结束后执行结果会返回给任务控制节点，或者写到HDFS/Hive等。</p>
<p>1 Application = 1 Driver + 多 Job </p>
<p>1 Job（多个 RDD + RDD的操作） = 多个Stage </p>
<p>1 Stage = 多个Task</p>
<p><img src="..\imgs\Spark组件关系.jpg" alt></p>
<p><img src="..\imgs\Spark基本组件.jpg" alt></p>
<h2 id="3-运行流程"><a href="#3-运行流程" class="headerlink" title="3. 运行流程"></a>3. 运行流程</h2><h3 id="（1）概念层级"><a href="#（1）概念层级" class="headerlink" title="（1）概念层级"></a>（1）概念层级</h3><p>解释1：</p>
<blockquote>
<ol>
<li><p>一个Spark提交时，由Driver运行main方法创建一个SparkContext，由SparkContext负责和Yarn的通信、资源的申请、任务的分配和监控等。</p>
<p>SparkContext会向Yarn注册并申请运行Executor的资源。</p>
</li>
<li><p>Yarn为Executor分配资源，启动Executor进程，Executor发送心跳到Yarn上</p>
</li>
<li><p>SparkContext根据RDD的依赖关系构建DAG图，DAG调度解析后将图分解成多个Stage，并计算出之间的依赖关系，将这些Job集提交给Task调度器处理。Executor向SparkContext申请Task，Task调度器将Task分发给Executor运行，同时，SparkContext将Application代码发放给Executor。</p>
</li>
<li><p>任务在Executor上运行，结果反馈给Job调度器，再反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</p>
</li>
</ol>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/1115a2d3fe534bd8b172961157a56eb2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemt5Q29kZXI=,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>解释2：</p>
<blockquote>
<p>每个 worker 节点包含一个或者多个 executor，一个 executor 中又包含多个 task。task 是真正实现并行计算的最小工作单元。</p>
<ul>
<li><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Driver 是一个 Java 进程，负责执行 Spark 任务的 main 方法，它的职责有：</p>
<ul>
<li><p>执行用户提交的代码，创建 SparkContext 或者 SparkSession</p>
</li>
<li><p>将用户代码转化为Spark任务（Jobs）</p>
</li>
<li><ul>
<li>创建血缘（Lineage），逻辑计划（Logical Plan）和物理计划（Physical Plan)</li>
</ul>
</li>
<li><p>在 Cluster Manager 的辅助下，把 task 任务分发调度出去</p>
</li>
<li><p>跟踪任务的执行情况</p>
</li>
</ul>
</li>
<li><h3 id="Spark-Context-Session"><a href="#Spark-Context-Session" class="headerlink" title="Spark Context/Session"></a>Spark Context/Session</h3><p>它是由Spark driver创建，每个 Spark 应用对应一个。程序和集群交互的入口。可以连接到 Cluster Manager</p>
</li>
<li><h3 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager"></a>Cluster Manager</h3><p>负责部署整个Spark 集群，包括上面提到的 driver 和 executors。具有以下几种部署模式</p>
<ol>
<li>Standalone 模式</li>
<li>YARN</li>
<li>Mesos</li>
<li>Kubernetes</li>
</ol>
</li>
<li><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>一个创建在 worker 节点的进程。一个 Executor 有多个 slots(线程) 可以并发执行多个 tasks。</p>
<ul>
<li>负责执行spark任务，把结果返回给 Driver</li>
<li>可以将数据缓存到 worker 节点的内存</li>
<li>一个 slot 就是一个线程，对应了一个 task</li>
</ul>
</li>
</ul>
<p><img src="..\imgs\Spark架构.jpg" alt></p>
</blockquote>
<p><img src="..\imgs\Spark代码执行流程.jpg" alt></p>
<h3 id="（2）代码层级"><a href="#（2）代码层级" class="headerlink" title="（2）代码层级"></a>（2）代码层级</h3><p>Spark 有懒加载的特性，也就是说 Spark 计算按兵不动，直到遇到 action 类型的 operator 的时候才会触发一次计算。</p>
<blockquote>
<ul>
<li><p>DAG</p>
<ul>
<li>Spark Job如何执行，都是由这个 DAG 来管的，包括决定 task 运行在什么节点</li>
</ul>
</li>
<li><p>Spark Job</p>
<ul>
<li>每个Spark Job 对应一个action</li>
</ul>
</li>
<li><p>Stages</p>
<ul>
<li>每个 Spark Job 包含一系列 stages</li>
<li>Stages 按照数据是否需要 shuffle 来划分（宽依赖）</li>
<li>Stages 之间的执行是串行的（除非stage 间计算的RDD不同）</li>
<li>因为 Stages 是串行的，所以 shuffle 越少越好</li>
</ul>
</li>
<li><p>Tasks</p>
<ul>
<li>每个 stage 包含一系列的 tasks</li>
<li>Tasks 是并行计算的最小单元</li>
<li>一个 stage 中的所有 tasks 执行同一段代码逻辑，只是基于不同的数据块</li>
<li>一个 task 只能在一个executor中执行，不能是多个</li>
<li>一个 stage 输出的 partition 数量等于这个 stage 执行 tasks 的数量</li>
</ul>
</li>
<li><p>Partition</p>
<ul>
<li>Spark 中 partition（分区） 可以理解为内存中的一个数据集</li>
<li>一个 partition 对应一个 task，一个 task 对应 一个 executor 中的一个 slot，一个 slot 对应物理资源是一个线程 thread</li>
<li>1 partition = 1 task = 1 slot = 1 thread</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="3-spark中Master与Worker区别及Driver与Executor区别"><a href="#3-spark中Master与Worker区别及Driver与Executor区别" class="headerlink" title="(3) spark中Master与Worker区别及Driver与Executor区别"></a>(3) spark中Master与Worker区别及Driver与Executor区别</h3><p><img src="..\imgs\Master和Worker关系.jpg" alt></p>
<p><img src="..\imgs\Driver和Executor关系.jpg" alt></p>
<p> Master和Worker是Spark的守护进程，即Spark在特定模式下正常运行所必须的进程。Driver和Executor是临时程序，当有具体任务提交到Spark集群才会开启的程序。</p>
<p><a href="https://blog.csdn.net/nicole_33/article/details/122520395" target="_blank" rel="noopener">了解 Spark中的master、worker和Driver、Executor</a></p>
<p><img src="..\imgs\Spark的worker理解.jpg" alt></p>
<blockquote>
<p>每个Worker上存在一个或者多个ExecutorBackend 进程。每个进程包含一个Executor对象，该对象持有一个线程池，每个线程可以执行一个task。<br>每个application包含一个 driver 和多个 executors，每个 executor里面运行的tasks都属于同一个application。<br>每个Worker上存在一个或者多个ExecutorBackend 进程。<br>每个进程包含一个Executor对象，该对象持有一个线程池，每个线程可以执行一个task</p>
</blockquote>
<h2 id="4-DAGScheduler具体流程"><a href="#4-DAGScheduler具体流程" class="headerlink" title="4. DAGScheduler具体流程"></a>4. DAGScheduler具体流程</h2><p>DAG负责的是将RDD中的数据依赖划分为不同可以并行的宽依赖task， 这些不同的task集合统称为stage，最后将这些stage推送给TaskScheduler进行调度，DAG的具体划分过程如下所示：</p>
<p><img src="..\imgs\DAG流程.jpg" alt></p>
<blockquote>
<ul>
<li><code>窄依赖经历的是map、filter等操作没有进行相关的shuffle，而宽依赖则通常都是join等操作需要进行一定的shuffle意味着需要打散均匀等操作</code></li>
<li>1 stage是触发action的时候 <strong>从后往前划分</strong> 的，所以本图要从RDD_G开始划分。</li>
<li>2 RDD_G依赖于RDD_B和RDD_F，随机决定先判断哪一个依赖，但是对于结果无影响。</li>
<li>3 RDD_B与RDD_G属于窄依赖，所以他们属于同一个stage，RDD_B与老爹RDD_A之间是宽依赖的关系，所以他们不能划分在一起，所以RDD_A自己是一个stage1</li>
<li>4 RDD_F与RDD_G是属于宽依赖，他们不能划分在一起，所以最后一个stage的范围也就限定了，RDD_B和RDD_G组成了Stage3</li>
<li>5 RDD_F与两个爹RDD_D、RDD_E之间是窄依赖关系，RDD_D与爹RDD_C之间也是窄依赖关系，所以他们都属于同一个stage2</li>
<li>6 执行过程中stage1和stage2相互之间没有前后关系所以可以并行执行，相应的每个stage内部各个partition对应的task也并行执行</li>
<li>7 stage3依赖stage1和stage2执行结果的partition，只有等前两个stage执行结束后才可以启动stage3.</li>
<li>8 我们前面有介绍过Spark的Task有两种：ShuffleMapTask和ResultTask，其中后者在DAG最后一个阶段推送给Executor，其余所有阶段推送的都是ShuffleMapTask。在这个案例中stage1和stage2中产生的都是ShuffleMapTask，在stage3中产生的ResultTask。</li>
<li>9 虽然stage的划分是从后往前计算划分的，但是依赖逻辑判断等结束后真正创建stage是从前往后的。也就是说如果从stage的ID作为标识的话，先需要执行的stage的ID要小于后需要执行的ID。就本案例来说，stage1和stage2的ID要小于stage3，至于stage1和stage2的ID谁大谁小是随机的，是由前面第2步决定的。</li>
</ul>
</blockquote>
<h2 id="5-MR和spark区别"><a href="#5-MR和spark区别" class="headerlink" title="5. MR和spark区别"></a>5. MR和spark区别</h2><blockquote>
<h3 id="（1）中间结果输出："><a href="#（1）中间结果输出：" class="headerlink" title="（1）中间结果输出："></a>（1）中间结果输出：</h3><ul>
<li><p>MapReduce：读–处理–写磁盘–读–处理–写磁盘（<strong>中间结果落地，即存入磁盘</strong>）</p>
</li>
<li><p>spark：读–处理–处理–（需要的时候）写磁盘（<strong>中间结果存入内存</strong>）</p>
</li>
</ul>
<p><strong>减少落地时间，速度快</strong></p>
<p><img src="../imgs/Hadoop%E5%92%8Cspark%E5%8C%BA%E5%88%AB.jpg" alt></p>
<h3 id="（2）数据格式："><a href="#（2）数据格式：" class="headerlink" title="（2）数据格式："></a>（2）数据格式：</h3><ul>
<li><p>MapReduce<strong>：从</strong>DB中读取数据再处理</p>
</li>
<li><p>spark：采用弹性分布式数据结构RDD存储数据</p>
</li>
</ul>
<h3 id="（3）容错性："><a href="#（3）容错性：" class="headerlink" title="（3）容错性："></a>（3）容错性：</h3><ul>
<li>Spark：采用RDD存储数据，若数据集丢失，可重建。</li>
</ul>
<h3 id="（4）通用性："><a href="#（4）通用性：" class="headerlink" title="（4）通用性："></a>（4）通用性：</h3><ul>
<li><p>MapReduce：只提供map和reduce两种操作。</p>
</li>
<li><p>spark：提供很多数据集操作类型（transformations、actions）【transformations包括map\filter\</p>
</li>
</ul>
<p>Groupbykey\sort等，action包括reduce、save、collect、lookup等】</p>
<h3 id="（5）执行策略"><a href="#（5）执行策略" class="headerlink" title="（5）执行策略"></a>（5）执行策略</h3><ul>
<li><p>MapReduce：数据shuffle前需排序</p>
</li>
<li><p>spark：不是所有场景都要排序</p>
</li>
</ul>
</blockquote>
<h2 id="6、spark1-x和spark2-x的区别"><a href="#6、spark1-x和spark2-x的区别" class="headerlink" title="6、spark1.x和spark2.x的区别"></a>6、spark1.x和spark2.x的区别</h2><blockquote>
<ul>
<li><p>Spark1.x：采用SparkContext作为进入点</p>
</li>
<li><p>Spark2.x：SparkSession 是 Spark SQL 的入口。</p>
<ul>
<li>采用SparkSession作为进入点，SparkSession可直接读取各种资料源，可直接与Hive元数据沟通，同时包含设定以及资源管理功能。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="7-spark-应用执行模式"><a href="#7-spark-应用执行模式" class="headerlink" title="7. spark 应用执行模式"></a>7. spark 应用执行模式</h2><h3 id="（1）local模式"><a href="#（1）local模式" class="headerlink" title="（1）local模式"></a>（1）local模式</h3><p>local 模式主要是用于本地代码测试操作</p>
<p>本质上就是一个单进程程序, 在一个进程中运行多个线程</p>
<p>类似于pandas , 都是一个单进程程序, 无法处理大规模数据, 只需要处理小规模数据</p>
<p><img src="..\imgs\Spark环境1.jpg" alt></p>
<h3 id="（2）standalone："><a href="#（2）standalone：" class="headerlink" title="（2）standalone："></a>（2）standalone：</h3><blockquote>
<p> Spark Standalone模式：该模式是不借助于第三方资源管理框架的完全分布式模式。Spark 使用自己的 Master 进程对应用程序运行过程中所需的资源进行调度和管理；对于中小规模的 Spark 集群首选 Standalone 模式。目前Spark 在 Standalone 模式下主要是借助 Zookeeper 实现单点故障问题；思想也是类似于 Hbase Master 单点故障解决方案。</p>
</blockquote>
<h3 id="（3）YARN"><a href="#（3）YARN" class="headerlink" title="（3）YARN"></a>（3）YARN</h3><blockquote>
<p>该模式是借助于第三方资源管理框架 Yarn 的完全分布式模式。Spark 作为一个提交程序的客户端将 Job 任务提交到 Yarn 上；然后通过 Yarn 来调度和管理 Job 任务执行过程中所需的资源。需要此模式需要先搭建 Yarn 集群，然后将 Spark 作为 Hadoop 中的一个组件纳入到 Yarn 的调度管理下，这样将更有利于系统资源的共享。</p>
</blockquote>
<h2 id="8-提交任务方法"><a href="#8-提交任务方法" class="headerlink" title="8. 提交任务方法"></a>8. 提交任务方法</h2><blockquote>
<p><strong>（1）spark shell</strong></p>
<ul>
<li>spark-shell 是 Spark 自带的交互式 Shell 程序，方便用户进行交互式编程，用户可以在该命令行下用 <a href="https://so.csdn.net/so/search?q=Scala&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener">Scala</a> 编写 spark 程序。</li>
<li><p>应用场景</p>
<ul>
<li>通常是以测试为主</li>
<li>所以一般直接以<code>./spark-shell</code>启动，进入本地模式测试</li>
</ul>
</li>
<li>local方式启动：./spark-shell</li>
<li>standalone集群模式启动：./spark-shell –master spark://master:7077</li>
<li>yarn client模式启动：./spark-shell –master yarn-client</li>
</ul>
<p><strong>（2）spark submit</strong></p>
<p>使用spark 自带的spark-submit工具提交任务</p>
<p>程序一旦打包好，就可以使用 bin/spark-submit 脚本启动应用了。这个脚本负责设置 spark 使用的 classpath 和依赖，支持不同类型的集群管理器和发布模式。</p>
<p><strong>它主要是用于提交编译并打包好的Jar包到集群环境中来运行</strong>，和hadoop中的hadoop jar命令很类似，hadoop jar是提交一个MR-task,而spark-submit是提交一个spark任务，这个脚本 可以设置Spark类路径（classpath）和应用程序依赖包，并且可以设置不同的Spark所支持的集群管理和部署模式。 相对于spark-shell来讲它不具有REPL(交互式的编程环境)的，在运行前需要指定应用的启动类，jar包路径,参数等内容。</p>
</blockquote>
<h2 id="9-参数配置："><a href="#9-参数配置：" class="headerlink" title="9. 参数配置："></a>9. 参数配置：</h2><p>参数名    参数说明</p>
<ul>
<li>-class    应用程序的主类，仅针对 java 或 scala 应用</li>
<li>-master    master 的地址，提交任务到哪里执行，例如 local,spark://host:port, yarn, local</li>
<li>-deploy-mode    在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</li>
<li>-name    应用程序的名称，会显示在Spark的网页用户界面</li>
<li>-jars    用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</li>
<li>-packages    包含在driver 和executor 的 classpath 中的 jar 的 maven 坐标</li>
<li>-exclude-packages    为了避免冲突 而指定不包含的 package</li>
<li>-repositories    远程 repository</li>
<li>-conf PROP=VALUE    指定 spark 配置属性的值，例如 -conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=256m”</li>
<li>-properties-file    加载的配置文件，默认为 conf/spark-defaults.conf</li>
<li>-driver-memory    Driver内存，默认 1G</li>
<li>-driver-java-options    传给 driver 的额外的 Java 选项</li>
<li>-driver-library-path    传给 driver 的额外的库路径</li>
<li>-driver-class-path    传给 driver 的额外的类路径</li>
<li>-driver-cores    Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</li>
<li>-executor-memory    每个 executor 的内存，默认是1G</li>
<li>-total-executor-cores    所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用</li>
<li>-num-executors    启动的 executor 数量。默认为2。在 yarn 下使用</li>
<li>-executor-core    每个 executor 的核数。在yarn或者standalone下使用</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Lee_yl</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lee_yl</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
